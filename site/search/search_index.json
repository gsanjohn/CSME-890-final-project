{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs This website was created as a part of CMSE-890 Reproducible Computational Workflows. The website serves as documentation for a Bayesian Block Analysis routine designed to be used with NuSTAR X-ray Telescope data. Overview The pages of this website are organized using the Diataxis format. Explanation - Understanding the project. How-To Install - How to clone the repository. Tutorials - Example function usage. API reference - Function documentation. A complete DFD can be found below:","title":"Welcome to MkDocs"},{"location":"#welcome-to-mkdocs","text":"This website was created as a part of CMSE-890 Reproducible Computational Workflows. The website serves as documentation for a Bayesian Block Analysis routine designed to be used with NuSTAR X-ray Telescope data.","title":"Welcome to MkDocs"},{"location":"#overview","text":"The pages of this website are organized using the Diataxis format. Explanation - Understanding the project. How-To Install - How to clone the repository. Tutorials - Example function usage. API reference - Function documentation. A complete DFD can be found below:","title":"Overview"},{"location":"API_homepage/","text":"Function reference Date: November 14, 2025 This reference manual details the main and support functions used in the routine. Main Script API Bayesian Block Routine Support Script API barycenter correction Bayesian Block Class Average Count rates Clean GTI Create Light Curve Loading Data Detailed Flare Analysis Filter events with common GTIs Energy Filter Find change points Flare Block Formatting Binned event correction factors Event correction factors Insert GTI gaps Make ReadMe Merge events from modules A and B Merge GTIs from modules A and B Plot lightcurve Save BBA Results Remove GTI Time Gaps","title":"API"},{"location":"API_homepage/#function-reference","text":"Date: November 14, 2025 This reference manual details the main and support functions used in the routine.","title":"Function reference"},{"location":"API_homepage/#main-script-api","text":"","title":"Main Script API"},{"location":"API_homepage/#bayesian-block-routine","text":"","title":"Bayesian Block Routine"},{"location":"API_homepage/#support-script-api","text":"","title":"Support Script API"},{"location":"API_homepage/#barycenter-correction","text":"","title":"barycenter correction"},{"location":"API_homepage/#bayesian-block-class","text":"","title":"Bayesian Block Class"},{"location":"API_homepage/#average-count-rates","text":"","title":"Average Count rates"},{"location":"API_homepage/#clean-gti","text":"","title":"Clean GTI"},{"location":"API_homepage/#create-light-curve","text":"","title":"Create Light Curve"},{"location":"API_homepage/#loading-data","text":"","title":"Loading Data"},{"location":"API_homepage/#detailed-flare-analysis","text":"","title":"Detailed Flare Analysis"},{"location":"API_homepage/#filter-events-with-common-gtis","text":"","title":"Filter events with common GTIs"},{"location":"API_homepage/#energy-filter","text":"","title":"Energy Filter"},{"location":"API_homepage/#find-change-points","text":"","title":"Find change points"},{"location":"API_homepage/#flare-block-formatting","text":"","title":"Flare Block Formatting"},{"location":"API_homepage/#binned-event-correction-factors","text":"","title":"Binned event correction factors"},{"location":"API_homepage/#event-correction-factors","text":"","title":"Event correction factors"},{"location":"API_homepage/#insert-gti-gaps","text":"","title":"Insert GTI gaps"},{"location":"API_homepage/#make-readme","text":"","title":"Make ReadMe"},{"location":"API_homepage/#merge-events-from-modules-a-and-b","text":"","title":"Merge events from modules A and B"},{"location":"API_homepage/#merge-gtis-from-modules-a-and-b","text":"","title":"Merge GTIs from modules A and B"},{"location":"API_homepage/#plot-lightcurve","text":"","title":"Plot lightcurve"},{"location":"API_homepage/#save-bba-results","text":"","title":"Save BBA Results"},{"location":"API_homepage/#remove-gti-time-gaps","text":"","title":"Remove GTI Time Gaps"},{"location":"barycenter_corr/","text":"barycorr ( corr_file , event_file_a , eventsA , eventsB , gtiA , gtiB ) Load barycenter corrected event file and xselected event file, get time correction, apply time correction. Parameters: Name Type Description Default corr_file str Path to the corrected event FITS file required event_file_a str path to event FITS file required eventsA DataFrame events dataframe required eventsB DataFrame events dataframe required gtiA DataFrame gti dataframe required gtiB DataFrame gti dataframe required Returns: Type Description DataFrame Event DataFrame with 'TIME' corrected, gti DataFrame with 'GTI' corrected. Source code in scripts/barycenter_corr.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def barycorr ( corr_file : str , event_file_a : str , eventsA : pd . DataFrame , eventsB : pd . DataFrame , gtiA : pd . DataFrame , gtiB : pd . DataFrame , ): \"\"\" Load barycenter corrected event file and xselected event file, get time correction, apply time correction. Args: corr_file (str): Path to the corrected event FITS file event_file_a (str): path to event FITS file eventsA (pd.DataFrame): events dataframe eventsB (pd.DataFrame): events dataframe gtiA (pd.DataFrame): gti dataframe gtiB (pd.DataFrame): gti dataframe Returns: (pd.DataFrame): Event DataFrame with 'TIME' corrected, gti DataFrame with 'GTI' corrected. \"\"\" # Read TSTART from barycenter corrected event with fits . open ( corr_file ) as hdul : header = hdul [ 0 ] . header tstartBC = header . get ( \"TSTART\" ) # Read TSTART from events file with fits . open ( event_file_a ) as hdul : header = hdul [ 0 ] . header tstart = header . get ( \"TSTART\" ) # get delta t deltaT = tstartBC - tstart # add correction to events eventsA [ \"TIME\" ] += deltaT eventsB [ \"TIME\" ] += deltaT # add correction to GTIs gtiA += deltaT gtiB += deltaT return eventsA , eventsB , gtiA , gtiB","title":"Barycenter corr"},{"location":"barycenter_corr/#scripts.barycenter_corr.barycorr","text":"Load barycenter corrected event file and xselected event file, get time correction, apply time correction. Parameters: Name Type Description Default corr_file str Path to the corrected event FITS file required event_file_a str path to event FITS file required eventsA DataFrame events dataframe required eventsB DataFrame events dataframe required gtiA DataFrame gti dataframe required gtiB DataFrame gti dataframe required Returns: Type Description DataFrame Event DataFrame with 'TIME' corrected, gti DataFrame with 'GTI' corrected. Source code in scripts/barycenter_corr.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def barycorr ( corr_file : str , event_file_a : str , eventsA : pd . DataFrame , eventsB : pd . DataFrame , gtiA : pd . DataFrame , gtiB : pd . DataFrame , ): \"\"\" Load barycenter corrected event file and xselected event file, get time correction, apply time correction. Args: corr_file (str): Path to the corrected event FITS file event_file_a (str): path to event FITS file eventsA (pd.DataFrame): events dataframe eventsB (pd.DataFrame): events dataframe gtiA (pd.DataFrame): gti dataframe gtiB (pd.DataFrame): gti dataframe Returns: (pd.DataFrame): Event DataFrame with 'TIME' corrected, gti DataFrame with 'GTI' corrected. \"\"\" # Read TSTART from barycenter corrected event with fits . open ( corr_file ) as hdul : header = hdul [ 0 ] . header tstartBC = header . get ( \"TSTART\" ) # Read TSTART from events file with fits . open ( event_file_a ) as hdul : header = hdul [ 0 ] . header tstart = header . get ( \"TSTART\" ) # get delta t deltaT = tstartBC - tstart # add correction to events eventsA [ \"TIME\" ] += deltaT eventsB [ \"TIME\" ] += deltaT # add correction to GTIs gtiA += deltaT gtiB += deltaT return eventsA , eventsB , gtiA , gtiB","title":"barycorr"},{"location":"bayesian_block/","text":"Bayesian Blocks for Time Series Analysis. Bayesian Blocks for Time Series Analysis Dynamic programming algorithm for solving a piecewise-constant model for various datasets. This is based on the algorithm presented in Scargle et al 2013 [1] . This code was ported from the astroML project [2] . THIS IS AN ASTROPY PACKAGE EDITED BY GRACE SANGER-JOHNSON Applications include: finding an optimal histogram with adaptive bin widths finding optimal segmentation of time series data detecting inflection points in the rate of event data The primary interface to these routines is the :func: bayesian_blocks function. This module provides fitness functions suitable for three types of data: Irregularly-spaced event data via the :class: Events class Regularly-spaced event data via the :class: RegularEvents class Irregularly-spaced point measurements via the :class: PointMeasures class For more fine-tuned control over the fitness functions used, it is possible to define custom :class: FitnessFunc classes directly and use them with the :func: bayesian_blocks routine. One common application of the Bayesian Blocks algorithm is the determination of optimal adaptive-width histogram bins. This uses the same fitness function as for irregularly-spaced time series events. The easiest interface for creating Bayesian Blocks histograms is the :func: astropy.stats.histogram function. References .. [1] https://ui.adsabs.harvard.edu/abs/2013ApJ...764..167S .. [2] https://www.astroml.org/ https://github.com//astroML/astroML/ .. [3] Bellman, R.E., Dreyfus, S.E., 1962. Applied Dynamic Programming. Princeton University Press, Princeton. https://press.princeton.edu/books/hardcover/9780691651873/applied-dynamic-programming .. [4] Bellman, R., Roth, R., 1969. Curve fitting by segmented straight lines. J. Amer. Statist. Assoc. 64, 1079\u20131084. https://www.tandfonline.com/doi/abs/10.1080/01621459.1969.10501038 Events Bases: FitnessFunc Bayesian blocks fitness for binned or unbinned events. Parameters p0 : float, optional False alarm probability, used to compute the prior on :math: N_{\\rm blocks} (see eq. 21 of Scargle 2013). For the Events type data, p0 does not seem to be an accurate representation of the actual false alarm probability. If you are using this fitness function for a triggering type condition, it is recommended that you run statistical trials on signal-free noise to determine an appropriate value of gamma or ncp_prior to use for a desired false alarm rate. gamma : float, optional If specified, then use this gamma to compute the general prior form, :math: p \\sim {\\tt gamma}^{N_{\\rm blocks}} . If gamma is specified, p0 is ignored. ncp_prior : float, optional If specified, use the value of ncp_prior to compute the prior as above, using the definition :math: {\\tt ncp\\_prior} = -\\ln({\\tt gamma}) . If ncp_prior is specified, gamma and p0 is ignored. Source code in scripts/expo_events.py 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 class Events ( FitnessFunc ): r \"\"\"Bayesian blocks fitness for binned or unbinned events. Parameters ---------- p0 : float, optional False alarm probability, used to compute the prior on :math:`N_{\\rm blocks}` (see eq. 21 of Scargle 2013). For the Events type data, ``p0`` does not seem to be an accurate representation of the actual false alarm probability. If you are using this fitness function for a triggering type condition, it is recommended that you run statistical trials on signal-free noise to determine an appropriate value of ``gamma`` or ``ncp_prior`` to use for a desired false alarm rate. gamma : float, optional If specified, then use this gamma to compute the general prior form, :math:`p \\sim {\\tt gamma}^{N_{\\rm blocks}}`. If gamma is specified, p0 is ignored. ncp_prior : float, optional If specified, use the value of ``ncp_prior`` to compute the prior as above, using the definition :math:`{\\tt ncp\\_prior} = -\\ln({\\tt gamma})`. If ``ncp_prior`` is specified, ``gamma`` and ``p0`` is ignored. \"\"\" def fitness ( self , N_k : NDArray [ float ], T_k : NDArray [ float ]) -> NDArray [ float ]: # Implement Eq. 19 from Scargle (2013), i.e., N_k * ln(N_k / T_k). # Note that when N_k -> 0, the limit of N_k * ln(N_k / T_k) is 0. # N_k is guaranteed to be non-negative integers by the `validate_input` # method, so no need to check for negative values here. # First, initialize an array of zeros to store the fitness values, # then calculate the fitness values only where N_k > 0. # For N_k == 0, the corresponding fitness values are zero already. out = np . zeros ( N_k . shape ) mask = N_k > 0 rate = np . divide ( N_k , T_k , out = out , where = mask ) ln_rate = np . log ( rate , out = out , where = mask ) return np . multiply ( N_k , ln_rate , out = out , where = mask ) def validate_input ( self , t : ArrayLike , x : ArrayLike | None , sigma : float | ArrayLike | None , ex : ArrayLike | None , ) -> tuple [ NDArray [ float ], NDArray [ float ], NDArray [ float ]]: t , x , sigma , ex = super () . validate_input ( t , x , sigma , ex ) if ( x is not None ) and ( np . any ( x % 1 > 0 ) or np . any ( x < 0 )): raise ValueError ( \"x must be non-negative integer counts for fitness='events'\" ) return t , x , sigma , ex FitnessFunc Base class for bayesian blocks fitness functions. Derived classes should overload the following method: fitness(self, **kwargs) : Compute the fitness given a set of named arguments. Arguments accepted by fitness must be among [T_k, N_k, a_k, b_k, c_k] (See [1]_ for details on the meaning of these parameters). Additionally, other methods may be overloaded as well: __init__(self, **kwargs) : Initialize the fitness function with any parameters beyond the normal p0 and gamma . validate_input(self, t, x, sigma) : Enable specific checks of the input data ( t , x , sigma ) to be performed prior to the fit. compute_ncp_prior(self, N) : If ncp_prior is not defined explicitly, this function is called in order to define it before fitting. This may be calculated from gamma , p0 , or whatever method you choose. p0_prior(self, N) : Specify the form of the prior given the false-alarm probability p0 (See [1]_ for details). For examples of implemented fitness functions, see :class: Events , :class: RegularEvents , and :class: PointMeasures . References .. [1] Scargle, J et al. (2013) https://ui.adsabs.harvard.edu/abs/2013ApJ...764..167S Source code in scripts/expo_events.py 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 class FitnessFunc : \"\"\"Base class for bayesian blocks fitness functions. Derived classes should overload the following method: ``fitness(self, **kwargs)``: Compute the fitness given a set of named arguments. Arguments accepted by fitness must be among ``[T_k, N_k, a_k, b_k, c_k]`` (See [1]_ for details on the meaning of these parameters). Additionally, other methods may be overloaded as well: ``__init__(self, **kwargs)``: Initialize the fitness function with any parameters beyond the normal ``p0`` and ``gamma``. ``validate_input(self, t, x, sigma)``: Enable specific checks of the input data (``t``, ``x``, ``sigma``) to be performed prior to the fit. ``compute_ncp_prior(self, N)``: If ``ncp_prior`` is not defined explicitly, this function is called in order to define it before fitting. This may be calculated from ``gamma``, ``p0``, or whatever method you choose. ``p0_prior(self, N)``: Specify the form of the prior given the false-alarm probability ``p0`` (See [1]_ for details). For examples of implemented fitness functions, see :class:`Events`, :class:`RegularEvents`, and :class:`PointMeasures`. References ---------- .. [1] Scargle, J et al. (2013) https://ui.adsabs.harvard.edu/abs/2013ApJ...764..167S \"\"\" def __init__ ( self , p0 : float = 0.05 , gamma : float | None = None , ncp_prior : float | None = None , ) -> None : self . p0 = p0 self . gamma = gamma self . ncp_prior = ncp_prior def validate_input ( self , t : ArrayLike , x : ArrayLike | None = None , sigma : float | ArrayLike | None = None , ex : ArrayLike | None = None , ) -> tuple [ NDArray [ float ], NDArray [ float ], NDArray [ float ], NDArray [ float ]]: \"\"\"Validate inputs to the model. Parameters ---------- t : array-like times of observations x : array-like, optional values observed at each time sigma : float or array-like, optional errors in values x ex : array-like, optional Returns ------- t, x, sigma, ex : array-like, float validated and perhaps modified versions of inputs \"\"\" # validate array input t = np . asarray ( t , dtype = float ) # find unique values of t t = np . array ( t ) if t . ndim != 1 : raise ValueError ( \"t must be a one-dimensional array\" ) unq_t , unq_ind , unq_inv = np . unique ( t , return_index = True , return_inverse = True ) # if x is not specified, x will be counts at each time if x is None : if sigma is not None : raise ValueError ( \"If sigma is specified, x must be specified\" ) else : sigma = 1.0 if len ( unq_t ) == len ( t ): x = np . ones_like ( t ) else : x = np . bincount ( unq_inv ) t = unq_t # if x is specified, then we need to simultaneously sort t and x else : # TODO: allow broadcasted x? x = np . asarray ( x , dtype = float ) if x . shape not in [(), ( 1 ,), ( t . size ,)]: raise ValueError ( \"x does not match shape of t\" ) x += np . zeros_like ( t ) if len ( unq_t ) != len ( t ): raise ValueError ( \"Repeated values in t not supported when x is specified\" ) t = unq_t x = x [ unq_ind ] # verify the given sigma value if sigma is None : sigma = 1.0 else : sigma = np . asarray ( sigma , dtype = float ) if sigma . shape not in [(), ( 1 ,), ( t . size ,)]: raise ValueError ( \"sigma does not match the shape of x\" ) # validate array input ex = np . asarray ( ex , dtype = float ) # if ex is not specified, x will be counts at each time if ex is None : if len ( unq_t ) == len ( t ): ex = np . ones_like ( t ) else : ex = np . bincount ( unq_inv ) return t , x , sigma , ex def fitness ( self , ** kwargs ): raise NotImplementedError () def p0_prior ( self , N : int ) -> float : \"\"\"Empirical prior, parametrized by the false alarm probability ``p0``. See eq. 21 in Scargle (2013). Note that there was an error in this equation in the original Scargle paper (the \"log\" was missing). The following corrected form is taken from https://arxiv.org/abs/1304.2818 \"\"\" return 4 - np . log ( 73.53 * self . p0 * ( N **- 0.478 )) # the fitness_args property will return the list of arguments accepted by # the method fitness(). This allows more efficient computation below. @property def _fitness_args ( self ) -> KeysView [ str ]: return signature ( self . fitness ) . parameters . keys () def compute_ncp_prior ( self , N : int ) -> float : \"\"\" If ``ncp_prior`` is not explicitly defined, compute it from ``gamma`` or ``p0``. \"\"\" if self . gamma is not None : return - np . log ( self . gamma ) elif self . p0 is not None : return self . p0_prior ( N ) else : raise ValueError ( \"``ncp_prior`` cannot be computed as neither \" \"``gamma`` nor ``p0`` is defined.\" ) def fit ( self , t : ArrayLike , x : ArrayLike | None = None , sigma : ArrayLike | float | None = None , ex : ArrayLike | None = None , ) -> NDArray [ float ]: \"\"\"Fit the Bayesian Blocks model given the specified fitness function. Parameters ---------- t : array-like data times (one dimensional, length N) x : array-like, optional data values sigma : array-like or float, optional data errors ex : array-like, optional Returns ------- edges : ndarray array containing the (M+1) edges defining the M optimal bins \"\"\" t , x , sigma , ex = self . validate_input ( t , x , sigma , ex ) # compute values needed for computation, below if \"a_k\" in self . _fitness_args : ak_raw = np . ones_like ( x ) / sigma ** 2 if \"b_k\" in self . _fitness_args : bk_raw = x / sigma ** 2 if \"c_k\" in self . _fitness_args : ck_raw = x * x / sigma ** 2 # create length-(N + 1) array of cell edges # edges = np.concatenate([t[:1], 0.5 * (t[1:] + t[:-1]), t[-1:]]) #ORIGINAL nbpts = len ( t ) edges = np . concatenate ( [ [ 0.5 * ( t [ 0 ] + t [ 1 ]) - t [ 0 ]], ( np . diff ( t [ 1 : nbpts ]) + np . diff ( t [ 0 : nbpts - 1 ])) / 2.0 , [ t [ - 1 ] - 0.5 * ( t [ nbpts - 2 ] + t [ nbpts - 1 ])], ] ) edges *= ex # edges = np.concatenate([t[:1], 0.5 * (t[1:] + t[:-1])]) # edges=np.concatenate([t[:1]*ex[:1], (0.5 * (t[1:] + t[:-1]))*(ex[-1]), t[-1:]*ex[-1:]]) # block_length = (t[-1] - edges) #ORIGINAL block_length = np . sum ( edges ) - np . concatenate (([ 0.0 ], np . cumsum ( edges ))) # arrays to store the best configuration N = len ( t ) best = np . zeros ( N , dtype = float ) last = np . zeros ( N , dtype = int ) # Compute ncp_prior if not defined if self . ncp_prior is None : ncp_prior = self . compute_ncp_prior ( N ) else : ncp_prior = self . ncp_prior # ---------------------------------------------------------------- # Start with first data cell; add one cell at each iteration # ---------------------------------------------------------------- for R in range ( N ): # Compute fit_vec : fitness of putative last block (end at R) kwds = {} # T_k: width/duration of each block if \"T_k\" in self . _fitness_args : kwds [ \"T_k\" ] = block_length [: ( R + 1 )] - block_length [ R + 1 ] # N_k: number of elements in each block if \"N_k\" in self . _fitness_args : kwds [ \"N_k\" ] = np . cumsum ( x [: ( R + 1 )][:: - 1 ])[:: - 1 ] # a_k: eq. 31 if \"a_k\" in self . _fitness_args : kwds [ \"a_k\" ] = 0.5 * np . cumsum ( ak_raw [: ( R + 1 )][:: - 1 ])[:: - 1 ] # b_k: eq. 32 if \"b_k\" in self . _fitness_args : kwds [ \"b_k\" ] = - np . cumsum ( bk_raw [: ( R + 1 )][:: - 1 ])[:: - 1 ] # c_k: eq. 33 if \"c_k\" in self . _fitness_args : kwds [ \"c_k\" ] = 0.5 * np . cumsum ( ck_raw [: ( R + 1 )][:: - 1 ])[:: - 1 ] # evaluate fitness function fit_vec = self . fitness ( ** kwds ) A_R = fit_vec - ncp_prior A_R [ 1 :] += best [: R ] i_max = np . argmax ( A_R ) last [ R ] = i_max best [ R ] = A_R [ i_max ] # ---------------------------------------------------------------- # Now find changepoints by iteratively peeling off the last block # ---------------------------------------------------------------- change_points = np . zeros ( N , dtype = int ) i_cp = N ind = N while i_cp > 0 : i_cp -= 1 change_points [ i_cp ] = ind if ind == 0 : break ind = last [ ind - 1 ] if i_cp == 0 : change_points [ i_cp ] = 0 change_points = change_points [ i_cp :] # the last one will be the end of the array so make end of array change_points [ - 1 ] -= 1 return t [ change_points ] compute_ncp_prior ( N ) If ncp_prior is not explicitly defined, compute it from gamma or p0 . Source code in scripts/expo_events.py 348 349 350 351 352 353 354 355 356 357 358 359 360 361 def compute_ncp_prior ( self , N : int ) -> float : \"\"\" If ``ncp_prior`` is not explicitly defined, compute it from ``gamma`` or ``p0``. \"\"\" if self . gamma is not None : return - np . log ( self . gamma ) elif self . p0 is not None : return self . p0_prior ( N ) else : raise ValueError ( \"``ncp_prior`` cannot be computed as neither \" \"``gamma`` nor ``p0`` is defined.\" ) fit ( t , x = None , sigma = None , ex = None ) Fit the Bayesian Blocks model given the specified fitness function. Parameters t : array-like data times (one dimensional, length N) x : array-like, optional data values sigma : array-like or float, optional data errors ex : array-like, optional Returns edges : ndarray array containing the (M+1) edges defining the M optimal bins Source code in scripts/expo_events.py 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 def fit ( self , t : ArrayLike , x : ArrayLike | None = None , sigma : ArrayLike | float | None = None , ex : ArrayLike | None = None , ) -> NDArray [ float ]: \"\"\"Fit the Bayesian Blocks model given the specified fitness function. Parameters ---------- t : array-like data times (one dimensional, length N) x : array-like, optional data values sigma : array-like or float, optional data errors ex : array-like, optional Returns ------- edges : ndarray array containing the (M+1) edges defining the M optimal bins \"\"\" t , x , sigma , ex = self . validate_input ( t , x , sigma , ex ) # compute values needed for computation, below if \"a_k\" in self . _fitness_args : ak_raw = np . ones_like ( x ) / sigma ** 2 if \"b_k\" in self . _fitness_args : bk_raw = x / sigma ** 2 if \"c_k\" in self . _fitness_args : ck_raw = x * x / sigma ** 2 # create length-(N + 1) array of cell edges # edges = np.concatenate([t[:1], 0.5 * (t[1:] + t[:-1]), t[-1:]]) #ORIGINAL nbpts = len ( t ) edges = np . concatenate ( [ [ 0.5 * ( t [ 0 ] + t [ 1 ]) - t [ 0 ]], ( np . diff ( t [ 1 : nbpts ]) + np . diff ( t [ 0 : nbpts - 1 ])) / 2.0 , [ t [ - 1 ] - 0.5 * ( t [ nbpts - 2 ] + t [ nbpts - 1 ])], ] ) edges *= ex # edges = np.concatenate([t[:1], 0.5 * (t[1:] + t[:-1])]) # edges=np.concatenate([t[:1]*ex[:1], (0.5 * (t[1:] + t[:-1]))*(ex[-1]), t[-1:]*ex[-1:]]) # block_length = (t[-1] - edges) #ORIGINAL block_length = np . sum ( edges ) - np . concatenate (([ 0.0 ], np . cumsum ( edges ))) # arrays to store the best configuration N = len ( t ) best = np . zeros ( N , dtype = float ) last = np . zeros ( N , dtype = int ) # Compute ncp_prior if not defined if self . ncp_prior is None : ncp_prior = self . compute_ncp_prior ( N ) else : ncp_prior = self . ncp_prior # ---------------------------------------------------------------- # Start with first data cell; add one cell at each iteration # ---------------------------------------------------------------- for R in range ( N ): # Compute fit_vec : fitness of putative last block (end at R) kwds = {} # T_k: width/duration of each block if \"T_k\" in self . _fitness_args : kwds [ \"T_k\" ] = block_length [: ( R + 1 )] - block_length [ R + 1 ] # N_k: number of elements in each block if \"N_k\" in self . _fitness_args : kwds [ \"N_k\" ] = np . cumsum ( x [: ( R + 1 )][:: - 1 ])[:: - 1 ] # a_k: eq. 31 if \"a_k\" in self . _fitness_args : kwds [ \"a_k\" ] = 0.5 * np . cumsum ( ak_raw [: ( R + 1 )][:: - 1 ])[:: - 1 ] # b_k: eq. 32 if \"b_k\" in self . _fitness_args : kwds [ \"b_k\" ] = - np . cumsum ( bk_raw [: ( R + 1 )][:: - 1 ])[:: - 1 ] # c_k: eq. 33 if \"c_k\" in self . _fitness_args : kwds [ \"c_k\" ] = 0.5 * np . cumsum ( ck_raw [: ( R + 1 )][:: - 1 ])[:: - 1 ] # evaluate fitness function fit_vec = self . fitness ( ** kwds ) A_R = fit_vec - ncp_prior A_R [ 1 :] += best [: R ] i_max = np . argmax ( A_R ) last [ R ] = i_max best [ R ] = A_R [ i_max ] # ---------------------------------------------------------------- # Now find changepoints by iteratively peeling off the last block # ---------------------------------------------------------------- change_points = np . zeros ( N , dtype = int ) i_cp = N ind = N while i_cp > 0 : i_cp -= 1 change_points [ i_cp ] = ind if ind == 0 : break ind = last [ ind - 1 ] if i_cp == 0 : change_points [ i_cp ] = 0 change_points = change_points [ i_cp :] # the last one will be the end of the array so make end of array change_points [ - 1 ] -= 1 return t [ change_points ] p0_prior ( N ) Empirical prior, parametrized by the false alarm probability p0 . See eq. 21 in Scargle (2013). Note that there was an error in this equation in the original Scargle paper (the \"log\" was missing). The following corrected form is taken from https://arxiv.org/abs/1304.2818 Source code in scripts/expo_events.py 331 332 333 334 335 336 337 338 339 340 def p0_prior ( self , N : int ) -> float : \"\"\"Empirical prior, parametrized by the false alarm probability ``p0``. See eq. 21 in Scargle (2013). Note that there was an error in this equation in the original Scargle paper (the \"log\" was missing). The following corrected form is taken from https://arxiv.org/abs/1304.2818 \"\"\" return 4 - np . log ( 73.53 * self . p0 * ( N **- 0.478 )) validate_input ( t , x = None , sigma = None , ex = None ) Validate inputs to the model. Parameters t : array-like times of observations x : array-like, optional values observed at each time sigma : float or array-like, optional errors in values x ex : array-like, optional Returns t, x, sigma, ex : array-like, float validated and perhaps modified versions of inputs Source code in scripts/expo_events.py 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 def validate_input ( self , t : ArrayLike , x : ArrayLike | None = None , sigma : float | ArrayLike | None = None , ex : ArrayLike | None = None , ) -> tuple [ NDArray [ float ], NDArray [ float ], NDArray [ float ], NDArray [ float ]]: \"\"\"Validate inputs to the model. Parameters ---------- t : array-like times of observations x : array-like, optional values observed at each time sigma : float or array-like, optional errors in values x ex : array-like, optional Returns ------- t, x, sigma, ex : array-like, float validated and perhaps modified versions of inputs \"\"\" # validate array input t = np . asarray ( t , dtype = float ) # find unique values of t t = np . array ( t ) if t . ndim != 1 : raise ValueError ( \"t must be a one-dimensional array\" ) unq_t , unq_ind , unq_inv = np . unique ( t , return_index = True , return_inverse = True ) # if x is not specified, x will be counts at each time if x is None : if sigma is not None : raise ValueError ( \"If sigma is specified, x must be specified\" ) else : sigma = 1.0 if len ( unq_t ) == len ( t ): x = np . ones_like ( t ) else : x = np . bincount ( unq_inv ) t = unq_t # if x is specified, then we need to simultaneously sort t and x else : # TODO: allow broadcasted x? x = np . asarray ( x , dtype = float ) if x . shape not in [(), ( 1 ,), ( t . size ,)]: raise ValueError ( \"x does not match shape of t\" ) x += np . zeros_like ( t ) if len ( unq_t ) != len ( t ): raise ValueError ( \"Repeated values in t not supported when x is specified\" ) t = unq_t x = x [ unq_ind ] # verify the given sigma value if sigma is None : sigma = 1.0 else : sigma = np . asarray ( sigma , dtype = float ) if sigma . shape not in [(), ( 1 ,), ( t . size ,)]: raise ValueError ( \"sigma does not match the shape of x\" ) # validate array input ex = np . asarray ( ex , dtype = float ) # if ex is not specified, x will be counts at each time if ex is None : if len ( unq_t ) == len ( t ): ex = np . ones_like ( t ) else : ex = np . bincount ( unq_inv ) return t , x , sigma , ex PointMeasures Bases: FitnessFunc Bayesian blocks fitness for point measures. Parameters p0 : float, optional False alarm probability, used to compute the prior on :math: N_{\\rm blocks} (see eq. 21 of Scargle 2013). If gamma is specified, p0 is ignored. gamma : float, optional If specified, then use this gamma to compute the general prior form, :math: p \\sim {\\tt gamma}^{N_{\\rm blocks}} . If gamma is specified, p0 is ignored. ncp_prior : float, optional If specified, use the value of ncp_prior to compute the prior as above, using the definition :math: {\\tt ncp\\_prior} = -\\ln({\\tt gamma}) . If ncp_prior is specified, gamma and p0 are ignored. Source code in scripts/expo_events.py 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 class PointMeasures ( FitnessFunc ): r \"\"\"Bayesian blocks fitness for point measures. Parameters ---------- p0 : float, optional False alarm probability, used to compute the prior on :math:`N_{\\rm blocks}` (see eq. 21 of Scargle 2013). If gamma is specified, p0 is ignored. gamma : float, optional If specified, then use this gamma to compute the general prior form, :math:`p \\sim {\\tt gamma}^{N_{\\rm blocks}}`. If gamma is specified, p0 is ignored. ncp_prior : float, optional If specified, use the value of ``ncp_prior`` to compute the prior as above, using the definition :math:`{\\tt ncp\\_prior} = -\\ln({\\tt gamma})`. If ``ncp_prior`` is specified, ``gamma`` and ``p0`` are ignored. \"\"\" def __init__ ( self , p0 : float = 0.05 , gamma : float | None = None , ncp_prior : float | None = None , ) -> None : super () . __init__ ( p0 , gamma , ncp_prior ) def fitness ( self , a_k : NDArray [ float ], b_k : ArrayLike ) -> NDArray [ float ]: # eq. 41 from Scargle 2013 return ( b_k * b_k ) / ( 4 * a_k ) def validate_input ( self , t : ArrayLike , x : ArrayLike | None , sigma : float | ArrayLike | None , ) -> tuple [ NDArray [ float ], NDArray [ float ], NDArray [ float ]]: if x is None : raise ValueError ( \"x must be specified for point measures\" ) return super () . validate_input ( t , x , sigma ) RegularEvents Bases: FitnessFunc Bayesian blocks fitness for regular events. This is for data which has a fundamental \"tick\" length, so that all measured values are multiples of this tick length. In each tick, there are either zero or one counts. Parameters dt : float tick rate for data p0 : float, optional False alarm probability, used to compute the prior on :math: N_{\\rm blocks} (see eq. 21 of Scargle 2013). If gamma is specified, p0 is ignored. gamma : float, optional If specified, then use this gamma to compute the general prior form, :math: p \\sim {\\tt gamma}^{N_{\\rm blocks}} . If gamma is specified, p0 is ignored. ncp_prior : float, optional If specified, use the value of ncp_prior to compute the prior as above, using the definition :math: {\\tt ncp\\_prior} = -\\ln({\\tt gamma}) . If ncp_prior is specified, gamma and p0 are ignored. Source code in scripts/expo_events.py 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 class RegularEvents ( FitnessFunc ): r \"\"\"Bayesian blocks fitness for regular events. This is for data which has a fundamental \"tick\" length, so that all measured values are multiples of this tick length. In each tick, there are either zero or one counts. Parameters ---------- dt : float tick rate for data p0 : float, optional False alarm probability, used to compute the prior on :math:`N_{\\rm blocks}` (see eq. 21 of Scargle 2013). If gamma is specified, p0 is ignored. gamma : float, optional If specified, then use this gamma to compute the general prior form, :math:`p \\sim {\\tt gamma}^{N_{\\rm blocks}}`. If gamma is specified, p0 is ignored. ncp_prior : float, optional If specified, use the value of ``ncp_prior`` to compute the prior as above, using the definition :math:`{\\tt ncp\\_prior} = -\\ln({\\tt gamma})`. If ``ncp_prior`` is specified, ``gamma`` and ``p0`` are ignored. \"\"\" def __init__ ( self , dt : float , p0 : float = 0.05 , gamma : float | None = None , ncp_prior : float | None = None , ) -> None : self . dt = dt super () . __init__ ( p0 , gamma , ncp_prior ) def validate_input ( self , t : ArrayLike , x : ArrayLike | None = None , sigma : float | ArrayLike | None = None , ) -> tuple [ NDArray [ float ], NDArray [ float ], NDArray [ float ]]: t , x , sigma = super () . validate_input ( t , x , sigma ) if not np . all (( x == 0 ) | ( x == 1 )): raise ValueError ( \"Regular events must have only 0 and 1 in x\" ) return t , x , sigma def fitness ( self , T_k : NDArray [ float ], N_k : NDArray [ float ]) -> NDArray [ float ]: # Eq. C23 of Scargle 2013 M_k = T_k / self . dt N_over_M = N_k / M_k eps = 1e-8 if np . any ( N_over_M > 1 + eps ): warnings . warn ( \"regular events: N/M > 1. Is the time step correct?\" , AstropyUserWarning , ) one_m_NM = 1 - N_over_M N_over_M [ N_over_M <= 0 ] = 1 one_m_NM [ one_m_NM <= 0 ] = 1 return N_k * np . log ( N_over_M ) + ( M_k - N_k ) * np . log ( one_m_NM ) bayesian_blocks ( t , x = None , sigma = None , ex = None , fitness = 'events' , ** kwargs ) Compute optimal segmentation of data with Scargle's Bayesian Blocks. This is a flexible implementation of the Bayesian Blocks algorithm described in Scargle 2013 [1]_. Parameters t : array-like data times (one dimensional, length N) x : array-like, optional data values sigma : array-like or float, optional data errors fitness : str or object the fitness function to use for the model. If a string, the following options are supported: - 'events' : binned or unbinned event data. Arguments are ``gamma``, which gives the slope of the prior on the number of bins, or ``ncp_prior``, which is :math:`-\\ln({\\tt gamma})`. - 'regular_events' : non-overlapping events measured at multiples of a fundamental tick rate, ``dt``, which must be specified as an additional argument. Extra arguments are ``p0``, which gives the false alarm probability to compute the prior, or ``gamma``, which gives the slope of the prior on the number of bins, or ``ncp_prior``, which is :math:`-\\ln({\\tt gamma})`. - 'measures' : fitness for a measured sequence with Gaussian errors. Extra arguments are ``p0``, which gives the false alarm probability to compute the prior, or ``gamma``, which gives the slope of the prior on the number of bins, or ``ncp_prior``, which is :math:`-\\ln({\\tt gamma})`. In all three cases, if more than one of ``p0``, ``gamma``, and ``ncp_prior`` is chosen, ``ncp_prior`` takes precedence over ``gamma`` which takes precedence over ``p0``. Alternatively, the fitness parameter can be an instance of :class:`FitnessFunc` or a subclass thereof. **kwargs : any additional keyword arguments will be passed to the specified :class: FitnessFunc derived class. Returns edges : ndarray array containing the (N+1) edges defining the N bins Examples .. testsetup:: >>> np.random.seed(12345) Event data: t = np.random.normal(size=100) edges = bayesian_blocks(t, fitness='events', p0=0.01) Event data with repeats: t = np.random.normal(size=100) t[80:] = t[:20] edges = bayesian_blocks(t, fitness='events', p0=0.01) Regular event data: dt = 0.05 t = dt * np.arange(1000) x = np.zeros(len(t)) x[np.random.randint(0, len(t), len(t) // 10)] = 1 edges = bayesian_blocks(t, x, fitness='regular_events', dt=dt) Measured point data with errors: t = 100 * np.random.random(100) x = np.exp(-0.5 * (t - 50) ** 2) sigma = 0.1 x_obs = np.random.normal(x, sigma) edges = bayesian_blocks(t, x_obs, sigma, fitness='measures') References .. [1] Scargle, J et al. (2013) https://ui.adsabs.harvard.edu/abs/2013ApJ...764..167S .. [2] Bellman, R.E., Dreyfus, S.E., 1962. Applied Dynamic Programming. Princeton University Press, Princeton. https://press.princeton.edu/books/hardcover/9780691651873/applied-dynamic-programming .. [3] Bellman, R., Roth, R., 1969. Curve fitting by segmented straight lines. J. Amer. Statist. Assoc. 64, 1079\u20131084. https://www.tandfonline.com/doi/abs/10.1080/01621459.1969.10501038 See Also astropy.stats.histogram : compute a histogram using bayesian blocks Source code in scripts/expo_events.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 def bayesian_blocks ( t : ArrayLike , x : ArrayLike | None = None , sigma : ArrayLike | float | None = None , ex : ArrayLike | None = None , fitness : Literal [ \"events\" , \"regular_events\" , \"measures\" ] | FitnessFunc = \"events\" , ** kwargs , ) -> NDArray [ float ]: r \"\"\"Compute optimal segmentation of data with Scargle's Bayesian Blocks. This is a flexible implementation of the Bayesian Blocks algorithm described in Scargle 2013 [1]_. Parameters ---------- t : array-like data times (one dimensional, length N) x : array-like, optional data values sigma : array-like or float, optional data errors fitness : str or object the fitness function to use for the model. If a string, the following options are supported: - 'events' : binned or unbinned event data. Arguments are ``gamma``, which gives the slope of the prior on the number of bins, or ``ncp_prior``, which is :math:`-\\ln({\\tt gamma})`. - 'regular_events' : non-overlapping events measured at multiples of a fundamental tick rate, ``dt``, which must be specified as an additional argument. Extra arguments are ``p0``, which gives the false alarm probability to compute the prior, or ``gamma``, which gives the slope of the prior on the number of bins, or ``ncp_prior``, which is :math:`-\\ln({\\tt gamma})`. - 'measures' : fitness for a measured sequence with Gaussian errors. Extra arguments are ``p0``, which gives the false alarm probability to compute the prior, or ``gamma``, which gives the slope of the prior on the number of bins, or ``ncp_prior``, which is :math:`-\\ln({\\tt gamma})`. In all three cases, if more than one of ``p0``, ``gamma``, and ``ncp_prior`` is chosen, ``ncp_prior`` takes precedence over ``gamma`` which takes precedence over ``p0``. Alternatively, the fitness parameter can be an instance of :class:`FitnessFunc` or a subclass thereof. **kwargs : any additional keyword arguments will be passed to the specified :class:`FitnessFunc` derived class. Returns ------- edges : ndarray array containing the (N+1) edges defining the N bins Examples -------- .. testsetup:: >>> np.random.seed(12345) Event data: >>> t = np.random.normal(size=100) >>> edges = bayesian_blocks(t, fitness='events', p0=0.01) Event data with repeats: >>> t = np.random.normal(size=100) >>> t[80:] = t[:20] >>> edges = bayesian_blocks(t, fitness='events', p0=0.01) Regular event data: >>> dt = 0.05 >>> t = dt * np.arange(1000) >>> x = np.zeros(len(t)) >>> x[np.random.randint(0, len(t), len(t) // 10)] = 1 >>> edges = bayesian_blocks(t, x, fitness='regular_events', dt=dt) Measured point data with errors: >>> t = 100 * np.random.random(100) >>> x = np.exp(-0.5 * (t - 50) ** 2) >>> sigma = 0.1 >>> x_obs = np.random.normal(x, sigma) >>> edges = bayesian_blocks(t, x_obs, sigma, fitness='measures') References ---------- .. [1] Scargle, J et al. (2013) https://ui.adsabs.harvard.edu/abs/2013ApJ...764..167S .. [2] Bellman, R.E., Dreyfus, S.E., 1962. Applied Dynamic Programming. Princeton University Press, Princeton. https://press.princeton.edu/books/hardcover/9780691651873/applied-dynamic-programming .. [3] Bellman, R., Roth, R., 1969. Curve fitting by segmented straight lines. J. Amer. Statist. Assoc. 64, 1079\u20131084. https://www.tandfonline.com/doi/abs/10.1080/01621459.1969.10501038 See Also -------- astropy.stats.histogram : compute a histogram using bayesian blocks \"\"\" FITNESS_DICT = { \"events\" : Events , \"regular_events\" : RegularEvents , \"measures\" : PointMeasures , } fitness = FITNESS_DICT . get ( fitness , fitness ) if type ( fitness ) is type and issubclass ( fitness , FitnessFunc ): fitfunc = fitness ( ** kwargs ) elif isinstance ( fitness , FitnessFunc ): fitfunc = fitness else : raise ValueError ( \"fitness parameter not understood\" ) return fitfunc . fit ( t , x , sigma , ex )","title":"Bayesian block"},{"location":"bayesian_block/#scripts.expo_events--bayesian-blocks-for-time-series-analysis","text":"Dynamic programming algorithm for solving a piecewise-constant model for various datasets. This is based on the algorithm presented in Scargle et al 2013 [1] . This code was ported from the astroML project [2] . THIS IS AN ASTROPY PACKAGE EDITED BY GRACE SANGER-JOHNSON Applications include: finding an optimal histogram with adaptive bin widths finding optimal segmentation of time series data detecting inflection points in the rate of event data The primary interface to these routines is the :func: bayesian_blocks function. This module provides fitness functions suitable for three types of data: Irregularly-spaced event data via the :class: Events class Regularly-spaced event data via the :class: RegularEvents class Irregularly-spaced point measurements via the :class: PointMeasures class For more fine-tuned control over the fitness functions used, it is possible to define custom :class: FitnessFunc classes directly and use them with the :func: bayesian_blocks routine. One common application of the Bayesian Blocks algorithm is the determination of optimal adaptive-width histogram bins. This uses the same fitness function as for irregularly-spaced time series events. The easiest interface for creating Bayesian Blocks histograms is the :func: astropy.stats.histogram function.","title":"Bayesian Blocks for Time Series Analysis"},{"location":"bayesian_block/#scripts.expo_events--references","text":".. [1] https://ui.adsabs.harvard.edu/abs/2013ApJ...764..167S .. [2] https://www.astroml.org/ https://github.com//astroML/astroML/ .. [3] Bellman, R.E., Dreyfus, S.E., 1962. Applied Dynamic Programming. Princeton University Press, Princeton. https://press.princeton.edu/books/hardcover/9780691651873/applied-dynamic-programming .. [4] Bellman, R., Roth, R., 1969. Curve fitting by segmented straight lines. J. Amer. Statist. Assoc. 64, 1079\u20131084. https://www.tandfonline.com/doi/abs/10.1080/01621459.1969.10501038","title":"References"},{"location":"bayesian_block/#scripts.expo_events.Events","text":"Bases: FitnessFunc Bayesian blocks fitness for binned or unbinned events.","title":"Events"},{"location":"bayesian_block/#scripts.expo_events.Events--parameters","text":"p0 : float, optional False alarm probability, used to compute the prior on :math: N_{\\rm blocks} (see eq. 21 of Scargle 2013). For the Events type data, p0 does not seem to be an accurate representation of the actual false alarm probability. If you are using this fitness function for a triggering type condition, it is recommended that you run statistical trials on signal-free noise to determine an appropriate value of gamma or ncp_prior to use for a desired false alarm rate. gamma : float, optional If specified, then use this gamma to compute the general prior form, :math: p \\sim {\\tt gamma}^{N_{\\rm blocks}} . If gamma is specified, p0 is ignored. ncp_prior : float, optional If specified, use the value of ncp_prior to compute the prior as above, using the definition :math: {\\tt ncp\\_prior} = -\\ln({\\tt gamma}) . If ncp_prior is specified, gamma and p0 is ignored. Source code in scripts/expo_events.py 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 class Events ( FitnessFunc ): r \"\"\"Bayesian blocks fitness for binned or unbinned events. Parameters ---------- p0 : float, optional False alarm probability, used to compute the prior on :math:`N_{\\rm blocks}` (see eq. 21 of Scargle 2013). For the Events type data, ``p0`` does not seem to be an accurate representation of the actual false alarm probability. If you are using this fitness function for a triggering type condition, it is recommended that you run statistical trials on signal-free noise to determine an appropriate value of ``gamma`` or ``ncp_prior`` to use for a desired false alarm rate. gamma : float, optional If specified, then use this gamma to compute the general prior form, :math:`p \\sim {\\tt gamma}^{N_{\\rm blocks}}`. If gamma is specified, p0 is ignored. ncp_prior : float, optional If specified, use the value of ``ncp_prior`` to compute the prior as above, using the definition :math:`{\\tt ncp\\_prior} = -\\ln({\\tt gamma})`. If ``ncp_prior`` is specified, ``gamma`` and ``p0`` is ignored. \"\"\" def fitness ( self , N_k : NDArray [ float ], T_k : NDArray [ float ]) -> NDArray [ float ]: # Implement Eq. 19 from Scargle (2013), i.e., N_k * ln(N_k / T_k). # Note that when N_k -> 0, the limit of N_k * ln(N_k / T_k) is 0. # N_k is guaranteed to be non-negative integers by the `validate_input` # method, so no need to check for negative values here. # First, initialize an array of zeros to store the fitness values, # then calculate the fitness values only where N_k > 0. # For N_k == 0, the corresponding fitness values are zero already. out = np . zeros ( N_k . shape ) mask = N_k > 0 rate = np . divide ( N_k , T_k , out = out , where = mask ) ln_rate = np . log ( rate , out = out , where = mask ) return np . multiply ( N_k , ln_rate , out = out , where = mask ) def validate_input ( self , t : ArrayLike , x : ArrayLike | None , sigma : float | ArrayLike | None , ex : ArrayLike | None , ) -> tuple [ NDArray [ float ], NDArray [ float ], NDArray [ float ]]: t , x , sigma , ex = super () . validate_input ( t , x , sigma , ex ) if ( x is not None ) and ( np . any ( x % 1 > 0 ) or np . any ( x < 0 )): raise ValueError ( \"x must be non-negative integer counts for fitness='events'\" ) return t , x , sigma , ex","title":"Parameters"},{"location":"bayesian_block/#scripts.expo_events.FitnessFunc","text":"Base class for bayesian blocks fitness functions. Derived classes should overload the following method: fitness(self, **kwargs) : Compute the fitness given a set of named arguments. Arguments accepted by fitness must be among [T_k, N_k, a_k, b_k, c_k] (See [1]_ for details on the meaning of these parameters). Additionally, other methods may be overloaded as well: __init__(self, **kwargs) : Initialize the fitness function with any parameters beyond the normal p0 and gamma . validate_input(self, t, x, sigma) : Enable specific checks of the input data ( t , x , sigma ) to be performed prior to the fit. compute_ncp_prior(self, N) : If ncp_prior is not defined explicitly, this function is called in order to define it before fitting. This may be calculated from gamma , p0 , or whatever method you choose. p0_prior(self, N) : Specify the form of the prior given the false-alarm probability p0 (See [1]_ for details). For examples of implemented fitness functions, see :class: Events , :class: RegularEvents , and :class: PointMeasures .","title":"FitnessFunc"},{"location":"bayesian_block/#scripts.expo_events.FitnessFunc--references","text":".. [1] Scargle, J et al. (2013) https://ui.adsabs.harvard.edu/abs/2013ApJ...764..167S Source code in scripts/expo_events.py 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 class FitnessFunc : \"\"\"Base class for bayesian blocks fitness functions. Derived classes should overload the following method: ``fitness(self, **kwargs)``: Compute the fitness given a set of named arguments. Arguments accepted by fitness must be among ``[T_k, N_k, a_k, b_k, c_k]`` (See [1]_ for details on the meaning of these parameters). Additionally, other methods may be overloaded as well: ``__init__(self, **kwargs)``: Initialize the fitness function with any parameters beyond the normal ``p0`` and ``gamma``. ``validate_input(self, t, x, sigma)``: Enable specific checks of the input data (``t``, ``x``, ``sigma``) to be performed prior to the fit. ``compute_ncp_prior(self, N)``: If ``ncp_prior`` is not defined explicitly, this function is called in order to define it before fitting. This may be calculated from ``gamma``, ``p0``, or whatever method you choose. ``p0_prior(self, N)``: Specify the form of the prior given the false-alarm probability ``p0`` (See [1]_ for details). For examples of implemented fitness functions, see :class:`Events`, :class:`RegularEvents`, and :class:`PointMeasures`. References ---------- .. [1] Scargle, J et al. (2013) https://ui.adsabs.harvard.edu/abs/2013ApJ...764..167S \"\"\" def __init__ ( self , p0 : float = 0.05 , gamma : float | None = None , ncp_prior : float | None = None , ) -> None : self . p0 = p0 self . gamma = gamma self . ncp_prior = ncp_prior def validate_input ( self , t : ArrayLike , x : ArrayLike | None = None , sigma : float | ArrayLike | None = None , ex : ArrayLike | None = None , ) -> tuple [ NDArray [ float ], NDArray [ float ], NDArray [ float ], NDArray [ float ]]: \"\"\"Validate inputs to the model. Parameters ---------- t : array-like times of observations x : array-like, optional values observed at each time sigma : float or array-like, optional errors in values x ex : array-like, optional Returns ------- t, x, sigma, ex : array-like, float validated and perhaps modified versions of inputs \"\"\" # validate array input t = np . asarray ( t , dtype = float ) # find unique values of t t = np . array ( t ) if t . ndim != 1 : raise ValueError ( \"t must be a one-dimensional array\" ) unq_t , unq_ind , unq_inv = np . unique ( t , return_index = True , return_inverse = True ) # if x is not specified, x will be counts at each time if x is None : if sigma is not None : raise ValueError ( \"If sigma is specified, x must be specified\" ) else : sigma = 1.0 if len ( unq_t ) == len ( t ): x = np . ones_like ( t ) else : x = np . bincount ( unq_inv ) t = unq_t # if x is specified, then we need to simultaneously sort t and x else : # TODO: allow broadcasted x? x = np . asarray ( x , dtype = float ) if x . shape not in [(), ( 1 ,), ( t . size ,)]: raise ValueError ( \"x does not match shape of t\" ) x += np . zeros_like ( t ) if len ( unq_t ) != len ( t ): raise ValueError ( \"Repeated values in t not supported when x is specified\" ) t = unq_t x = x [ unq_ind ] # verify the given sigma value if sigma is None : sigma = 1.0 else : sigma = np . asarray ( sigma , dtype = float ) if sigma . shape not in [(), ( 1 ,), ( t . size ,)]: raise ValueError ( \"sigma does not match the shape of x\" ) # validate array input ex = np . asarray ( ex , dtype = float ) # if ex is not specified, x will be counts at each time if ex is None : if len ( unq_t ) == len ( t ): ex = np . ones_like ( t ) else : ex = np . bincount ( unq_inv ) return t , x , sigma , ex def fitness ( self , ** kwargs ): raise NotImplementedError () def p0_prior ( self , N : int ) -> float : \"\"\"Empirical prior, parametrized by the false alarm probability ``p0``. See eq. 21 in Scargle (2013). Note that there was an error in this equation in the original Scargle paper (the \"log\" was missing). The following corrected form is taken from https://arxiv.org/abs/1304.2818 \"\"\" return 4 - np . log ( 73.53 * self . p0 * ( N **- 0.478 )) # the fitness_args property will return the list of arguments accepted by # the method fitness(). This allows more efficient computation below. @property def _fitness_args ( self ) -> KeysView [ str ]: return signature ( self . fitness ) . parameters . keys () def compute_ncp_prior ( self , N : int ) -> float : \"\"\" If ``ncp_prior`` is not explicitly defined, compute it from ``gamma`` or ``p0``. \"\"\" if self . gamma is not None : return - np . log ( self . gamma ) elif self . p0 is not None : return self . p0_prior ( N ) else : raise ValueError ( \"``ncp_prior`` cannot be computed as neither \" \"``gamma`` nor ``p0`` is defined.\" ) def fit ( self , t : ArrayLike , x : ArrayLike | None = None , sigma : ArrayLike | float | None = None , ex : ArrayLike | None = None , ) -> NDArray [ float ]: \"\"\"Fit the Bayesian Blocks model given the specified fitness function. Parameters ---------- t : array-like data times (one dimensional, length N) x : array-like, optional data values sigma : array-like or float, optional data errors ex : array-like, optional Returns ------- edges : ndarray array containing the (M+1) edges defining the M optimal bins \"\"\" t , x , sigma , ex = self . validate_input ( t , x , sigma , ex ) # compute values needed for computation, below if \"a_k\" in self . _fitness_args : ak_raw = np . ones_like ( x ) / sigma ** 2 if \"b_k\" in self . _fitness_args : bk_raw = x / sigma ** 2 if \"c_k\" in self . _fitness_args : ck_raw = x * x / sigma ** 2 # create length-(N + 1) array of cell edges # edges = np.concatenate([t[:1], 0.5 * (t[1:] + t[:-1]), t[-1:]]) #ORIGINAL nbpts = len ( t ) edges = np . concatenate ( [ [ 0.5 * ( t [ 0 ] + t [ 1 ]) - t [ 0 ]], ( np . diff ( t [ 1 : nbpts ]) + np . diff ( t [ 0 : nbpts - 1 ])) / 2.0 , [ t [ - 1 ] - 0.5 * ( t [ nbpts - 2 ] + t [ nbpts - 1 ])], ] ) edges *= ex # edges = np.concatenate([t[:1], 0.5 * (t[1:] + t[:-1])]) # edges=np.concatenate([t[:1]*ex[:1], (0.5 * (t[1:] + t[:-1]))*(ex[-1]), t[-1:]*ex[-1:]]) # block_length = (t[-1] - edges) #ORIGINAL block_length = np . sum ( edges ) - np . concatenate (([ 0.0 ], np . cumsum ( edges ))) # arrays to store the best configuration N = len ( t ) best = np . zeros ( N , dtype = float ) last = np . zeros ( N , dtype = int ) # Compute ncp_prior if not defined if self . ncp_prior is None : ncp_prior = self . compute_ncp_prior ( N ) else : ncp_prior = self . ncp_prior # ---------------------------------------------------------------- # Start with first data cell; add one cell at each iteration # ---------------------------------------------------------------- for R in range ( N ): # Compute fit_vec : fitness of putative last block (end at R) kwds = {} # T_k: width/duration of each block if \"T_k\" in self . _fitness_args : kwds [ \"T_k\" ] = block_length [: ( R + 1 )] - block_length [ R + 1 ] # N_k: number of elements in each block if \"N_k\" in self . _fitness_args : kwds [ \"N_k\" ] = np . cumsum ( x [: ( R + 1 )][:: - 1 ])[:: - 1 ] # a_k: eq. 31 if \"a_k\" in self . _fitness_args : kwds [ \"a_k\" ] = 0.5 * np . cumsum ( ak_raw [: ( R + 1 )][:: - 1 ])[:: - 1 ] # b_k: eq. 32 if \"b_k\" in self . _fitness_args : kwds [ \"b_k\" ] = - np . cumsum ( bk_raw [: ( R + 1 )][:: - 1 ])[:: - 1 ] # c_k: eq. 33 if \"c_k\" in self . _fitness_args : kwds [ \"c_k\" ] = 0.5 * np . cumsum ( ck_raw [: ( R + 1 )][:: - 1 ])[:: - 1 ] # evaluate fitness function fit_vec = self . fitness ( ** kwds ) A_R = fit_vec - ncp_prior A_R [ 1 :] += best [: R ] i_max = np . argmax ( A_R ) last [ R ] = i_max best [ R ] = A_R [ i_max ] # ---------------------------------------------------------------- # Now find changepoints by iteratively peeling off the last block # ---------------------------------------------------------------- change_points = np . zeros ( N , dtype = int ) i_cp = N ind = N while i_cp > 0 : i_cp -= 1 change_points [ i_cp ] = ind if ind == 0 : break ind = last [ ind - 1 ] if i_cp == 0 : change_points [ i_cp ] = 0 change_points = change_points [ i_cp :] # the last one will be the end of the array so make end of array change_points [ - 1 ] -= 1 return t [ change_points ]","title":"References"},{"location":"bayesian_block/#scripts.expo_events.FitnessFunc.compute_ncp_prior","text":"If ncp_prior is not explicitly defined, compute it from gamma or p0 . Source code in scripts/expo_events.py 348 349 350 351 352 353 354 355 356 357 358 359 360 361 def compute_ncp_prior ( self , N : int ) -> float : \"\"\" If ``ncp_prior`` is not explicitly defined, compute it from ``gamma`` or ``p0``. \"\"\" if self . gamma is not None : return - np . log ( self . gamma ) elif self . p0 is not None : return self . p0_prior ( N ) else : raise ValueError ( \"``ncp_prior`` cannot be computed as neither \" \"``gamma`` nor ``p0`` is defined.\" )","title":"compute_ncp_prior"},{"location":"bayesian_block/#scripts.expo_events.FitnessFunc.fit","text":"Fit the Bayesian Blocks model given the specified fitness function.","title":"fit"},{"location":"bayesian_block/#scripts.expo_events.FitnessFunc.fit--parameters","text":"t : array-like data times (one dimensional, length N) x : array-like, optional data values sigma : array-like or float, optional data errors ex : array-like, optional","title":"Parameters"},{"location":"bayesian_block/#scripts.expo_events.FitnessFunc.fit--returns","text":"edges : ndarray array containing the (M+1) edges defining the M optimal bins Source code in scripts/expo_events.py 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 def fit ( self , t : ArrayLike , x : ArrayLike | None = None , sigma : ArrayLike | float | None = None , ex : ArrayLike | None = None , ) -> NDArray [ float ]: \"\"\"Fit the Bayesian Blocks model given the specified fitness function. Parameters ---------- t : array-like data times (one dimensional, length N) x : array-like, optional data values sigma : array-like or float, optional data errors ex : array-like, optional Returns ------- edges : ndarray array containing the (M+1) edges defining the M optimal bins \"\"\" t , x , sigma , ex = self . validate_input ( t , x , sigma , ex ) # compute values needed for computation, below if \"a_k\" in self . _fitness_args : ak_raw = np . ones_like ( x ) / sigma ** 2 if \"b_k\" in self . _fitness_args : bk_raw = x / sigma ** 2 if \"c_k\" in self . _fitness_args : ck_raw = x * x / sigma ** 2 # create length-(N + 1) array of cell edges # edges = np.concatenate([t[:1], 0.5 * (t[1:] + t[:-1]), t[-1:]]) #ORIGINAL nbpts = len ( t ) edges = np . concatenate ( [ [ 0.5 * ( t [ 0 ] + t [ 1 ]) - t [ 0 ]], ( np . diff ( t [ 1 : nbpts ]) + np . diff ( t [ 0 : nbpts - 1 ])) / 2.0 , [ t [ - 1 ] - 0.5 * ( t [ nbpts - 2 ] + t [ nbpts - 1 ])], ] ) edges *= ex # edges = np.concatenate([t[:1], 0.5 * (t[1:] + t[:-1])]) # edges=np.concatenate([t[:1]*ex[:1], (0.5 * (t[1:] + t[:-1]))*(ex[-1]), t[-1:]*ex[-1:]]) # block_length = (t[-1] - edges) #ORIGINAL block_length = np . sum ( edges ) - np . concatenate (([ 0.0 ], np . cumsum ( edges ))) # arrays to store the best configuration N = len ( t ) best = np . zeros ( N , dtype = float ) last = np . zeros ( N , dtype = int ) # Compute ncp_prior if not defined if self . ncp_prior is None : ncp_prior = self . compute_ncp_prior ( N ) else : ncp_prior = self . ncp_prior # ---------------------------------------------------------------- # Start with first data cell; add one cell at each iteration # ---------------------------------------------------------------- for R in range ( N ): # Compute fit_vec : fitness of putative last block (end at R) kwds = {} # T_k: width/duration of each block if \"T_k\" in self . _fitness_args : kwds [ \"T_k\" ] = block_length [: ( R + 1 )] - block_length [ R + 1 ] # N_k: number of elements in each block if \"N_k\" in self . _fitness_args : kwds [ \"N_k\" ] = np . cumsum ( x [: ( R + 1 )][:: - 1 ])[:: - 1 ] # a_k: eq. 31 if \"a_k\" in self . _fitness_args : kwds [ \"a_k\" ] = 0.5 * np . cumsum ( ak_raw [: ( R + 1 )][:: - 1 ])[:: - 1 ] # b_k: eq. 32 if \"b_k\" in self . _fitness_args : kwds [ \"b_k\" ] = - np . cumsum ( bk_raw [: ( R + 1 )][:: - 1 ])[:: - 1 ] # c_k: eq. 33 if \"c_k\" in self . _fitness_args : kwds [ \"c_k\" ] = 0.5 * np . cumsum ( ck_raw [: ( R + 1 )][:: - 1 ])[:: - 1 ] # evaluate fitness function fit_vec = self . fitness ( ** kwds ) A_R = fit_vec - ncp_prior A_R [ 1 :] += best [: R ] i_max = np . argmax ( A_R ) last [ R ] = i_max best [ R ] = A_R [ i_max ] # ---------------------------------------------------------------- # Now find changepoints by iteratively peeling off the last block # ---------------------------------------------------------------- change_points = np . zeros ( N , dtype = int ) i_cp = N ind = N while i_cp > 0 : i_cp -= 1 change_points [ i_cp ] = ind if ind == 0 : break ind = last [ ind - 1 ] if i_cp == 0 : change_points [ i_cp ] = 0 change_points = change_points [ i_cp :] # the last one will be the end of the array so make end of array change_points [ - 1 ] -= 1 return t [ change_points ]","title":"Returns"},{"location":"bayesian_block/#scripts.expo_events.FitnessFunc.p0_prior","text":"Empirical prior, parametrized by the false alarm probability p0 . See eq. 21 in Scargle (2013). Note that there was an error in this equation in the original Scargle paper (the \"log\" was missing). The following corrected form is taken from https://arxiv.org/abs/1304.2818 Source code in scripts/expo_events.py 331 332 333 334 335 336 337 338 339 340 def p0_prior ( self , N : int ) -> float : \"\"\"Empirical prior, parametrized by the false alarm probability ``p0``. See eq. 21 in Scargle (2013). Note that there was an error in this equation in the original Scargle paper (the \"log\" was missing). The following corrected form is taken from https://arxiv.org/abs/1304.2818 \"\"\" return 4 - np . log ( 73.53 * self . p0 * ( N **- 0.478 ))","title":"p0_prior"},{"location":"bayesian_block/#scripts.expo_events.FitnessFunc.validate_input","text":"Validate inputs to the model.","title":"validate_input"},{"location":"bayesian_block/#scripts.expo_events.FitnessFunc.validate_input--parameters","text":"t : array-like times of observations x : array-like, optional values observed at each time sigma : float or array-like, optional errors in values x ex : array-like, optional","title":"Parameters"},{"location":"bayesian_block/#scripts.expo_events.FitnessFunc.validate_input--returns","text":"t, x, sigma, ex : array-like, float validated and perhaps modified versions of inputs Source code in scripts/expo_events.py 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 def validate_input ( self , t : ArrayLike , x : ArrayLike | None = None , sigma : float | ArrayLike | None = None , ex : ArrayLike | None = None , ) -> tuple [ NDArray [ float ], NDArray [ float ], NDArray [ float ], NDArray [ float ]]: \"\"\"Validate inputs to the model. Parameters ---------- t : array-like times of observations x : array-like, optional values observed at each time sigma : float or array-like, optional errors in values x ex : array-like, optional Returns ------- t, x, sigma, ex : array-like, float validated and perhaps modified versions of inputs \"\"\" # validate array input t = np . asarray ( t , dtype = float ) # find unique values of t t = np . array ( t ) if t . ndim != 1 : raise ValueError ( \"t must be a one-dimensional array\" ) unq_t , unq_ind , unq_inv = np . unique ( t , return_index = True , return_inverse = True ) # if x is not specified, x will be counts at each time if x is None : if sigma is not None : raise ValueError ( \"If sigma is specified, x must be specified\" ) else : sigma = 1.0 if len ( unq_t ) == len ( t ): x = np . ones_like ( t ) else : x = np . bincount ( unq_inv ) t = unq_t # if x is specified, then we need to simultaneously sort t and x else : # TODO: allow broadcasted x? x = np . asarray ( x , dtype = float ) if x . shape not in [(), ( 1 ,), ( t . size ,)]: raise ValueError ( \"x does not match shape of t\" ) x += np . zeros_like ( t ) if len ( unq_t ) != len ( t ): raise ValueError ( \"Repeated values in t not supported when x is specified\" ) t = unq_t x = x [ unq_ind ] # verify the given sigma value if sigma is None : sigma = 1.0 else : sigma = np . asarray ( sigma , dtype = float ) if sigma . shape not in [(), ( 1 ,), ( t . size ,)]: raise ValueError ( \"sigma does not match the shape of x\" ) # validate array input ex = np . asarray ( ex , dtype = float ) # if ex is not specified, x will be counts at each time if ex is None : if len ( unq_t ) == len ( t ): ex = np . ones_like ( t ) else : ex = np . bincount ( unq_inv ) return t , x , sigma , ex","title":"Returns"},{"location":"bayesian_block/#scripts.expo_events.PointMeasures","text":"Bases: FitnessFunc Bayesian blocks fitness for point measures.","title":"PointMeasures"},{"location":"bayesian_block/#scripts.expo_events.PointMeasures--parameters","text":"p0 : float, optional False alarm probability, used to compute the prior on :math: N_{\\rm blocks} (see eq. 21 of Scargle 2013). If gamma is specified, p0 is ignored. gamma : float, optional If specified, then use this gamma to compute the general prior form, :math: p \\sim {\\tt gamma}^{N_{\\rm blocks}} . If gamma is specified, p0 is ignored. ncp_prior : float, optional If specified, use the value of ncp_prior to compute the prior as above, using the definition :math: {\\tt ncp\\_prior} = -\\ln({\\tt gamma}) . If ncp_prior is specified, gamma and p0 are ignored. Source code in scripts/expo_events.py 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 class PointMeasures ( FitnessFunc ): r \"\"\"Bayesian blocks fitness for point measures. Parameters ---------- p0 : float, optional False alarm probability, used to compute the prior on :math:`N_{\\rm blocks}` (see eq. 21 of Scargle 2013). If gamma is specified, p0 is ignored. gamma : float, optional If specified, then use this gamma to compute the general prior form, :math:`p \\sim {\\tt gamma}^{N_{\\rm blocks}}`. If gamma is specified, p0 is ignored. ncp_prior : float, optional If specified, use the value of ``ncp_prior`` to compute the prior as above, using the definition :math:`{\\tt ncp\\_prior} = -\\ln({\\tt gamma})`. If ``ncp_prior`` is specified, ``gamma`` and ``p0`` are ignored. \"\"\" def __init__ ( self , p0 : float = 0.05 , gamma : float | None = None , ncp_prior : float | None = None , ) -> None : super () . __init__ ( p0 , gamma , ncp_prior ) def fitness ( self , a_k : NDArray [ float ], b_k : ArrayLike ) -> NDArray [ float ]: # eq. 41 from Scargle 2013 return ( b_k * b_k ) / ( 4 * a_k ) def validate_input ( self , t : ArrayLike , x : ArrayLike | None , sigma : float | ArrayLike | None , ) -> tuple [ NDArray [ float ], NDArray [ float ], NDArray [ float ]]: if x is None : raise ValueError ( \"x must be specified for point measures\" ) return super () . validate_input ( t , x , sigma )","title":"Parameters"},{"location":"bayesian_block/#scripts.expo_events.RegularEvents","text":"Bases: FitnessFunc Bayesian blocks fitness for regular events. This is for data which has a fundamental \"tick\" length, so that all measured values are multiples of this tick length. In each tick, there are either zero or one counts.","title":"RegularEvents"},{"location":"bayesian_block/#scripts.expo_events.RegularEvents--parameters","text":"dt : float tick rate for data p0 : float, optional False alarm probability, used to compute the prior on :math: N_{\\rm blocks} (see eq. 21 of Scargle 2013). If gamma is specified, p0 is ignored. gamma : float, optional If specified, then use this gamma to compute the general prior form, :math: p \\sim {\\tt gamma}^{N_{\\rm blocks}} . If gamma is specified, p0 is ignored. ncp_prior : float, optional If specified, use the value of ncp_prior to compute the prior as above, using the definition :math: {\\tt ncp\\_prior} = -\\ln({\\tt gamma}) . If ncp_prior is specified, gamma and p0 are ignored. Source code in scripts/expo_events.py 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 class RegularEvents ( FitnessFunc ): r \"\"\"Bayesian blocks fitness for regular events. This is for data which has a fundamental \"tick\" length, so that all measured values are multiples of this tick length. In each tick, there are either zero or one counts. Parameters ---------- dt : float tick rate for data p0 : float, optional False alarm probability, used to compute the prior on :math:`N_{\\rm blocks}` (see eq. 21 of Scargle 2013). If gamma is specified, p0 is ignored. gamma : float, optional If specified, then use this gamma to compute the general prior form, :math:`p \\sim {\\tt gamma}^{N_{\\rm blocks}}`. If gamma is specified, p0 is ignored. ncp_prior : float, optional If specified, use the value of ``ncp_prior`` to compute the prior as above, using the definition :math:`{\\tt ncp\\_prior} = -\\ln({\\tt gamma})`. If ``ncp_prior`` is specified, ``gamma`` and ``p0`` are ignored. \"\"\" def __init__ ( self , dt : float , p0 : float = 0.05 , gamma : float | None = None , ncp_prior : float | None = None , ) -> None : self . dt = dt super () . __init__ ( p0 , gamma , ncp_prior ) def validate_input ( self , t : ArrayLike , x : ArrayLike | None = None , sigma : float | ArrayLike | None = None , ) -> tuple [ NDArray [ float ], NDArray [ float ], NDArray [ float ]]: t , x , sigma = super () . validate_input ( t , x , sigma ) if not np . all (( x == 0 ) | ( x == 1 )): raise ValueError ( \"Regular events must have only 0 and 1 in x\" ) return t , x , sigma def fitness ( self , T_k : NDArray [ float ], N_k : NDArray [ float ]) -> NDArray [ float ]: # Eq. C23 of Scargle 2013 M_k = T_k / self . dt N_over_M = N_k / M_k eps = 1e-8 if np . any ( N_over_M > 1 + eps ): warnings . warn ( \"regular events: N/M > 1. Is the time step correct?\" , AstropyUserWarning , ) one_m_NM = 1 - N_over_M N_over_M [ N_over_M <= 0 ] = 1 one_m_NM [ one_m_NM <= 0 ] = 1 return N_k * np . log ( N_over_M ) + ( M_k - N_k ) * np . log ( one_m_NM )","title":"Parameters"},{"location":"bayesian_block/#scripts.expo_events.bayesian_blocks","text":"Compute optimal segmentation of data with Scargle's Bayesian Blocks. This is a flexible implementation of the Bayesian Blocks algorithm described in Scargle 2013 [1]_.","title":"bayesian_blocks"},{"location":"bayesian_block/#scripts.expo_events.bayesian_blocks--parameters","text":"t : array-like data times (one dimensional, length N) x : array-like, optional data values sigma : array-like or float, optional data errors fitness : str or object the fitness function to use for the model. If a string, the following options are supported: - 'events' : binned or unbinned event data. Arguments are ``gamma``, which gives the slope of the prior on the number of bins, or ``ncp_prior``, which is :math:`-\\ln({\\tt gamma})`. - 'regular_events' : non-overlapping events measured at multiples of a fundamental tick rate, ``dt``, which must be specified as an additional argument. Extra arguments are ``p0``, which gives the false alarm probability to compute the prior, or ``gamma``, which gives the slope of the prior on the number of bins, or ``ncp_prior``, which is :math:`-\\ln({\\tt gamma})`. - 'measures' : fitness for a measured sequence with Gaussian errors. Extra arguments are ``p0``, which gives the false alarm probability to compute the prior, or ``gamma``, which gives the slope of the prior on the number of bins, or ``ncp_prior``, which is :math:`-\\ln({\\tt gamma})`. In all three cases, if more than one of ``p0``, ``gamma``, and ``ncp_prior`` is chosen, ``ncp_prior`` takes precedence over ``gamma`` which takes precedence over ``p0``. Alternatively, the fitness parameter can be an instance of :class:`FitnessFunc` or a subclass thereof. **kwargs : any additional keyword arguments will be passed to the specified :class: FitnessFunc derived class.","title":"Parameters"},{"location":"bayesian_block/#scripts.expo_events.bayesian_blocks--returns","text":"edges : ndarray array containing the (N+1) edges defining the N bins","title":"Returns"},{"location":"bayesian_block/#scripts.expo_events.bayesian_blocks--examples","text":".. testsetup:: >>> np.random.seed(12345) Event data: t = np.random.normal(size=100) edges = bayesian_blocks(t, fitness='events', p0=0.01) Event data with repeats: t = np.random.normal(size=100) t[80:] = t[:20] edges = bayesian_blocks(t, fitness='events', p0=0.01) Regular event data: dt = 0.05 t = dt * np.arange(1000) x = np.zeros(len(t)) x[np.random.randint(0, len(t), len(t) // 10)] = 1 edges = bayesian_blocks(t, x, fitness='regular_events', dt=dt) Measured point data with errors: t = 100 * np.random.random(100) x = np.exp(-0.5 * (t - 50) ** 2) sigma = 0.1 x_obs = np.random.normal(x, sigma) edges = bayesian_blocks(t, x_obs, sigma, fitness='measures')","title":"Examples"},{"location":"bayesian_block/#scripts.expo_events.bayesian_blocks--references","text":".. [1] Scargle, J et al. (2013) https://ui.adsabs.harvard.edu/abs/2013ApJ...764..167S .. [2] Bellman, R.E., Dreyfus, S.E., 1962. Applied Dynamic Programming. Princeton University Press, Princeton. https://press.princeton.edu/books/hardcover/9780691651873/applied-dynamic-programming .. [3] Bellman, R., Roth, R., 1969. Curve fitting by segmented straight lines. J. Amer. Statist. Assoc. 64, 1079\u20131084. https://www.tandfonline.com/doi/abs/10.1080/01621459.1969.10501038","title":"References"},{"location":"bayesian_block/#scripts.expo_events.bayesian_blocks--see-also","text":"astropy.stats.histogram : compute a histogram using bayesian blocks Source code in scripts/expo_events.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 def bayesian_blocks ( t : ArrayLike , x : ArrayLike | None = None , sigma : ArrayLike | float | None = None , ex : ArrayLike | None = None , fitness : Literal [ \"events\" , \"regular_events\" , \"measures\" ] | FitnessFunc = \"events\" , ** kwargs , ) -> NDArray [ float ]: r \"\"\"Compute optimal segmentation of data with Scargle's Bayesian Blocks. This is a flexible implementation of the Bayesian Blocks algorithm described in Scargle 2013 [1]_. Parameters ---------- t : array-like data times (one dimensional, length N) x : array-like, optional data values sigma : array-like or float, optional data errors fitness : str or object the fitness function to use for the model. If a string, the following options are supported: - 'events' : binned or unbinned event data. Arguments are ``gamma``, which gives the slope of the prior on the number of bins, or ``ncp_prior``, which is :math:`-\\ln({\\tt gamma})`. - 'regular_events' : non-overlapping events measured at multiples of a fundamental tick rate, ``dt``, which must be specified as an additional argument. Extra arguments are ``p0``, which gives the false alarm probability to compute the prior, or ``gamma``, which gives the slope of the prior on the number of bins, or ``ncp_prior``, which is :math:`-\\ln({\\tt gamma})`. - 'measures' : fitness for a measured sequence with Gaussian errors. Extra arguments are ``p0``, which gives the false alarm probability to compute the prior, or ``gamma``, which gives the slope of the prior on the number of bins, or ``ncp_prior``, which is :math:`-\\ln({\\tt gamma})`. In all three cases, if more than one of ``p0``, ``gamma``, and ``ncp_prior`` is chosen, ``ncp_prior`` takes precedence over ``gamma`` which takes precedence over ``p0``. Alternatively, the fitness parameter can be an instance of :class:`FitnessFunc` or a subclass thereof. **kwargs : any additional keyword arguments will be passed to the specified :class:`FitnessFunc` derived class. Returns ------- edges : ndarray array containing the (N+1) edges defining the N bins Examples -------- .. testsetup:: >>> np.random.seed(12345) Event data: >>> t = np.random.normal(size=100) >>> edges = bayesian_blocks(t, fitness='events', p0=0.01) Event data with repeats: >>> t = np.random.normal(size=100) >>> t[80:] = t[:20] >>> edges = bayesian_blocks(t, fitness='events', p0=0.01) Regular event data: >>> dt = 0.05 >>> t = dt * np.arange(1000) >>> x = np.zeros(len(t)) >>> x[np.random.randint(0, len(t), len(t) // 10)] = 1 >>> edges = bayesian_blocks(t, x, fitness='regular_events', dt=dt) Measured point data with errors: >>> t = 100 * np.random.random(100) >>> x = np.exp(-0.5 * (t - 50) ** 2) >>> sigma = 0.1 >>> x_obs = np.random.normal(x, sigma) >>> edges = bayesian_blocks(t, x_obs, sigma, fitness='measures') References ---------- .. [1] Scargle, J et al. (2013) https://ui.adsabs.harvard.edu/abs/2013ApJ...764..167S .. [2] Bellman, R.E., Dreyfus, S.E., 1962. Applied Dynamic Programming. Princeton University Press, Princeton. https://press.princeton.edu/books/hardcover/9780691651873/applied-dynamic-programming .. [3] Bellman, R., Roth, R., 1969. Curve fitting by segmented straight lines. J. Amer. Statist. Assoc. 64, 1079\u20131084. https://www.tandfonline.com/doi/abs/10.1080/01621459.1969.10501038 See Also -------- astropy.stats.histogram : compute a histogram using bayesian blocks \"\"\" FITNESS_DICT = { \"events\" : Events , \"regular_events\" : RegularEvents , \"measures\" : PointMeasures , } fitness = FITNESS_DICT . get ( fitness , fitness ) if type ( fitness ) is type and issubclass ( fitness , FitnessFunc ): fitfunc = fitness ( ** kwargs ) elif isinstance ( fitness , FitnessFunc ): fitfunc = fitness else : raise ValueError ( \"fitness parameter not understood\" ) return fitfunc . fit ( t , x , sigma , ex )","title":"See Also"},{"location":"calculate_average_rate/","text":"calculate_average_rate ( events , gti , flare_gti = None , calculate_average_count_rate = False ) Calculate the average count rate during GTI intervals, optionally filtered by flare GTI. Parameters: Name Type Description Default events DataFrame Unified photon event DataFrame with 'TIME' and 'Exposure' columns. required gti DataFrame Common GTI DataFrame with 'START' and 'STOP' columns. required flare_gti DataFrame Flare GTI DataFrame with 'START' and 'STOP' columns. None calculate_average_count_rate bool Whether to calculate average count rates. False Returns: Type Description DataFrame Summary DataFrame with 'START', 'STOP', and 'Count Rate'. Source code in scripts/calculate_average_rate.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def calculate_average_rate ( events : pd . DataFrame , gti : pd . DataFrame , flare_gti = None , calculate_average_count_rate = False , ): \"\"\" Calculate the average count rate during GTI intervals, optionally filtered by flare GTI. Parameters: events (pd.DataFrame): Unified photon event DataFrame with 'TIME' and 'Exposure' columns. gti (pd.DataFrame): Common GTI DataFrame with 'START' and 'STOP' columns. flare_gti (pd.DataFrame, optional): Flare GTI DataFrame with 'START' and 'STOP' columns. calculate_average_count_rate (bool): Whether to calculate average count rates. Returns: (pd.DataFrame): Summary DataFrame with 'START', 'STOP', and 'Count Rate'. \"\"\" if not calculate_average_count_rate : print ( \"Skipping average count rate calculation as per configuration.\" ) return None try : # Validate inputs if events . empty or gti . empty : raise ValueError ( \"Input events or GTI DataFrame is empty.\" ) if not all ( col in events . columns for col in [ \"TIME\" , \"Exposure\" ]): raise ValueError ( \"Events DataFrame must contain 'TIME' and 'Exposure' columns.\" ) if not all ( col in gti . columns for col in [ \"START\" , \"STOP\" ]): raise ValueError ( \"GTI DataFrame must contain 'START' and 'STOP' columns.\" ) # Use flare GTI if provided, else default to common GTI intervals = flare_gti if flare_gti is not None else gti # Results list to collect average count rates for each interval results = [] for _ , interval in intervals . iterrows (): start , stop = interval [ \"START\" ], interval [ \"STOP\" ] # Filter events within the interval events_in_interval = events [ ( events [ \"TIME\" ] >= start ) & ( events [ \"TIME\" ] < stop ) ] if not events_in_interval . empty : # Total corrected exposure is the sum of 1 / Exposure total_corrected_exposure = ( 1.0 / events_in_interval [ \"Exposure\" ]) . sum () # Calculate interval duration interval_duration = stop - start # Calculate average count rate count_rate = total_corrected_exposure / interval_duration else : count_rate = np . nan # No events in this interval # Append the result results . append ({ \"START\" : start , \"STOP\" : stop , \"Count Rate\" : count_rate }) # Convert results to a DataFrame results_df = pd . DataFrame ( results ) # Log output print ( f \" Processed { len ( results_df ) } intervals.\" ) print ( f \" Count rate range: { results_df [ 'Count Rate' ] . min () } - { results_df [ 'Count Rate' ] . max () } \" ) return results_df except Exception as e : raise RuntimeError ( f \"Error in calculating average rate: { e } \" )","title":"Calculate average rate"},{"location":"calculate_average_rate/#scripts.calculate_average_rate.calculate_average_rate","text":"Calculate the average count rate during GTI intervals, optionally filtered by flare GTI. Parameters: Name Type Description Default events DataFrame Unified photon event DataFrame with 'TIME' and 'Exposure' columns. required gti DataFrame Common GTI DataFrame with 'START' and 'STOP' columns. required flare_gti DataFrame Flare GTI DataFrame with 'START' and 'STOP' columns. None calculate_average_count_rate bool Whether to calculate average count rates. False Returns: Type Description DataFrame Summary DataFrame with 'START', 'STOP', and 'Count Rate'. Source code in scripts/calculate_average_rate.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def calculate_average_rate ( events : pd . DataFrame , gti : pd . DataFrame , flare_gti = None , calculate_average_count_rate = False , ): \"\"\" Calculate the average count rate during GTI intervals, optionally filtered by flare GTI. Parameters: events (pd.DataFrame): Unified photon event DataFrame with 'TIME' and 'Exposure' columns. gti (pd.DataFrame): Common GTI DataFrame with 'START' and 'STOP' columns. flare_gti (pd.DataFrame, optional): Flare GTI DataFrame with 'START' and 'STOP' columns. calculate_average_count_rate (bool): Whether to calculate average count rates. Returns: (pd.DataFrame): Summary DataFrame with 'START', 'STOP', and 'Count Rate'. \"\"\" if not calculate_average_count_rate : print ( \"Skipping average count rate calculation as per configuration.\" ) return None try : # Validate inputs if events . empty or gti . empty : raise ValueError ( \"Input events or GTI DataFrame is empty.\" ) if not all ( col in events . columns for col in [ \"TIME\" , \"Exposure\" ]): raise ValueError ( \"Events DataFrame must contain 'TIME' and 'Exposure' columns.\" ) if not all ( col in gti . columns for col in [ \"START\" , \"STOP\" ]): raise ValueError ( \"GTI DataFrame must contain 'START' and 'STOP' columns.\" ) # Use flare GTI if provided, else default to common GTI intervals = flare_gti if flare_gti is not None else gti # Results list to collect average count rates for each interval results = [] for _ , interval in intervals . iterrows (): start , stop = interval [ \"START\" ], interval [ \"STOP\" ] # Filter events within the interval events_in_interval = events [ ( events [ \"TIME\" ] >= start ) & ( events [ \"TIME\" ] < stop ) ] if not events_in_interval . empty : # Total corrected exposure is the sum of 1 / Exposure total_corrected_exposure = ( 1.0 / events_in_interval [ \"Exposure\" ]) . sum () # Calculate interval duration interval_duration = stop - start # Calculate average count rate count_rate = total_corrected_exposure / interval_duration else : count_rate = np . nan # No events in this interval # Append the result results . append ({ \"START\" : start , \"STOP\" : stop , \"Count Rate\" : count_rate }) # Convert results to a DataFrame results_df = pd . DataFrame ( results ) # Log output print ( f \" Processed { len ( results_df ) } intervals.\" ) print ( f \" Count rate range: { results_df [ 'Count Rate' ] . min () } - { results_df [ 'Count Rate' ] . max () } \" ) return results_df except Exception as e : raise RuntimeError ( f \"Error in calculating average rate: { e } \" )","title":"calculate_average_rate"},{"location":"clean_gti/","text":"clean_gti ( gti , events , threshold = 30 , starttrim = 10 , stoptrim = 10 ) Clean GTIs by trimming and filtering based on duration and events. Parameters: Name Type Description Default gti DataFrame GTI DataFrame with 'START' and 'STOP' columns. required events DataFrame Events DataFrame with 'TIME' column. required threshold float Minimum duration for a valid GTI (seconds). 30 starttrim float Seconds to trim from GTI start times. 10 stoptrim float Seconds to trim from GTI stop times. 10 Returns: Type Description DataFrame Cleaned GTI DataFrame. Source code in scripts/clean_gti.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def clean_gti ( gti : pd . DataFrame , events : pd . DataFrame , threshold = 30 , starttrim = 10 , stoptrim = 10 ): \"\"\" Clean GTIs by trimming and filtering based on duration and events. Parameters: gti (pd.DataFrame): GTI DataFrame with 'START' and 'STOP' columns. events (pd.DataFrame): Events DataFrame with 'TIME' column. threshold (float): Minimum duration for a valid GTI (seconds). starttrim (float): Seconds to trim from GTI start times. stoptrim (float): Seconds to trim from GTI stop times. Returns: (pd.DataFrame): Cleaned GTI DataFrame. \"\"\" try : # Trim GTI intervals gti [ \"START\" ] += starttrim gti [ \"STOP\" ] -= stoptrim # Calculate GTI durations gti [ \"DURATION\" ] = gti [ \"STOP\" ] - gti [ \"START\" ] # Filter GTIs by duration valid_gti = gti [ gti [ \"DURATION\" ] > threshold ] # Count events within each GTI valid_gti = valid_gti . assign ( EVENT_COUNT = valid_gti . apply ( lambda row : ( ( events [ \"TIME\" ] >= row [ \"START\" ]) & ( events [ \"TIME\" ] <= row [ \"STOP\" ]) ) . sum (), axis = 1 , ) ) # Retain GTIs with at least one event final_gti = valid_gti [ valid_gti [ \"EVENT_COUNT\" ] > 0 ] # Log changes for debugging print ( f \" GTIs before cleaning: { len ( gti ) } \" ) print ( f \" GTIs after duration filter: { len ( valid_gti ) } \" ) print ( f \" GTIs after event filter: { len ( final_gti ) } \" ) # Drop intermediate columns and return return final_gti # for debugging use # return final_gti.drop(columns=['DURATION', 'EVENT_COUNT']) except Exception as e : raise RuntimeError ( f \"Error during GTI cleaning: { e } \" )","title":"Clean gti"},{"location":"clean_gti/#scripts.clean_gti.clean_gti","text":"Clean GTIs by trimming and filtering based on duration and events. Parameters: Name Type Description Default gti DataFrame GTI DataFrame with 'START' and 'STOP' columns. required events DataFrame Events DataFrame with 'TIME' column. required threshold float Minimum duration for a valid GTI (seconds). 30 starttrim float Seconds to trim from GTI start times. 10 stoptrim float Seconds to trim from GTI stop times. 10 Returns: Type Description DataFrame Cleaned GTI DataFrame. Source code in scripts/clean_gti.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def clean_gti ( gti : pd . DataFrame , events : pd . DataFrame , threshold = 30 , starttrim = 10 , stoptrim = 10 ): \"\"\" Clean GTIs by trimming and filtering based on duration and events. Parameters: gti (pd.DataFrame): GTI DataFrame with 'START' and 'STOP' columns. events (pd.DataFrame): Events DataFrame with 'TIME' column. threshold (float): Minimum duration for a valid GTI (seconds). starttrim (float): Seconds to trim from GTI start times. stoptrim (float): Seconds to trim from GTI stop times. Returns: (pd.DataFrame): Cleaned GTI DataFrame. \"\"\" try : # Trim GTI intervals gti [ \"START\" ] += starttrim gti [ \"STOP\" ] -= stoptrim # Calculate GTI durations gti [ \"DURATION\" ] = gti [ \"STOP\" ] - gti [ \"START\" ] # Filter GTIs by duration valid_gti = gti [ gti [ \"DURATION\" ] > threshold ] # Count events within each GTI valid_gti = valid_gti . assign ( EVENT_COUNT = valid_gti . apply ( lambda row : ( ( events [ \"TIME\" ] >= row [ \"START\" ]) & ( events [ \"TIME\" ] <= row [ \"STOP\" ]) ) . sum (), axis = 1 , ) ) # Retain GTIs with at least one event final_gti = valid_gti [ valid_gti [ \"EVENT_COUNT\" ] > 0 ] # Log changes for debugging print ( f \" GTIs before cleaning: { len ( gti ) } \" ) print ( f \" GTIs after duration filter: { len ( valid_gti ) } \" ) print ( f \" GTIs after event filter: { len ( final_gti ) } \" ) # Drop intermediate columns and return return final_gti # for debugging use # return final_gti.drop(columns=['DURATION', 'EVENT_COUNT']) except Exception as e : raise RuntimeError ( f \"Error during GTI cleaning: { e } \" )","title":"clean_gti"},{"location":"create_lightcurve/","text":"generate_lightcurve ( events_df , gti_df , binsize = 100 , energy_range = ( 3.0 , 79.0 ), gti_average = True ) Generate a regularly binned light curve from filtered event data. Parameters: Name Type Description Default events_df DataFrame Filtered events with columns ['TIME', 'Energy', 'Exposure']. required gti_df DataFrame GTI intervals with columns ['START', 'STOP']. required binsize int Time bin size in seconds (default: 100). 100 energy_range tuple (Emin, Emax) energy range in keV (default: (3.0, 79.0)). (3.0, 79.0) gti_average bool Use GTI-based binning if True (default: True). True Returns: Type Description DataFrame Regularly binned light curve with columns: ['bin_start', 'bin_end', 'count_rate', 'upper_limit', 'lower_limit']. Source code in scripts/create_lightcurve.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def generate_lightcurve ( events_df : pd . DataFrame , gti_df : pd . DataFrame , binsize = 100 , energy_range = ( 3.0 , 79.0 ), gti_average = True , ): \"\"\" Generate a regularly binned light curve from filtered event data. Parameters: events_df (pd.DataFrame): Filtered events with columns ['TIME', 'Energy', 'Exposure']. gti_df (pd.DataFrame): GTI intervals with columns ['START', 'STOP']. binsize (int): Time bin size in seconds (default: 100). energy_range (tuple): (Emin, Emax) energy range in keV (default: (3.0, 79.0)). gti_average (bool): Use GTI-based binning if True (default: True). Returns: (pd.DataFrame): Regularly binned light curve with columns: ['bin_start', 'bin_end', 'count_rate', 'upper_limit', 'lower_limit']. \"\"\" # Validate required columns required_columns = [ \"TIME\" , \"Energy\" ] for col in required_columns : if col not in events_df . columns : raise ValueError ( f \"Missing required column: { col } \" ) # Filter events by energy range filtered_events = events_df [ ( events_df [ \"Energy\" ] >= energy_range [ 0 ]) & ( events_df [ \"Energy\" ] <= energy_range [ 1 ]) ] if filtered_events . empty : raise ValueError ( \"No events found in the specified energy range.\" ) # Define time bins based on GTIs and events if gti_average : bins = [] for _ , gti in gti_df . iterrows (): bins += list ( np . arange ( gti [ \"START\" ], gti [ \"STOP\" ], binsize )) bins = sorted ( set ( bins )) else : observation_start = gti_df [ \"START\" ] . min () observation_stop = max ( gti_df [ \"STOP\" ] . max (), events_df [ \"TIME\" ] . max () ) # Extend to max event time bins = np . arange ( observation_start , observation_stop + binsize , binsize ) # Bin the events bin_edges = np . array ( bins ) event_counts , _ = np . histogram ( filtered_events [ \"TIME\" ], bins = bin_edges ) # Calculate bin durations bin_durations = np . diff ( bin_edges ) count_rates = event_counts / bin_durations # Confidence intervals using Gehrels' method upper_limits = upper_limit_gehrels ( 0.8413 , event_counts ) / bin_durations lower_limits = lower_limit_gehrels ( 0.8413 , event_counts ) / bin_durations # Set count_rate, upper_limit, and lower_limit to NaN for rows where count_rate = 0.0 mask_zero_counts = event_counts == 0 count_rates [ mask_zero_counts ] = np . nan upper_limits [ mask_zero_counts ] = np . nan lower_limits [ mask_zero_counts ] = np . nan # Remove bins outside GTIs (allow partial overlaps) valid_bins = [] for start , stop in zip ( bin_edges [: - 1 ], bin_edges [ 1 :]): if any (( gti_df [ \"START\" ] < stop ) & ( gti_df [ \"STOP\" ] > start )): valid_bins . append (( start , stop )) # Correct for Livetime, vignetting, and PSF completeness for each light curve bin correction_factor = get_binned_corr_factor ( events_df , valid_bins ) count_rates [: len ( valid_bins )] /= correction_factor upper_limits [: len ( valid_bins )] /= correction_factor lower_limits [: len ( valid_bins )] /= correction_factor # Format as DataFrame valid_bins = np . array ( valid_bins ) lightcurve_df = pd . DataFrame ( { \"bin_start\" : valid_bins [:, 0 ], \"bin_end\" : valid_bins [:, 1 ], \"count_rate\" : count_rates [: len ( valid_bins )], \"upper_limit\" : upper_limits [: len ( valid_bins )], \"lower_limit\" : lower_limits [: len ( valid_bins )], } ) # Filter out bins longer than the specified binsize - trim in between GTI bins lightcurve_df = lightcurve_df [ ( lightcurve_df [ \"bin_end\" ] - lightcurve_df [ \"bin_start\" ]) <= binsize ] print ( f \" Light Curve Time Range: { bin_edges [ 0 ] } - { bin_edges [ - 1 ] } \" ) print ( f \" Event Time Range: { events_df [ 'TIME' ] . min () } - { events_df [ 'TIME' ] . max () } \" ) print ( f \" Removed { len ( valid_bins ) - len ( lightcurve_df ) } oversized bins.\" ) return lightcurve_df","title":"Create lightcurve"},{"location":"create_lightcurve/#scripts.create_lightcurve.generate_lightcurve","text":"Generate a regularly binned light curve from filtered event data. Parameters: Name Type Description Default events_df DataFrame Filtered events with columns ['TIME', 'Energy', 'Exposure']. required gti_df DataFrame GTI intervals with columns ['START', 'STOP']. required binsize int Time bin size in seconds (default: 100). 100 energy_range tuple (Emin, Emax) energy range in keV (default: (3.0, 79.0)). (3.0, 79.0) gti_average bool Use GTI-based binning if True (default: True). True Returns: Type Description DataFrame Regularly binned light curve with columns: ['bin_start', 'bin_end', 'count_rate', 'upper_limit', 'lower_limit']. Source code in scripts/create_lightcurve.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def generate_lightcurve ( events_df : pd . DataFrame , gti_df : pd . DataFrame , binsize = 100 , energy_range = ( 3.0 , 79.0 ), gti_average = True , ): \"\"\" Generate a regularly binned light curve from filtered event data. Parameters: events_df (pd.DataFrame): Filtered events with columns ['TIME', 'Energy', 'Exposure']. gti_df (pd.DataFrame): GTI intervals with columns ['START', 'STOP']. binsize (int): Time bin size in seconds (default: 100). energy_range (tuple): (Emin, Emax) energy range in keV (default: (3.0, 79.0)). gti_average (bool): Use GTI-based binning if True (default: True). Returns: (pd.DataFrame): Regularly binned light curve with columns: ['bin_start', 'bin_end', 'count_rate', 'upper_limit', 'lower_limit']. \"\"\" # Validate required columns required_columns = [ \"TIME\" , \"Energy\" ] for col in required_columns : if col not in events_df . columns : raise ValueError ( f \"Missing required column: { col } \" ) # Filter events by energy range filtered_events = events_df [ ( events_df [ \"Energy\" ] >= energy_range [ 0 ]) & ( events_df [ \"Energy\" ] <= energy_range [ 1 ]) ] if filtered_events . empty : raise ValueError ( \"No events found in the specified energy range.\" ) # Define time bins based on GTIs and events if gti_average : bins = [] for _ , gti in gti_df . iterrows (): bins += list ( np . arange ( gti [ \"START\" ], gti [ \"STOP\" ], binsize )) bins = sorted ( set ( bins )) else : observation_start = gti_df [ \"START\" ] . min () observation_stop = max ( gti_df [ \"STOP\" ] . max (), events_df [ \"TIME\" ] . max () ) # Extend to max event time bins = np . arange ( observation_start , observation_stop + binsize , binsize ) # Bin the events bin_edges = np . array ( bins ) event_counts , _ = np . histogram ( filtered_events [ \"TIME\" ], bins = bin_edges ) # Calculate bin durations bin_durations = np . diff ( bin_edges ) count_rates = event_counts / bin_durations # Confidence intervals using Gehrels' method upper_limits = upper_limit_gehrels ( 0.8413 , event_counts ) / bin_durations lower_limits = lower_limit_gehrels ( 0.8413 , event_counts ) / bin_durations # Set count_rate, upper_limit, and lower_limit to NaN for rows where count_rate = 0.0 mask_zero_counts = event_counts == 0 count_rates [ mask_zero_counts ] = np . nan upper_limits [ mask_zero_counts ] = np . nan lower_limits [ mask_zero_counts ] = np . nan # Remove bins outside GTIs (allow partial overlaps) valid_bins = [] for start , stop in zip ( bin_edges [: - 1 ], bin_edges [ 1 :]): if any (( gti_df [ \"START\" ] < stop ) & ( gti_df [ \"STOP\" ] > start )): valid_bins . append (( start , stop )) # Correct for Livetime, vignetting, and PSF completeness for each light curve bin correction_factor = get_binned_corr_factor ( events_df , valid_bins ) count_rates [: len ( valid_bins )] /= correction_factor upper_limits [: len ( valid_bins )] /= correction_factor lower_limits [: len ( valid_bins )] /= correction_factor # Format as DataFrame valid_bins = np . array ( valid_bins ) lightcurve_df = pd . DataFrame ( { \"bin_start\" : valid_bins [:, 0 ], \"bin_end\" : valid_bins [:, 1 ], \"count_rate\" : count_rates [: len ( valid_bins )], \"upper_limit\" : upper_limits [: len ( valid_bins )], \"lower_limit\" : lower_limits [: len ( valid_bins )], } ) # Filter out bins longer than the specified binsize - trim in between GTI bins lightcurve_df = lightcurve_df [ ( lightcurve_df [ \"bin_end\" ] - lightcurve_df [ \"bin_start\" ]) <= binsize ] print ( f \" Light Curve Time Range: { bin_edges [ 0 ] } - { bin_edges [ - 1 ] } \" ) print ( f \" Event Time Range: { events_df [ 'TIME' ] . min () } - { events_df [ 'TIME' ] . max () } \" ) print ( f \" Removed { len ( valid_bins ) - len ( lightcurve_df ) } oversized bins.\" ) return lightcurve_df","title":"generate_lightcurve"},{"location":"data_loader/","text":"duplicate_fits ( event_file , lccorrfile , output_dir ) Takes an event file and the corresponding light curve correction file and makes a duplicate of them in the output directory. Parameters: Name Type Description Default event_file str FITS file to be duplicated required lccorrfile str light curve correction file to be duplicated required output_dir str output directory for duplicated files required Source code in scripts/data_loader.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def duplicate_fits ( event_file : str , lccorrfile : str , output_dir : str ): \"\"\" Takes an event file and the corresponding light curve correction file and makes a duplicate of them in the output directory. Parameters: event_file (str): FITS file to be duplicated lccorrfile (str): light curve correction file to be duplicated output_dir (str): output directory for duplicated files \"\"\" input_file = [ event_file , lccorrfile ] output_file = [ \"event_file_C\" , \"lccorrfile_C\" ] for i in range ( len ( input_file )): # Open original FITS with fits . open ( input_file [ i ]) as hdul : new_hdus = [] for hdu in hdul : if isinstance ( hdu , fits . PrimaryHDU ): # Primary HDU (usually image data) \u2192 keep header, empty data new_hdu = fits . PrimaryHDU ( data = None , header = hdu . header ) elif isinstance ( hdu , fits . ImageHDU ): # Image extension \u2192 keep header, drop data new_hdu = fits . ImageHDU ( data = None , header = hdu . header ) elif isinstance ( hdu , fits . BinTableHDU ) or isinstance ( hdu , fits . TableHDU ): # Table extensions \u2192 preserve column structure but no rows cols = hdu . columns new_hdu = hdu . __class__ . from_columns ( cols , nrows = 0 ) # Copy extra header keywords for key , val in hdu . header . items (): if key not in new_hdu . header : new_hdu . header [ key ] = val else : # For any other HDU type, just copy header and strip data new_hdu = hdu . copy () new_hdu . data = None new_hdus . append ( new_hdu ) # Write new FITS file with same structure, empty data hdul_new = fits . HDUList ( new_hdus ) hdul_new . writeto ( output_dir + output_file [ i ], overwrite = True ) print ( f \"Copy of FITS with same structure (all extensions) saved to { output_file } \" ) load_event_file ( file_path ) Load an event file using Astropy and convert it to a DataFrame. Parameters: Name Type Description Default file_path str Path to the event FITS file. required Returns: Type Description DataFrame Event DataFrame with 'TIME', 'PI', and 'Energy' columns. Source code in scripts/data_loader.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 def load_event_file ( file_path ): \"\"\" Load an event file using Astropy and convert it to a DataFrame. Parameters: file_path (str): Path to the event FITS file. Returns: (pd.DataFrame): Event DataFrame with 'TIME', 'PI', and 'Energy' columns. \"\"\" try : with fits . open ( file_path ) as hdul : event_data = hdul [ 1 ] . data # Convert columns to native byte order time_data = event_data [ \"TIME\" ] . byteswap () . newbyteorder () pi_data = event_data [ \"PI\" ] . byteswap () . newbyteorder () # Create DataFrame df = pd . DataFrame ({ \"TIME\" : time_data , \"PI\" : pi_data }) # Calculate Energy df [ \"Energy\" ] = df [ \"PI\" ] * 0.04 + 1.6 return df except Exception as e : raise RuntimeError ( f \"Failed to load event file { file_path } : { e } \" ) load_gti_file ( file_path ) Load a GTI file using Astropy and convert it to a DataFrame. Parameters: Name Type Description Default file_path str Path to the GTI FITS file. required Returns: Type Description DataFrame GTI DataFrame with 'START' and 'STOP' columns. Source code in scripts/data_loader.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def load_gti_file ( file_path ): \"\"\" Load a GTI file using Astropy and convert it to a DataFrame. Parameters: file_path (str): Path to the GTI FITS file. Returns: (pd.DataFrame): GTI DataFrame with 'START' and 'STOP' columns. \"\"\" try : with fits . open ( file_path ) as hdul : gti_data = hdul [ 2 ] . data validate_gti_columns ( gti_data . names ) return pd . DataFrame ({ \"START\" : gti_data [ \"START\" ], \"STOP\" : gti_data [ \"STOP\" ]}) except Exception as e : raise RuntimeError ( f \"Failed to load GTI from file { file_path } : { e } \" ) validate_gti_columns ( columns ) Validate that the GTI file contains the required columns. Parameters: Name Type Description Default columns list List of column names. required Raises: Type Description ValueError If required columns are missing. Source code in scripts/data_loader.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def validate_gti_columns ( columns : list ): \"\"\" Validate that the GTI file contains the required columns. Parameters: columns (list): List of column names. Raises: ValueError: If required columns are missing. \"\"\" required_columns = { \"START\" , \"STOP\" } if not required_columns . issubset ( set ( columns )): raise ValueError ( f \"GTI file is missing required columns: { required_columns - set ( columns ) } \" )","title":"Data loader"},{"location":"data_loader/#scripts.data_loader.duplicate_fits","text":"Takes an event file and the corresponding light curve correction file and makes a duplicate of them in the output directory. Parameters: Name Type Description Default event_file str FITS file to be duplicated required lccorrfile str light curve correction file to be duplicated required output_dir str output directory for duplicated files required Source code in scripts/data_loader.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def duplicate_fits ( event_file : str , lccorrfile : str , output_dir : str ): \"\"\" Takes an event file and the corresponding light curve correction file and makes a duplicate of them in the output directory. Parameters: event_file (str): FITS file to be duplicated lccorrfile (str): light curve correction file to be duplicated output_dir (str): output directory for duplicated files \"\"\" input_file = [ event_file , lccorrfile ] output_file = [ \"event_file_C\" , \"lccorrfile_C\" ] for i in range ( len ( input_file )): # Open original FITS with fits . open ( input_file [ i ]) as hdul : new_hdus = [] for hdu in hdul : if isinstance ( hdu , fits . PrimaryHDU ): # Primary HDU (usually image data) \u2192 keep header, empty data new_hdu = fits . PrimaryHDU ( data = None , header = hdu . header ) elif isinstance ( hdu , fits . ImageHDU ): # Image extension \u2192 keep header, drop data new_hdu = fits . ImageHDU ( data = None , header = hdu . header ) elif isinstance ( hdu , fits . BinTableHDU ) or isinstance ( hdu , fits . TableHDU ): # Table extensions \u2192 preserve column structure but no rows cols = hdu . columns new_hdu = hdu . __class__ . from_columns ( cols , nrows = 0 ) # Copy extra header keywords for key , val in hdu . header . items (): if key not in new_hdu . header : new_hdu . header [ key ] = val else : # For any other HDU type, just copy header and strip data new_hdu = hdu . copy () new_hdu . data = None new_hdus . append ( new_hdu ) # Write new FITS file with same structure, empty data hdul_new = fits . HDUList ( new_hdus ) hdul_new . writeto ( output_dir + output_file [ i ], overwrite = True ) print ( f \"Copy of FITS with same structure (all extensions) saved to { output_file } \" )","title":"duplicate_fits"},{"location":"data_loader/#scripts.data_loader.load_event_file","text":"Load an event file using Astropy and convert it to a DataFrame. Parameters: Name Type Description Default file_path str Path to the event FITS file. required Returns: Type Description DataFrame Event DataFrame with 'TIME', 'PI', and 'Energy' columns. Source code in scripts/data_loader.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 def load_event_file ( file_path ): \"\"\" Load an event file using Astropy and convert it to a DataFrame. Parameters: file_path (str): Path to the event FITS file. Returns: (pd.DataFrame): Event DataFrame with 'TIME', 'PI', and 'Energy' columns. \"\"\" try : with fits . open ( file_path ) as hdul : event_data = hdul [ 1 ] . data # Convert columns to native byte order time_data = event_data [ \"TIME\" ] . byteswap () . newbyteorder () pi_data = event_data [ \"PI\" ] . byteswap () . newbyteorder () # Create DataFrame df = pd . DataFrame ({ \"TIME\" : time_data , \"PI\" : pi_data }) # Calculate Energy df [ \"Energy\" ] = df [ \"PI\" ] * 0.04 + 1.6 return df except Exception as e : raise RuntimeError ( f \"Failed to load event file { file_path } : { e } \" )","title":"load_event_file"},{"location":"data_loader/#scripts.data_loader.load_gti_file","text":"Load a GTI file using Astropy and convert it to a DataFrame. Parameters: Name Type Description Default file_path str Path to the GTI FITS file. required Returns: Type Description DataFrame GTI DataFrame with 'START' and 'STOP' columns. Source code in scripts/data_loader.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def load_gti_file ( file_path ): \"\"\" Load a GTI file using Astropy and convert it to a DataFrame. Parameters: file_path (str): Path to the GTI FITS file. Returns: (pd.DataFrame): GTI DataFrame with 'START' and 'STOP' columns. \"\"\" try : with fits . open ( file_path ) as hdul : gti_data = hdul [ 2 ] . data validate_gti_columns ( gti_data . names ) return pd . DataFrame ({ \"START\" : gti_data [ \"START\" ], \"STOP\" : gti_data [ \"STOP\" ]}) except Exception as e : raise RuntimeError ( f \"Failed to load GTI from file { file_path } : { e } \" )","title":"load_gti_file"},{"location":"data_loader/#scripts.data_loader.validate_gti_columns","text":"Validate that the GTI file contains the required columns. Parameters: Name Type Description Default columns list List of column names. required Raises: Type Description ValueError If required columns are missing. Source code in scripts/data_loader.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def validate_gti_columns ( columns : list ): \"\"\" Validate that the GTI file contains the required columns. Parameters: columns (list): List of column names. Raises: ValueError: If required columns are missing. \"\"\" required_columns = { \"START\" , \"STOP\" } if not required_columns . issubset ( set ( columns )): raise ValueError ( f \"GTI file is missing required columns: { required_columns - set ( columns ) } \" )","title":"validate_gti_columns"},{"location":"detailed_flare_analysis/","text":"detailed_flare_analysis ( event_df , bayesian_blocks_df , p0 = 0.1 , flare_analysis_flag = False ) Perform a detailed Bayesian Block analysis on the flaring activity block. Parameters: Name Type Description Default event_df DataFrame Event DataFrame with 'TIME' column. required bayesian_blocks_df DataFrame Results from Step 10 (Bayesian Block Analysis). required p0 float False positive rate for detailed segmentation. 0.1 flare_analysis_flag bool Whether to perform detailed flare analysis. False Returns: Type Description DataFrame Refined Bayesian Block DataFrame for the flaring activity. Source code in scripts/detailed_flare_analysis.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def detailed_flare_analysis ( event_df , bayesian_blocks_df , p0 = 0.1 , flare_analysis_flag = False ): \"\"\" Perform a detailed Bayesian Block analysis on the flaring activity block. Parameters: event_df (pd.DataFrame): Event DataFrame with 'TIME' column. bayesian_blocks_df (pd.DataFrame): Results from Step 10 (Bayesian Block Analysis). p0 (float): False positive rate for detailed segmentation. flare_analysis_flag (bool): Whether to perform detailed flare analysis. Returns: (pd.DataFrame): Refined Bayesian Block DataFrame for the flaring activity. \"\"\" if not flare_analysis_flag : print ( \"Skipping detailed flare analysis as per configuration.\" ) return None try : # Identify the flaring block flare_block = bayesian_blocks_df . loc [ bayesian_blocks_df [ \"rate\" ] . idxmax () ] # Assuming flaring block is where the event rate is maximum flaring_start , flaring_stop = flare_block [ \"start\" ], flare_block [ \"stop\" ] print ( f \"Flaring activity identified between { flaring_start } and { flaring_stop } .\" ) # Extract events in the flaring block flaring_events = event_df [ ( event_df [ \"TIME\" ] >= flaring_start ) & ( event_df [ \"TIME\" ] < flaring_stop ) ] if flaring_events . empty : print ( \"No events found in the flaring interval. Skipping detailed analysis.\" ) return None # Apply Bayesian segmentation with a maybe higher p0 flaring_times = flaring_events [ \"TIME\" ] . values detailed_change_points = bayesian_blocks ( flaring_times , fitness = \"events\" , p0 = p0 ) # Calculate detailed block statistics block_start = detailed_change_points [: - 1 ] block_stop = detailed_change_points [ 1 :] durations = block_stop - block_start # Count events in each interval counts = ( pd . cut ( flaring_times , bins = detailed_change_points ) . value_counts () . sort_index () . values ) rates = counts / durations # Create detailed Bayesian block DataFrame detailed_blocks_df = pd . DataFrame ( { \"start\" : block_start , \"stop\" : block_stop , \"duration\" : durations , \"counts\" : counts , \"rate\" : rates , \"fp_rate\" : p0 , } ) # Log results # print(f\"Refined Bayesian Blocks for Flaring Activity:\") # print(detailed_blocks_df) # Plot the detailed segmentation # plt.figure(figsize=(10, 6)) # plt.hist(flaring_times, bins=100, histtype='step', color='gray', label='Photon Events') # for cp in detailed_change_points: # plt.axvline(cp, color='r', linestyle='--', label='Detailed Change Point' if 'Detailed Change Point' not in plt.gca().get_legend_handles_labels()[1] else None) # plt.title(\"Detailed Bayesian Segmentation of Flaring Activity\") # plt.xlabel(\"Time\") # plt.ylabel(\"Event Counts\") # plt.legend() # plt.show() return detailed_blocks_df except Exception as e : raise RuntimeError ( f \"Error in detailed flare analysis: { e } \" )","title":"Detailed flare analysis"},{"location":"detailed_flare_analysis/#scripts.detailed_flare_analysis.detailed_flare_analysis","text":"Perform a detailed Bayesian Block analysis on the flaring activity block. Parameters: Name Type Description Default event_df DataFrame Event DataFrame with 'TIME' column. required bayesian_blocks_df DataFrame Results from Step 10 (Bayesian Block Analysis). required p0 float False positive rate for detailed segmentation. 0.1 flare_analysis_flag bool Whether to perform detailed flare analysis. False Returns: Type Description DataFrame Refined Bayesian Block DataFrame for the flaring activity. Source code in scripts/detailed_flare_analysis.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def detailed_flare_analysis ( event_df , bayesian_blocks_df , p0 = 0.1 , flare_analysis_flag = False ): \"\"\" Perform a detailed Bayesian Block analysis on the flaring activity block. Parameters: event_df (pd.DataFrame): Event DataFrame with 'TIME' column. bayesian_blocks_df (pd.DataFrame): Results from Step 10 (Bayesian Block Analysis). p0 (float): False positive rate for detailed segmentation. flare_analysis_flag (bool): Whether to perform detailed flare analysis. Returns: (pd.DataFrame): Refined Bayesian Block DataFrame for the flaring activity. \"\"\" if not flare_analysis_flag : print ( \"Skipping detailed flare analysis as per configuration.\" ) return None try : # Identify the flaring block flare_block = bayesian_blocks_df . loc [ bayesian_blocks_df [ \"rate\" ] . idxmax () ] # Assuming flaring block is where the event rate is maximum flaring_start , flaring_stop = flare_block [ \"start\" ], flare_block [ \"stop\" ] print ( f \"Flaring activity identified between { flaring_start } and { flaring_stop } .\" ) # Extract events in the flaring block flaring_events = event_df [ ( event_df [ \"TIME\" ] >= flaring_start ) & ( event_df [ \"TIME\" ] < flaring_stop ) ] if flaring_events . empty : print ( \"No events found in the flaring interval. Skipping detailed analysis.\" ) return None # Apply Bayesian segmentation with a maybe higher p0 flaring_times = flaring_events [ \"TIME\" ] . values detailed_change_points = bayesian_blocks ( flaring_times , fitness = \"events\" , p0 = p0 ) # Calculate detailed block statistics block_start = detailed_change_points [: - 1 ] block_stop = detailed_change_points [ 1 :] durations = block_stop - block_start # Count events in each interval counts = ( pd . cut ( flaring_times , bins = detailed_change_points ) . value_counts () . sort_index () . values ) rates = counts / durations # Create detailed Bayesian block DataFrame detailed_blocks_df = pd . DataFrame ( { \"start\" : block_start , \"stop\" : block_stop , \"duration\" : durations , \"counts\" : counts , \"rate\" : rates , \"fp_rate\" : p0 , } ) # Log results # print(f\"Refined Bayesian Blocks for Flaring Activity:\") # print(detailed_blocks_df) # Plot the detailed segmentation # plt.figure(figsize=(10, 6)) # plt.hist(flaring_times, bins=100, histtype='step', color='gray', label='Photon Events') # for cp in detailed_change_points: # plt.axvline(cp, color='r', linestyle='--', label='Detailed Change Point' if 'Detailed Change Point' not in plt.gca().get_legend_handles_labels()[1] else None) # plt.title(\"Detailed Bayesian Segmentation of Flaring Activity\") # plt.xlabel(\"Time\") # plt.ylabel(\"Event Counts\") # plt.legend() # plt.show() return detailed_blocks_df except Exception as e : raise RuntimeError ( f \"Error in detailed flare analysis: { e } \" )","title":"detailed_flare_analysis"},{"location":"event_common_gti/","text":"filter_events_with_common_gti ( events , gti ) Filter events to retain those within the merged (common) GTIs. Parameters: Name Type Description Default events DataFrame Event DataFrame with 'TIME' column. required gti DataFrame Merged GTI DataFrame with 'START' and 'STOP' columns. required Returns: Type Description DataFrame Filtered event DataFrame with valid observation times. Source code in scripts/event_common_gti.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def filter_events_with_common_gti ( events , gti ): \"\"\" Filter events to retain those within the merged (common) GTIs. Parameters: events (pd.DataFrame): Event DataFrame with 'TIME' column. gti (pd.DataFrame): Merged GTI DataFrame with 'START' and 'STOP' columns. Returns: (pd.DataFrame): Filtered event DataFrame with valid observation times. \"\"\" try : filtered_events = [] for _ , interval in gti . iterrows (): start , stop = interval [ \"START\" ], interval [ \"STOP\" ] events_in_gti = events [( events [ \"TIME\" ] >= start ) & ( events [ \"TIME\" ] <= stop )] filtered_events . append ( events_in_gti ) filtered_events = pd . concat ( filtered_events , ignore_index = True ) print ( f \" Original events: { len ( events ) } \" ) print ( f \" Filtered events: { len ( filtered_events ) } \" ) return filtered_events except Exception as e : raise RuntimeError ( f \"Error during event filtering: { e } \" )","title":"Event common gti"},{"location":"event_common_gti/#scripts.event_common_gti.filter_events_with_common_gti","text":"Filter events to retain those within the merged (common) GTIs. Parameters: Name Type Description Default events DataFrame Event DataFrame with 'TIME' column. required gti DataFrame Merged GTI DataFrame with 'START' and 'STOP' columns. required Returns: Type Description DataFrame Filtered event DataFrame with valid observation times. Source code in scripts/event_common_gti.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def filter_events_with_common_gti ( events , gti ): \"\"\" Filter events to retain those within the merged (common) GTIs. Parameters: events (pd.DataFrame): Event DataFrame with 'TIME' column. gti (pd.DataFrame): Merged GTI DataFrame with 'START' and 'STOP' columns. Returns: (pd.DataFrame): Filtered event DataFrame with valid observation times. \"\"\" try : filtered_events = [] for _ , interval in gti . iterrows (): start , stop = interval [ \"START\" ], interval [ \"STOP\" ] events_in_gti = events [( events [ \"TIME\" ] >= start ) & ( events [ \"TIME\" ] <= stop )] filtered_events . append ( events_in_gti ) filtered_events = pd . concat ( filtered_events , ignore_index = True ) print ( f \" Original events: { len ( events ) } \" ) print ( f \" Filtered events: { len ( filtered_events ) } \" ) return filtered_events except Exception as e : raise RuntimeError ( f \"Error during event filtering: { e } \" )","title":"filter_events_with_common_gti"},{"location":"event_filter/","text":"filter_events_by_energy ( data , energy_min , energy_max ) Filter events based on energy range. Parameters: Name Type Description Default data DataFrame DataFrame containing event data. required energy_min float Minimum energy threshold. required energy_max float Maximum energy threshold. required Returns: Type Description DataFrame Filtered DataFrame with events within the specified energy range. Source code in scripts/event_filter.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 def filter_events_by_energy ( data , energy_min , energy_max ): \"\"\" Filter events based on energy range. Parameters: data (pd.DataFrame): DataFrame containing event data. energy_min (float): Minimum energy threshold. energy_max (float): Maximum energy threshold. Returns: (pd.DataFrame): Filtered DataFrame with events within the specified energy range. \"\"\" try : filtered_data = data [ ( data [ \"Energy\" ] >= energy_min ) & ( data [ \"Energy\" ] <= energy_max ) ] return filtered_data except KeyError : raise KeyError ( \"The input data does not contain the 'Energy' column.\" )","title":"Event filter"},{"location":"event_filter/#scripts.event_filter.filter_events_by_energy","text":"Filter events based on energy range. Parameters: Name Type Description Default data DataFrame DataFrame containing event data. required energy_min float Minimum energy threshold. required energy_max float Maximum energy threshold. required Returns: Type Description DataFrame Filtered DataFrame with events within the specified energy range. Source code in scripts/event_filter.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 def filter_events_by_energy ( data , energy_min , energy_max ): \"\"\" Filter events based on energy range. Parameters: data (pd.DataFrame): DataFrame containing event data. energy_min (float): Minimum energy threshold. energy_max (float): Maximum energy threshold. Returns: (pd.DataFrame): Filtered DataFrame with events within the specified energy range. \"\"\" try : filtered_data = data [ ( data [ \"Energy\" ] >= energy_min ) & ( data [ \"Energy\" ] <= energy_max ) ] return filtered_data except KeyError : raise KeyError ( \"The input data does not contain the 'Energy' column.\" )","title":"filter_events_by_energy"},{"location":"explain/","text":"The Use of Bayesian Block Representations with NuSTAR Observations This package was developed to apply Bayesian Block statistical methods to NuSTAR X-ray data of Sgr A* with the goal of determining \"flaring\" periods where the count rate of Sgr * is above the baseline quiesent rate Zhang 2017 . The Bayesian Block statistical method Scargle 2013 detects statistically signifigant change points in time series data and was developed with astronomical applications in mind. The idea is to optimally segment data from a target, apply likelihood functions, and surpress observational errors. Most Baysian Block analysis applies to three types of data measurements: time-tagged events, binned events, and measuremnts of data at various time intervals (e.g. gamma ray observations). These three types of data have different likelihood statitics, being, generally speaking, Bernoulli, Poisson, and Gaussian. NuSTAR data can be best described by time-tagged event data, as the telescope reports photon arrival times. Segmentation for time-tagged works by creating time bins of photon arrival times. The width of the bin is defined by the time since the last photon arrival and the time until the next photon arrival. To account for observational errors, which mostly include telescope effects such as detection effeciency, each time bin is divided by a correction factor that has a value between 0 and 1, with 1 being perfect detector effeciency. Luckily, the NuSTAR telescope already reports the detection effeciency per photon hit, so we don't have to do any calculations to determine this. A likelihood function is applied to track the probability of how many photons should arrive in a given time before it is considered to be a change point in counts. This is done iteratively until another change point is detected. These change points then mark the start and stop time of \"blocks\" that have similar count rates. Since the purpose of my project is looking for periods of flaring from Sgr A*, blocks with higher count rates than Sgr A* quiescence is labeled a flare. A data chalenge for applying a Bayesian Block algorithm to NuSTAR data in particular is the observation gaps in the data and the fact that NuSTAR has two observation modules. Observational gaps in NuSTAR data is due to the telescope shutting down as it passes over the South Atlantic Anomaly to protect the instruments on board. Because of this, NuSTAR reports \"Good Time Intervals\" (GTIs) of good and usable data. Time between these GTIS must be removed before running the data through the algorithm, or it will detect every gap as a change point. For accurate time reporting, the observational gaps are inserted back into the data so that the times of change points reflect actual time. To account for the fact that NuSTAR has two observational modules that simultanously look at a target, the data must be combined. That combination happens at many stages. First common GTIs must be found between the two modules, and then photon arrival times and correction factors are combined into one list to go through the bayesian block algorithm.","title":"Explanation"},{"location":"explain/#the-use-of-bayesian-block-representations-with-nustar-observations","text":"This package was developed to apply Bayesian Block statistical methods to NuSTAR X-ray data of Sgr A* with the goal of determining \"flaring\" periods where the count rate of Sgr * is above the baseline quiesent rate Zhang 2017 . The Bayesian Block statistical method Scargle 2013 detects statistically signifigant change points in time series data and was developed with astronomical applications in mind. The idea is to optimally segment data from a target, apply likelihood functions, and surpress observational errors. Most Baysian Block analysis applies to three types of data measurements: time-tagged events, binned events, and measuremnts of data at various time intervals (e.g. gamma ray observations). These three types of data have different likelihood statitics, being, generally speaking, Bernoulli, Poisson, and Gaussian. NuSTAR data can be best described by time-tagged event data, as the telescope reports photon arrival times. Segmentation for time-tagged works by creating time bins of photon arrival times. The width of the bin is defined by the time since the last photon arrival and the time until the next photon arrival. To account for observational errors, which mostly include telescope effects such as detection effeciency, each time bin is divided by a correction factor that has a value between 0 and 1, with 1 being perfect detector effeciency. Luckily, the NuSTAR telescope already reports the detection effeciency per photon hit, so we don't have to do any calculations to determine this. A likelihood function is applied to track the probability of how many photons should arrive in a given time before it is considered to be a change point in counts. This is done iteratively until another change point is detected. These change points then mark the start and stop time of \"blocks\" that have similar count rates. Since the purpose of my project is looking for periods of flaring from Sgr A*, blocks with higher count rates than Sgr A* quiescence is labeled a flare. A data chalenge for applying a Bayesian Block algorithm to NuSTAR data in particular is the observation gaps in the data and the fact that NuSTAR has two observation modules. Observational gaps in NuSTAR data is due to the telescope shutting down as it passes over the South Atlantic Anomaly to protect the instruments on board. Because of this, NuSTAR reports \"Good Time Intervals\" (GTIs) of good and usable data. Time between these GTIS must be removed before running the data through the algorithm, or it will detect every gap as a change point. For accurate time reporting, the observational gaps are inserted back into the data so that the times of change points reflect actual time. To account for the fact that NuSTAR has two observational modules that simultanously look at a target, the data must be combined. That combination happens at many stages. First common GTIs must be found between the two modules, and then photon arrival times and correction factors are combined into one list to go through the bayesian block algorithm.","title":"The Use of Bayesian Block Representations with NuSTAR Observations"},{"location":"find_blocks/","text":"find_blocks ( time , exposure , fp_rate = 0.05 , ncp_prior = None , do_iter = False ) Identify change points for Bayesian Blocks segmentation. Parameters: Name Type Description Default time ndarray Photon arrival times. required exposure ndarray Exposure correction factors. required fp_rate float False positive rate for change points. 0.05 ncp_prior float Prior number of change points. Defaults to None. None do_iter bool Iterative refinement flag. False Returns: Type Description dict Bayesian block structure with change points, counts, rates, etc. Source code in scripts/find_blocks.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def find_blocks ( time , exposure , fp_rate = 0.05 , ncp_prior = None , do_iter = False ): \"\"\" Identify change points for Bayesian Blocks segmentation. Parameters: time (np.ndarray): Photon arrival times. exposure (np.ndarray): Exposure correction factors. fp_rate (float): False positive rate for change points. ncp_prior (float, optional): Prior number of change points. Defaults to None. do_iter (bool): Iterative refinement flag. Returns: (dict): Bayesian block structure with change points, counts, rates, etc. \"\"\" # Step 1: Sort time and exposure sorted_indices = np . argsort ( time ) time = time [ sorted_indices ] exposure = exposure [ sorted_indices ] # Step 2: Calculate durations and event counts per block block_start = [ time [ 0 ]] block_stop = [] event_counts = [] cumulative_exposure = 0 count = 0 for i , t in enumerate ( time ): count += 1 cumulative_exposure += 1.0 / exposure [ i ] if ncp_prior and cumulative_exposure > ncp_prior : # Trigger segmentation block_stop . append ( t ) block_start . append ( t ) event_counts . append ( count ) count = 0 cumulative_exposure = 0 block_stop . append ( time [ - 1 ]) event_counts . append ( count ) # Step 3: Calculate rates block_durations = np . array ( block_stop ) - np . array ( block_start ) rates = np . array ( event_counts ) / block_durations # print(ncp_prior) return { \"change_points\" : block_start , \"start\" : block_start , \"stop\" : block_stop , \"duration\" : block_durations , \"counts\" : event_counts , \"rate\" : rates , } format_bayesian_block_output ( results , fp_rate , ncp_prior , do_iter ) Format Bayesian Block results into a DataFrame. Parameters: Name Type Description Default results DataFrame required fp_rate float false positive rate used in bayesian block analysis required ncp_prior float number of change point prior used in bayesian block analysis required do_iter boolean iterate over blocks required Returns: Type Description DataFrame Formated block information with upper and lower count rate limits Source code in scripts/find_blocks.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def format_bayesian_block_output ( results , fp_rate , ncp_prior , do_iter ): \"\"\" Format Bayesian Block results into a DataFrame. Parameters: results (pd.DataFrame): fp_rate(float): false positive rate used in bayesian block analysis ncp_prior(float): number of change point prior used in bayesian block analysis do_iter(boolean): iterate over blocks Returns: (pd.DataFrame): Formated block information with upper and lower count rate limits \"\"\" # print('right before printing', ncp_prior) df = pd . DataFrame ( { \"change_points\" : results [ \"change_points\" ], \"start\" : results [ \"start\" ], \"stop\" : results [ \"stop\" ], \"duration\" : results [ \"duration\" ], \"counts\" : results [ \"counts\" ], \"rate\" : results [ \"rate\" ], \"upperlim\" : upper_limit_gehrels ( 0.8413 , results [ \"counts\" ]) / results [ \"duration\" ], \"lowerlim\" : lower_limit_gehrels ( 0.8413 , results [ \"counts\" ]) / results [ \"duration\" ], \"ncp_prior\" : ncp_prior , \"fp_rate\" : fp_rate , \"do_iter\" : do_iter , } ) return df lower_limit_gehrels ( confidence_level , counts ) Compute the lower confidence limit for count rates. Parameters: Name Type Description Default confidence_level float Confidence level (e.g., 0.8413 for 1-sigma). required counts array - like Event counts. required Returns: Type Description ndarray Lower confidence limits. Source code in scripts/find_blocks.py 75 76 77 78 79 80 81 82 83 84 85 86 87 def lower_limit_gehrels ( confidence_level , counts ): \"\"\" Compute the lower confidence limit for count rates. Parameters: confidence_level (float): Confidence level (e.g., 0.8413 for 1-sigma). counts (array-like): Event counts. Returns: (np.ndarray): Lower confidence limits. \"\"\" counts = np . asarray ( counts ) # Ensure counts is a NumPy array return np . maximum ( counts - np . sqrt ( counts - 0.25 ), 0 ) # Avoid negative values upper_limit_gehrels ( confidence_level , counts ) Compute the upper confidence limit for count rates. Parameters: Name Type Description Default confidence_level float Confidence level (e.g., 0.8413 for 1-sigma). required counts array - like Event counts. required Returns: Type Description ndarray Upper confidence limits. Source code in scripts/find_blocks.py 60 61 62 63 64 65 66 67 68 69 70 71 72 def upper_limit_gehrels ( confidence_level , counts ): \"\"\" Compute the upper confidence limit for count rates. Parameters: confidence_level (float): Confidence level (e.g., 0.8413 for 1-sigma). counts (array-like): Event counts. Returns: (np.ndarray): Upper confidence limits. \"\"\" counts = np . asarray ( counts ) # Ensure counts is a NumPy array return counts + np . sqrt ( counts + 0.75 )","title":"Find blocks"},{"location":"find_blocks/#scripts.find_blocks.find_blocks","text":"Identify change points for Bayesian Blocks segmentation. Parameters: Name Type Description Default time ndarray Photon arrival times. required exposure ndarray Exposure correction factors. required fp_rate float False positive rate for change points. 0.05 ncp_prior float Prior number of change points. Defaults to None. None do_iter bool Iterative refinement flag. False Returns: Type Description dict Bayesian block structure with change points, counts, rates, etc. Source code in scripts/find_blocks.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def find_blocks ( time , exposure , fp_rate = 0.05 , ncp_prior = None , do_iter = False ): \"\"\" Identify change points for Bayesian Blocks segmentation. Parameters: time (np.ndarray): Photon arrival times. exposure (np.ndarray): Exposure correction factors. fp_rate (float): False positive rate for change points. ncp_prior (float, optional): Prior number of change points. Defaults to None. do_iter (bool): Iterative refinement flag. Returns: (dict): Bayesian block structure with change points, counts, rates, etc. \"\"\" # Step 1: Sort time and exposure sorted_indices = np . argsort ( time ) time = time [ sorted_indices ] exposure = exposure [ sorted_indices ] # Step 2: Calculate durations and event counts per block block_start = [ time [ 0 ]] block_stop = [] event_counts = [] cumulative_exposure = 0 count = 0 for i , t in enumerate ( time ): count += 1 cumulative_exposure += 1.0 / exposure [ i ] if ncp_prior and cumulative_exposure > ncp_prior : # Trigger segmentation block_stop . append ( t ) block_start . append ( t ) event_counts . append ( count ) count = 0 cumulative_exposure = 0 block_stop . append ( time [ - 1 ]) event_counts . append ( count ) # Step 3: Calculate rates block_durations = np . array ( block_stop ) - np . array ( block_start ) rates = np . array ( event_counts ) / block_durations # print(ncp_prior) return { \"change_points\" : block_start , \"start\" : block_start , \"stop\" : block_stop , \"duration\" : block_durations , \"counts\" : event_counts , \"rate\" : rates , }","title":"find_blocks"},{"location":"find_blocks/#scripts.find_blocks.format_bayesian_block_output","text":"Format Bayesian Block results into a DataFrame. Parameters: Name Type Description Default results DataFrame required fp_rate float false positive rate used in bayesian block analysis required ncp_prior float number of change point prior used in bayesian block analysis required do_iter boolean iterate over blocks required Returns: Type Description DataFrame Formated block information with upper and lower count rate limits Source code in scripts/find_blocks.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def format_bayesian_block_output ( results , fp_rate , ncp_prior , do_iter ): \"\"\" Format Bayesian Block results into a DataFrame. Parameters: results (pd.DataFrame): fp_rate(float): false positive rate used in bayesian block analysis ncp_prior(float): number of change point prior used in bayesian block analysis do_iter(boolean): iterate over blocks Returns: (pd.DataFrame): Formated block information with upper and lower count rate limits \"\"\" # print('right before printing', ncp_prior) df = pd . DataFrame ( { \"change_points\" : results [ \"change_points\" ], \"start\" : results [ \"start\" ], \"stop\" : results [ \"stop\" ], \"duration\" : results [ \"duration\" ], \"counts\" : results [ \"counts\" ], \"rate\" : results [ \"rate\" ], \"upperlim\" : upper_limit_gehrels ( 0.8413 , results [ \"counts\" ]) / results [ \"duration\" ], \"lowerlim\" : lower_limit_gehrels ( 0.8413 , results [ \"counts\" ]) / results [ \"duration\" ], \"ncp_prior\" : ncp_prior , \"fp_rate\" : fp_rate , \"do_iter\" : do_iter , } ) return df","title":"format_bayesian_block_output"},{"location":"find_blocks/#scripts.find_blocks.lower_limit_gehrels","text":"Compute the lower confidence limit for count rates. Parameters: Name Type Description Default confidence_level float Confidence level (e.g., 0.8413 for 1-sigma). required counts array - like Event counts. required Returns: Type Description ndarray Lower confidence limits. Source code in scripts/find_blocks.py 75 76 77 78 79 80 81 82 83 84 85 86 87 def lower_limit_gehrels ( confidence_level , counts ): \"\"\" Compute the lower confidence limit for count rates. Parameters: confidence_level (float): Confidence level (e.g., 0.8413 for 1-sigma). counts (array-like): Event counts. Returns: (np.ndarray): Lower confidence limits. \"\"\" counts = np . asarray ( counts ) # Ensure counts is a NumPy array return np . maximum ( counts - np . sqrt ( counts - 0.25 ), 0 ) # Avoid negative values","title":"lower_limit_gehrels"},{"location":"find_blocks/#scripts.find_blocks.upper_limit_gehrels","text":"Compute the upper confidence limit for count rates. Parameters: Name Type Description Default confidence_level float Confidence level (e.g., 0.8413 for 1-sigma). required counts array - like Event counts. required Returns: Type Description ndarray Upper confidence limits. Source code in scripts/find_blocks.py 60 61 62 63 64 65 66 67 68 69 70 71 72 def upper_limit_gehrels ( confidence_level , counts ): \"\"\" Compute the upper confidence limit for count rates. Parameters: confidence_level (float): Confidence level (e.g., 0.8413 for 1-sigma). counts (array-like): Event counts. Returns: (np.ndarray): Upper confidence limits. \"\"\" counts = np . asarray ( counts ) # Ensure counts is a NumPy array return counts + np . sqrt ( counts + 0.75 )","title":"upper_limit_gehrels"},{"location":"find_blocks_astropy/","text":"bba_astropy ( time , ncp_prior , fp_rate = 0.05 , x_list = None ) Perform Bayesian Block segmentation using Astropy. Parameters: Name Type Description Default time ndarray Photon arrival times. required ncp_prior float (optional) Number of change point prior required fp_rate float False positive rate for change points. 0.05 Returns: Type Description DataFrame DataFrame with Bayesian Block intervals and statistics. Source code in scripts/find_blocks_astropy.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def bba_astropy ( time , ncp_prior , fp_rate = 0.05 , x_list = None ): \"\"\" Perform Bayesian Block segmentation using Astropy. Parameters: time (np.ndarray): Photon arrival times. ncp_prior (float): (optional) Number of change point prior fp_rate (float): False positive rate for change points. Returns: (pd.DataFrame): DataFrame with Bayesian Block intervals and statistics. \"\"\" # make x values (correct for exposure) exp_list = x_list # change_points = bayesian_blocks(time, fitness=\"events\", p0=fp_rate) change_points = bayesian_blocks ( time , ex = exp_list , fitness = \"events\" , ncp_prior = ncp_prior ) # exposure version # change_points = bayesian_blocks(time, fitness=\"events\", ncp_prior=ncp_prior) # Tuning Hyperparameter p0: # Perform Bayesian Blocks segmentation with different p0 values # for p0 in [0.5, 0.1, 0.05, 0.01, 0.005]: # change_points = bayesian_blocks(time, fitness=\"events\", p0=p0) # print(f\" p0 = {p0}, Number of change points: {len(change_points) - 1}\") print ( f \" Change points: { change_points } \" ) # Calculate intervals, durations, and rates block_start = change_points [: - 1 ] block_stop = change_points [ 1 :] durations = block_stop - block_start counts = np . histogram ( time , bins = change_points )[ 0 ] rates = counts / durations block_label = np . array ( range ( 0 , len ( durations ))) # Format results df = pd . DataFrame ( { \"start\" : block_start , \"stop\" : block_stop , \"duration\" : durations , \"counts\" : counts , \"rate\" : rates , \"upperlim\" : upper_limit_gehrels ( 0.8413 , counts ) / durations , \"lowerlim\" : lower_limit_gehrels ( 0.8413 , counts ) / durations , \"fp_rate\" : fp_rate , \"block_label\" : block_label , } ) return df","title":"Find blocks astropy"},{"location":"find_blocks_astropy/#scripts.find_blocks_astropy.bba_astropy","text":"Perform Bayesian Block segmentation using Astropy. Parameters: Name Type Description Default time ndarray Photon arrival times. required ncp_prior float (optional) Number of change point prior required fp_rate float False positive rate for change points. 0.05 Returns: Type Description DataFrame DataFrame with Bayesian Block intervals and statistics. Source code in scripts/find_blocks_astropy.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def bba_astropy ( time , ncp_prior , fp_rate = 0.05 , x_list = None ): \"\"\" Perform Bayesian Block segmentation using Astropy. Parameters: time (np.ndarray): Photon arrival times. ncp_prior (float): (optional) Number of change point prior fp_rate (float): False positive rate for change points. Returns: (pd.DataFrame): DataFrame with Bayesian Block intervals and statistics. \"\"\" # make x values (correct for exposure) exp_list = x_list # change_points = bayesian_blocks(time, fitness=\"events\", p0=fp_rate) change_points = bayesian_blocks ( time , ex = exp_list , fitness = \"events\" , ncp_prior = ncp_prior ) # exposure version # change_points = bayesian_blocks(time, fitness=\"events\", ncp_prior=ncp_prior) # Tuning Hyperparameter p0: # Perform Bayesian Blocks segmentation with different p0 values # for p0 in [0.5, 0.1, 0.05, 0.01, 0.005]: # change_points = bayesian_blocks(time, fitness=\"events\", p0=p0) # print(f\" p0 = {p0}, Number of change points: {len(change_points) - 1}\") print ( f \" Change points: { change_points } \" ) # Calculate intervals, durations, and rates block_start = change_points [: - 1 ] block_stop = change_points [ 1 :] durations = block_stop - block_start counts = np . histogram ( time , bins = change_points )[ 0 ] rates = counts / durations block_label = np . array ( range ( 0 , len ( durations ))) # Format results df = pd . DataFrame ( { \"start\" : block_start , \"stop\" : block_stop , \"duration\" : durations , \"counts\" : counts , \"rate\" : rates , \"upperlim\" : upper_limit_gehrels ( 0.8413 , counts ) / durations , \"lowerlim\" : lower_limit_gehrels ( 0.8413 , counts ) / durations , \"fp_rate\" : fp_rate , \"block_label\" : block_label , } ) return df","title":"bba_astropy"},{"location":"get_binned_corr_factor/","text":"get_binned_corr_factor ( merged_events , bin_times ) Calculate correction factors for photon events based on the correction factor FITS file. Parameters: Name Type Description Default merged_events DataFrame data frame containing photon arrival times and corrections (\"exposure\"). required bin_times array - like start and stop times of data bins. required Returns: Type Description ndarray Array same length as bin_times. Source code in scripts/get_binned_corr_factor.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def get_binned_corr_factor ( merged_events , bin_times ): \"\"\" Calculate correction factors for photon events based on the correction factor FITS file. Parameters: merged_events (pd.DataFrame): data frame containing photon arrival times and corrections (\"exposure\"). bin_times (array-like): start and stop times of data bins. Returns: (np.ndarray): Array same length as bin_times. \"\"\" fraction = merged_events [ \"CORRECTION_FACTOR\" ] tstart = merged_events [ \"TIME\" ] corr_factors = np . zeros ( len ( bin_times )) for i in range ( len ( bin_times )): ilo = np . argmin ( np . abs ( bin_times [ i ][ 0 ] - tstart )) ihi = np . argmin ( np . abs ( bin_times [ i ][ 1 ] - tstart )) corr_factors [ i ] = np . mean ( fraction [ ilo : ihi ]) return corr_factors","title":"Get binned corr factor"},{"location":"get_binned_corr_factor/#scripts.get_binned_corr_factor.get_binned_corr_factor","text":"Calculate correction factors for photon events based on the correction factor FITS file. Parameters: Name Type Description Default merged_events DataFrame data frame containing photon arrival times and corrections (\"exposure\"). required bin_times array - like start and stop times of data bins. required Returns: Type Description ndarray Array same length as bin_times. Source code in scripts/get_binned_corr_factor.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def get_binned_corr_factor ( merged_events , bin_times ): \"\"\" Calculate correction factors for photon events based on the correction factor FITS file. Parameters: merged_events (pd.DataFrame): data frame containing photon arrival times and corrections (\"exposure\"). bin_times (array-like): start and stop times of data bins. Returns: (np.ndarray): Array same length as bin_times. \"\"\" fraction = merged_events [ \"CORRECTION_FACTOR\" ] tstart = merged_events [ \"TIME\" ] corr_factors = np . zeros ( len ( bin_times )) for i in range ( len ( bin_times )): ilo = np . argmin ( np . abs ( bin_times [ i ][ 0 ] - tstart )) ihi = np . argmin ( np . abs ( bin_times [ i ][ 1 ] - tstart )) corr_factors [ i ] = np . mean ( fraction [ ilo : ihi ]) return corr_factors","title":"get_binned_corr_factor"},{"location":"get_event_corr_factor/","text":"get_event_corr_factor ( filename , tt ) Calculate correction factors for photon events based on the correction factor FITS file. Parameters: Name Type Description Default filename str Path to the correction factor FITS file. required tt array - like Photon arrival times (e.g., DataFrame['TIME']). required Returns: Type Description ndarray Array of correction factors corresponding to photon times. Source code in scripts/get_event_corr_factor.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def get_event_corr_factor ( filename , tt ): \"\"\" Calculate correction factors for photon events based on the correction factor FITS file. Parameters: filename (str): Path to the correction factor FITS file. tt (array-like): Photon arrival times (e.g., DataFrame['TIME']). Returns: (np.ndarray): Array of correction factors corresponding to photon times. \"\"\" try : # Read the correction factor FITS file with fits . open ( filename ) as hdul : data = hdul [ 1 ] . data # Convert relevant columns to a DataFrame with proper byte order df_corr = pd . DataFrame ( { \"TSTART\" : data [ \"TSTART\" ] . byteswap () . newbyteorder (), \"TSTOP\" : data [ \"TSTOP\" ] . byteswap () . newbyteorder (), \"FRACTION\" : data [ \"FRACTION\" ] . byteswap () . newbyteorder (), } ) # Debugging: Print the first few rows of the correction factor DataFrame # print(f\"Correction Factor DataFrame (Converted):\\n{df_corr.head()}\") # Initialize the correction factor array with a default value (e.g., 1.0) correction_factors = np . full ( len ( tt ), 1.0 ) # Match photon times to correction intervals for i , t in enumerate ( tt ): match = df_corr [( df_corr [ \"TSTART\" ] <= t ) & ( df_corr [ \"TSTOP\" ] > t )] if not match . empty : correction_factors [ i ] = match . iloc [ 0 ][ \"FRACTION\" ] else : # Log warning if no match is found # pass print ( f \"Warning: No match found for photon time { t } . Using default correction factor 1.0.\" ) return correction_factors except Exception as e : raise RuntimeError ( f \"Error in GetEventcorrfactor: { e } \" )","title":"Get event corr factor"},{"location":"get_event_corr_factor/#scripts.get_event_corr_factor.get_event_corr_factor","text":"Calculate correction factors for photon events based on the correction factor FITS file. Parameters: Name Type Description Default filename str Path to the correction factor FITS file. required tt array - like Photon arrival times (e.g., DataFrame['TIME']). required Returns: Type Description ndarray Array of correction factors corresponding to photon times. Source code in scripts/get_event_corr_factor.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def get_event_corr_factor ( filename , tt ): \"\"\" Calculate correction factors for photon events based on the correction factor FITS file. Parameters: filename (str): Path to the correction factor FITS file. tt (array-like): Photon arrival times (e.g., DataFrame['TIME']). Returns: (np.ndarray): Array of correction factors corresponding to photon times. \"\"\" try : # Read the correction factor FITS file with fits . open ( filename ) as hdul : data = hdul [ 1 ] . data # Convert relevant columns to a DataFrame with proper byte order df_corr = pd . DataFrame ( { \"TSTART\" : data [ \"TSTART\" ] . byteswap () . newbyteorder (), \"TSTOP\" : data [ \"TSTOP\" ] . byteswap () . newbyteorder (), \"FRACTION\" : data [ \"FRACTION\" ] . byteswap () . newbyteorder (), } ) # Debugging: Print the first few rows of the correction factor DataFrame # print(f\"Correction Factor DataFrame (Converted):\\n{df_corr.head()}\") # Initialize the correction factor array with a default value (e.g., 1.0) correction_factors = np . full ( len ( tt ), 1.0 ) # Match photon times to correction intervals for i , t in enumerate ( tt ): match = df_corr [( df_corr [ \"TSTART\" ] <= t ) & ( df_corr [ \"TSTOP\" ] > t )] if not match . empty : correction_factors [ i ] = match . iloc [ 0 ][ \"FRACTION\" ] else : # Log warning if no match is found # pass print ( f \"Warning: No match found for photon time { t } . Using default correction factor 1.0.\" ) return correction_factors except Exception as e : raise RuntimeError ( f \"Error in GetEventcorrfactor: { e } \" )","title":"get_event_corr_factor"},{"location":"howto/","text":"How to Install Example Functions Here, we walk-through downloading the bayesian block analysis routine repository. From the Command Line Navigate to the location in which you wish to install the repository in the command line. Then, run the following line to clone the repository: $ git clone https://github.com/gsanjohn/CSME-890-final-project.git You will then have access to the main script main.py , and other support scripts located in the Scripts directory. From VSCode Open a new VSCode window and under the start menu, select \"Clone Git Repository\". Then, paste the following URL to clone the repository: https://github.com/gsanjohn/CSME-890-final-project.git Then select the location where you would line to clone this repository to.","title":"How-To Install"},{"location":"howto/#how-to-install-example-functions","text":"Here, we walk-through downloading the bayesian block analysis routine repository.","title":"How to Install Example Functions"},{"location":"howto/#from-the-command-line","text":"Navigate to the location in which you wish to install the repository in the command line. Then, run the following line to clone the repository: $ git clone https://github.com/gsanjohn/CSME-890-final-project.git You will then have access to the main script main.py , and other support scripts located in the Scripts directory.","title":"From the Command Line"},{"location":"howto/#from-vscode","text":"Open a new VSCode window and under the start menu, select \"Clone Git Repository\". Then, paste the following URL to clone the repository: https://github.com/gsanjohn/CSME-890-final-project.git Then select the location where you would line to clone this repository to.","title":"From VSCode"},{"location":"insert_gaps/","text":"insert_gti_gaps ( bba_df , gti_df ) Adjust BBA blocks to account for GTI gaps, splitting blocks as necessary. Parameters: Name Type Description Default bba_df DataFrame Bayesian Blocks DataFrame with 'start' and 'stop' columns. required gti_df DataFrame GTI DataFrame with 'START' and 'STOP' columns. required Returns: Type Description DataFrame Corrected BBA DataFrame with gaps reintroduced and blocks split if needed. DataFrame GTI gaps DataFrame with calculated gap durations. Source code in scripts/insert_gaps.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def insert_gti_gaps ( bba_df , gti_df ): \"\"\" Adjust BBA blocks to account for GTI gaps, splitting blocks as necessary. Parameters: bba_df (pd.DataFrame): Bayesian Blocks DataFrame with 'start' and 'stop' columns. gti_df (pd.DataFrame): GTI DataFrame with 'START' and 'STOP' columns. Returns: (pd.DataFrame): Corrected BBA DataFrame with gaps reintroduced and blocks split if needed. (pd.DataFrame): GTI gaps DataFrame with calculated gap durations. \"\"\" # print(f\"BBA Blocks Time Range: {bba_df['start'].min()} - {bba_df['stop'].max()}\") # print(f\"GTI Time Range: {gti_df['START'].min()} - {gti_df['STOP'].max()}\") # Step 1: Identify Gaps Between GTIs gti_gaps = [] for i in range ( len ( gti_df ) - 1 ): gap_start = gti_df . iloc [ i ][ \"STOP\" ] gap_stop = gti_df . iloc [ i + 1 ][ \"START\" ] gap_duration = gap_stop - gap_start if gap_duration > 0 : gti_gaps . append ( { \"GAP_START\" : gap_start , \"GAP_STOP\" : gap_stop , \"GAP_DURATION\" : gap_duration , } ) # print(f\"Identified Gap: START={gap_start}, STOP={gap_stop}, DURATION={gap_duration}\") gti_gaps_df = pd . DataFrame ( gti_gaps ) # Step 2: Adjust BBA Blocks Iteratively cumulative_shift = 0 corrected_blocks = bba_df . copy () for _ , gap in gti_gaps_df . iterrows (): gap_start = gap [ \"GAP_START\" ] gap_stop = gap [ \"GAP_STOP\" ] gap_duration = gap [ \"GAP_DURATION\" ] print ( f \"Processing Gap: START= { gap_start } , STOP= { gap_stop } , DURATION= { gap_duration } \" ) new_corrected_blocks = [] for _ , block in corrected_blocks . iterrows (): block_start = block [ \"start\" ] block_stop = block [ \"stop\" ] if block_stop <= gap_start : new_corrected_blocks . append ( { ** block . to_dict (), \"start\" : block_start , \"stop\" : block_stop } ) print ( f \"Block Unaffected by Gap: start= { block_start } , stop= { block_stop } \" ) elif block_start >= gap_start : new_corrected_blocks . append ( { ** block . to_dict (), \"start\" : block_start + gap_duration , \"stop\" : block_stop + gap_duration , } ) print ( f \"Block Shifted After Gap: start= { block_start + gap_duration } , stop= { block_stop + gap_duration } \" ) else : print ( f \"Block Overlaps Gap: start= { block_start } , stop= { block_stop } \" ) if block_start < gap_start : new_corrected_blocks . append ( { ** block . to_dict (), \"start\" : block_start , \"stop\" : gap_start } ) print ( f \" Sub-block 1: start= { block_start } , stop= { gap_start } \" ) new_corrected_blocks . append ( { ** block . to_dict (), \"start\" : gap_stop , \"stop\" : block_stop + gap_duration , } ) print ( f \" Sub-block 2: start= { gap_stop } , stop= { block_stop + gap_duration } \" ) corrected_blocks = pd . DataFrame ( new_corrected_blocks ) cumulative_shift += gap_duration print ( f \"Cumulative Shift Updated: { cumulative_shift } seconds\" ) return corrected_blocks , gti_gaps_df","title":"Insert gaps"},{"location":"insert_gaps/#scripts.insert_gaps.insert_gti_gaps","text":"Adjust BBA blocks to account for GTI gaps, splitting blocks as necessary. Parameters: Name Type Description Default bba_df DataFrame Bayesian Blocks DataFrame with 'start' and 'stop' columns. required gti_df DataFrame GTI DataFrame with 'START' and 'STOP' columns. required Returns: Type Description DataFrame Corrected BBA DataFrame with gaps reintroduced and blocks split if needed. DataFrame GTI gaps DataFrame with calculated gap durations. Source code in scripts/insert_gaps.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def insert_gti_gaps ( bba_df , gti_df ): \"\"\" Adjust BBA blocks to account for GTI gaps, splitting blocks as necessary. Parameters: bba_df (pd.DataFrame): Bayesian Blocks DataFrame with 'start' and 'stop' columns. gti_df (pd.DataFrame): GTI DataFrame with 'START' and 'STOP' columns. Returns: (pd.DataFrame): Corrected BBA DataFrame with gaps reintroduced and blocks split if needed. (pd.DataFrame): GTI gaps DataFrame with calculated gap durations. \"\"\" # print(f\"BBA Blocks Time Range: {bba_df['start'].min()} - {bba_df['stop'].max()}\") # print(f\"GTI Time Range: {gti_df['START'].min()} - {gti_df['STOP'].max()}\") # Step 1: Identify Gaps Between GTIs gti_gaps = [] for i in range ( len ( gti_df ) - 1 ): gap_start = gti_df . iloc [ i ][ \"STOP\" ] gap_stop = gti_df . iloc [ i + 1 ][ \"START\" ] gap_duration = gap_stop - gap_start if gap_duration > 0 : gti_gaps . append ( { \"GAP_START\" : gap_start , \"GAP_STOP\" : gap_stop , \"GAP_DURATION\" : gap_duration , } ) # print(f\"Identified Gap: START={gap_start}, STOP={gap_stop}, DURATION={gap_duration}\") gti_gaps_df = pd . DataFrame ( gti_gaps ) # Step 2: Adjust BBA Blocks Iteratively cumulative_shift = 0 corrected_blocks = bba_df . copy () for _ , gap in gti_gaps_df . iterrows (): gap_start = gap [ \"GAP_START\" ] gap_stop = gap [ \"GAP_STOP\" ] gap_duration = gap [ \"GAP_DURATION\" ] print ( f \"Processing Gap: START= { gap_start } , STOP= { gap_stop } , DURATION= { gap_duration } \" ) new_corrected_blocks = [] for _ , block in corrected_blocks . iterrows (): block_start = block [ \"start\" ] block_stop = block [ \"stop\" ] if block_stop <= gap_start : new_corrected_blocks . append ( { ** block . to_dict (), \"start\" : block_start , \"stop\" : block_stop } ) print ( f \"Block Unaffected by Gap: start= { block_start } , stop= { block_stop } \" ) elif block_start >= gap_start : new_corrected_blocks . append ( { ** block . to_dict (), \"start\" : block_start + gap_duration , \"stop\" : block_stop + gap_duration , } ) print ( f \"Block Shifted After Gap: start= { block_start + gap_duration } , stop= { block_stop + gap_duration } \" ) else : print ( f \"Block Overlaps Gap: start= { block_start } , stop= { block_stop } \" ) if block_start < gap_start : new_corrected_blocks . append ( { ** block . to_dict (), \"start\" : block_start , \"stop\" : gap_start } ) print ( f \" Sub-block 1: start= { block_start } , stop= { gap_start } \" ) new_corrected_blocks . append ( { ** block . to_dict (), \"start\" : gap_stop , \"stop\" : block_stop + gap_duration , } ) print ( f \" Sub-block 2: start= { gap_stop } , stop= { block_stop + gap_duration } \" ) corrected_blocks = pd . DataFrame ( new_corrected_blocks ) cumulative_shift += gap_duration print ( f \"Cumulative Shift Updated: { cumulative_shift } seconds\" ) return corrected_blocks , gti_gaps_df","title":"insert_gti_gaps"},{"location":"main/","text":"main () Bayesian Block alorithm for use with NuSTAR data. 3 pairs of files are necessary as input lightcurve correction factor file (produced by nuproducts) event list extracted from region of interest (produced by xselect) barycorr file to do barycenter correction Has capability to handle only one module. To do that, simply comment out path names for the missing module Flow of the code define filenames and paths load events and filter by energy Load and clean GTIs Merge GTIs Filter events outside of common GTIs Get exposure correction factor Merge events from FPM A and B Get average count rates in GTIs Remove time gaps from events (in preperation for BB analysis) Perform Bayesian Block Routine Re-insert GTI gaps into data Print out txt file of BBA results Bin events into light curve Make light curve plot NOTE: parameters do not appear in the function signature, but instead are editable parameters of the code. Parameters: Name Type Description Default year str year of the observation being analyzed (used for leap second correction) required obsID str observation ID of observation being analyzed required output_dir str path to output directory required event_file_a str path to xselected event file for module A (optional) required event_file_b str path to xselected event file for module B (optional) required barycorr_event str path to barycenter corrected event file for module A or B (optional) required lccorrfileA str path to light curve correction file for module A (optional) required lccorrfileB str path to light curve correction file for module B (optional) required energy_min int energy range in keV for bayesian block analysis. Default 3.0 required energy_max int energy range in keV for bayesian block analysis. Default 30.0 required fp_rate int false positive rate used for bayesian block detection. Default 0.01 required binsize int bin size in seconds for light curve plotting. Default 100 required energy_range list energy range in keV for plotted light curve. Default: (3.0, 79.0) required Returns: 12_bba_results.txt: text file with times, count rates, upper and lower limits 14_lightcurve_plot.png: Light curve plot with 1 sigma upper and lower count rates per block ReadMe.txt: text file inputs and various script settings Grace Sanger-Johnson 9/23/2025 based on runBB.pro IDL package created by Nicolas Barriere Source code in main.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 def main (): \"\"\"Bayesian Block alorithm for use with NuSTAR data. 3 pairs of files are necessary as input: - lightcurve correction factor file (produced by nuproducts) - event list extracted from region of interest (produced by xselect) - barycorr file to do barycenter correction Has capability to handle only one module. To do that, simply comment out path names for the missing module Flow of the code: 1. define filenames and paths 2. load events and filter by energy 3. Load and clean GTIs 4. Merge GTIs 5. Filter events outside of common GTIs 6. Get exposure correction factor 7. Merge events from FPM A and B 8. Get average count rates in GTIs 9. Remove time gaps from events (in preperation for BB analysis) 10. Perform Bayesian Block Routine 11. Re-insert GTI gaps into data 12. Print out txt file of BBA results 13. Bin events into light curve 14. Make light curve plot NOTE: parameters do not appear in the function signature, but instead are editable parameters of the code. Parameters: year (str): year of the observation being analyzed (used for leap second correction) obsID (str): observation ID of observation being analyzed output_dir (str): path to output directory event_file_a (str): path to xselected event file for module A (optional) event_file_b (str): path to xselected event file for module B (optional) barycorr_event (str): path to barycenter corrected event file for module A or B (optional) lccorrfileA (str): path to light curve correction file for module A (optional) lccorrfileB (str): path to light curve correction file for module B (optional) energy_min (int): energy range in keV for bayesian block analysis. Default 3.0 energy_max (int): energy range in keV for bayesian block analysis. Default 30.0 fp_rate (int): false positive rate used for bayesian block detection. Default 0.01 binsize (int): bin size in seconds for light curve plotting. Default 100 energy_range (list): energy range in keV for plotted light curve. Default: (3.0, 79.0) Returns: 12_bba_results.txt: text file with times, count rates, upper and lower limits 14_lightcurve_plot.png: Light curve plot with 1 sigma upper and lower count rates per block ReadMe.txt: text file inputs and various script settings Grace Sanger-Johnson 9/23/2025 based on runBB.pro IDL package created by Nicolas Barriere \"\"\" year = \"2016\" obsID = \"40202001002\" path = ( \"/Users/gracesanger-johnson/xray_astro/SgrA/\" + year + \"/\" + obsID + \"/event_cl2/BBA_results\" ) # Define output directory output_dir = path + \"/3-30_BB_output/\" # make sure directory exists os . makedirs ( output_dir , exist_ok = True ) # output_dir = path+\"/testing/fp_testing/\" # Set default file paths (DO NOT EDIT): event_file_a = None event_file_b = None # Define input file paths barycorr_event = path + \"/nu\" + obsID + \"A01_cl_barycorr.evt\" event_file_a = path + \"/nu\" + obsID + \"A01_xselected.evt\" event_file_b = path + \"/nu\" + obsID + \"B01_xselected.evt\" lccorrfileA = path + \"/SgrA_correct_50ac_fpmA_lcsrccorrfile.fits\" lccorrfileB = path + \"/SgrA_correct_50ac_fpmB_lcsrccorrfile.fits\" # --- Define Parameters / General Used Ones --- energy_min = 3.0 # energy_max = 79.0 energy_max = 30.0 # --- For Step 3 / GTI Cleaning --- print ( \" \\n ------ Start Data Processing Pipeline ------ \\n \" ) ######################## Step 1: Load Data ####################### # copy other fits files if one module is missing one_mod = False if event_file_a is None : one_mod = True present_mod = \"B\" # gti_file_a, event_file_a, lccorrfileA= one_module(gti_file_b, event_file_b, lccorrfileB,output_dir) # duplicate_fits(gti_file_b, event_file_b, lccorrfileB, output_dir) duplicate_fits ( event_file_b , lccorrfileB , output_dir ) # gti_file_a = output_dir + \"gti_file_C\" event_file_a = output_dir + \"event_file_C\" lccorrfileA = output_dir + \"lccorrfile_C\" elif event_file_b is None : one_mod = True present_mod = \"A\" # gti_file_b, event_file_b, lccorrfileB = one_module(gti_file_a, event_file_a, lccorrfileA,output_dir) # duplicate_fits(gti_file_a, event_file_a, lccorrfileA, output_dir) duplicate_fits ( event_file_a , lccorrfileA , output_dir ) # gti_file_b = output_dir + \"gti_file_C\" event_file_b = output_dir + \"event_file_C\" lccorrfileB = output_dir + \"lccorrfile_C\" print ( \"Step 1: Loading GTI and event data...\" ) gtiA = load_gti_file ( event_file_a ) gtiB = load_gti_file ( event_file_b ) eventsA = load_event_file ( event_file_a ) eventsB = load_event_file ( event_file_b ) print ( f \" Module A: { len ( gtiA ) } GTI intervals, { len ( eventsA ) } events.\" ) print ( f \" Module B: { len ( gtiB ) } GTI intervals, { len ( eventsB ) } events.\" ) print ( \"Step 1.5: apply barycenter correction....\" ) barycorr ( barycorr_event , event_file_a , eventsA , eventsB , gtiA , gtiB ) print ( \"Step 1 Complete: Loaded GTI and event data for modules A and B. \\n \" ) # Debugging: Print energy ranges # print(f\"Energy range in Module A: min={eventsA['Energy'].min()}, max={eventsA['Energy'].max()}\") # print(f\"Energy range in Module B: min={eventsB['Energy'].min()}, max={eventsB['Energy'].max()}\") # print(\"\\nInspecting the first 10 rows of Module A event data:\") # print(eventsA.head(10)) # print(\"\\nInspecting the first 10 rows of Module B event data:\") # print(eventsB.head(10)) ################## Step 2: Filter Events by Energy ################## print ( \" \\n Step 2: Filtering events by energy...\" ) filtered_eventsA = filter_events_by_energy ( eventsA , energy_min , energy_max ) filtered_eventsB = filter_events_by_energy ( eventsB , energy_min , energy_max ) print ( f \" Module A: { len ( filtered_eventsA ) } events remaining.\" ) print ( f \" Module B: { len ( filtered_eventsB ) } events remaining.\" ) print ( f \" Filtered events for energy range [ { energy_min } , { energy_max } ] keV.\" ) # Save or pass the filtered data for the next steps filtered_eventsA . to_csv ( output_dir + \"2_filtered_events_moduleA.csv\" , index = False ) filtered_eventsB . to_csv ( output_dir + \"2_filtered_events_moduleB.csv\" , index = False ) print ( \"Step 2 Complete: Filtered event data saved for both modules. \\n \" ) ######################## Step 3: Clean GTIs ######################## # Parameters for GTI cleaning gti_threshold = 30 # Minimum duration (seconds) gti_starttrim = 15 # Trim seconds from start gti_stoptrim = 15 # Trim seconds from stop print ( \" \\n Step 3: Cleaning GTIs...\" ) print ( \" Module A: \" ) gtiA_cleaned = clean_gti ( gtiA , filtered_eventsA , threshold = gti_threshold , starttrim = gti_starttrim , stoptrim = gti_stoptrim , ) print ( \" Module B: \" ) gtiB_cleaned = clean_gti ( gtiB , filtered_eventsB , threshold = gti_threshold , starttrim = gti_starttrim , stoptrim = gti_stoptrim , ) # Save cleaned GTIs for debugging or further steps gtiA_cleaned . to_csv ( output_dir + \"3_gtiA_cleaned.csv\" , index = False ) gtiB_cleaned . to_csv ( output_dir + \"3_gtiB_cleaned.csv\" , index = False ) print ( \"Step 3 Complete: Cleaned GTIs saved. \\n \" ) ######################## Step 4: Merge GTIs ######################## print ( \" \\n Step 4: Merging GTIs...\" ) merged_gti = merge_gtis ( gtiA_cleaned , gtiB_cleaned ) # Save merged GTIs for debugging or further steps merged_gti . to_csv ( output_dir + \"4_merged_gti.csv\" , index = False ) print ( \"Step 4 Complete: Merged GTIs saved to 'merged_gti.csv'. \\n \" ) ########### Step 5: Filtering Events Using Common GTIs ############# print ( \" \\n Step 5: Filtering events using common GTIs...\" ) print ( \" Module A: \" ) filtered_eventsA_with_gti = filter_events_with_common_gti ( filtered_eventsA , merged_gti ) print ( \" Module B: \" ) filtered_eventsB_with_gti = filter_events_with_common_gti ( filtered_eventsB , merged_gti ) # Save the filtered events for debugging or further analysis filtered_eventsA_with_gti . to_csv ( output_dir + \"5_filtered_eventsA_with_gti.csv\" , index = False ) filtered_eventsB_with_gti . to_csv ( output_dir + \"5_filtered_eventsB_with_gti.csv\" , index = False ) print ( \"Step 5 Complete: Filtered events saved. \\n \" ) ########### Step 6: Get Correction Factors for Rvents ############# print ( \" \\n Step 6: Getting correction factors for photon events...\" ) # Apply the correction factor module for Module A print ( \" Module A: \" ) filtered_eventsA_with_gti [ \"CORRECTION_FACTOR\" ] = get_event_corr_factor ( lccorrfileA , filtered_eventsA_with_gti [ \"TIME\" ] ) # Apply the correction factor module for Module B print ( \" Module B: \" ) filtered_eventsB_with_gti [ \"CORRECTION_FACTOR\" ] = get_event_corr_factor ( lccorrfileB , filtered_eventsB_with_gti [ \"TIME\" ] ) # Save the updated DataFrames filtered_eventsA_with_gti . to_csv ( output_dir + \"6_filtered_eventsA_with_corr.csv\" , index = False ) filtered_eventsB_with_gti . to_csv ( output_dir + \"6_filtered_eventsB_with_corr.csv\" , index = False ) print ( \"Step 6 Complete: Correction factors saved. \\n \" ) #################### Step 7: Merge Events ###################### print ( \" \\n Step 7: Merging photon events...\" ) if one_mod is True : print ( \"making the missing module dataframe empty...\" ) if present_mod == \"A\" : filtered_eventsB_with_gti = empty_df ( filtered_eventsB_with_gti ) elif present_mod == \"B\" : filtered_eventsA_with_gti = empty_df ( filtered_eventsA_with_gti ) # Merge photon events with exposure factors and module labels events_merged = merge_events ( filtered_eventsA_with_gti , filtered_eventsB_with_gti , filtered_eventsA_with_gti [ \"CORRECTION_FACTOR\" ] . values , filtered_eventsB_with_gti [ \"CORRECTION_FACTOR\" ] . values , ) # delete NANs if present if one_mod is True : events_merged = events_merged . dropna () # Save the merged DataFrame events_merged . to_csv ( f \" { output_dir } 7_events_merged.csv\" , index = False ) print ( \"Step 7 Complete: Merged events saved. \\n \" ) ################# Step 8: Calculate Average Count Rate ################### # Configuration flag for count rate calculation calculate_average_count_rate = True # Set to False to skip this step # Optional flare GTI DataFrame (None if not provided) flare_gti = None # Replace with the flare GTI DataFrame if available print ( \" \\n Step 8: Calculating the average count rate during GTIs...\" ) # Calculate average count rates average_rates = calculate_average_rate ( events = events_merged , gti = merged_gti , flare_gti = flare_gti , calculate_average_count_rate = calculate_average_count_rate , ) if average_rates is not None : # Save the results to a CSV average_rates . to_csv ( f \" { output_dir } 8_average_count_rates.csv\" , index = False ) print ( \"Step 8 Complete: Average count rates saved. \\n \" ) ################# Step 9: Remove Gaps between GTIs ################### print ( \" \\n Step 9: Removing gaps between GTIs...\" ) # Original observation end time original_tt_stop = merged_gti [ \"STOP\" ] . max () print ( \" Original observation stop time:\" , original_tt_stop ) # Suppress gaps in event times events_no_gaps , updated_tt_stop , cumulative_gaps = suppress_gti_gaps ( event_df = events_merged , gti_df = merged_gti , original_tt_stop = original_tt_stop ) # Save the updated event DataFrame events_no_gaps . to_csv ( f \" { output_dir } 9_events_no_gaps.csv\" , index = False ) # Optionally save cumulative gaps for debugging cumulative_gaps_df = pd . DataFrame ({ \"Cumulative Gap Time\" : cumulative_gaps }) cumulative_gaps_df . to_csv ( f \" { output_dir } 9_cumulative_gaps.csv\" , index = False ) print ( \"Step 9 Complete: Events with suppressed gaps saved. \\n \" ) ################# Step 10: Bayesian Block Analysis ################### # Config Parameters plan_a = False # Plan A (Hardcode -- does not really exist -- DO NOT USE) plan_b = True # Plan B (Custom Astropy) fp_rate = 0.01 # False positive rate # ncp_prior = None # Prior number of change points ncp_prior = 4 - np . log10 ( fp_rate / ( 0.0136 * ( len ( events_no_gaps ) ** 0.478 )) ) # As Shuo did do_iter = False # Iterative refinement flag x_list = events_no_gaps [ \"Exposure\" ] . values # x_list=np.random.uniform(low=0.4, high=0.68, size=len(events_no_gaps['TIME'].values)) #for testing purposes print ( \" \\n Step 10: Bayesian Block Analysis...\" ) if plan_a : print ( \" Using Custom Bayesian Block Implementation...\" ) results = find_blocks ( events_no_gaps [ \"TIME\" ] . values , events_no_gaps [ \"Exposure\" ] . values , fp_rate , ncp_prior , do_iter , ) bayesian_blocks_df = format_bayesian_block_output ( results , fp_rate , ncp_prior , do_iter ) # Save results bayesian_blocks_df . to_csv ( output_dir + \"10_bayesian_blocks.csv\" , index = False ) else : print ( \" Using Astropy Bayesian Block Implementation...\" ) bayesian_blocks_df = bba_astropy ( events_no_gaps [ \"TIME\" ] . values , ncp_prior , fp_rate , x_list = x_list ) # Save results bayesian_blocks_df . to_csv ( output_dir + \"10_bayesian_blocks_astropy.csv\" , index = False ) print ( \"Step 10 Complete: Bayesian Block Analysis results saved. \\n \" ) ############ Step 10.1: Detailed Analysis of the Flaring Activity Block ############ # Configuration for detailed flare analysis do_detailed_flare_analysis = True # Do detailed analysis do_detailed_flare_analysis = False # Not do detailed analysis flare_p0 = 0.5 flaring_start = 262239084.32836443 # Configured flaring start time flaring_stop = 262241049.05592683 # Configured flaring stop time print ( \" \\n Step 10.1: Detailed Analysis of Flaring Activity Block...\" ) if do_detailed_flare_analysis : # Create the flaring block flaring_block = events_no_gaps [ ( events_no_gaps [ \"TIME\" ] >= flaring_start ) & ( events_no_gaps [ \"TIME\" ] < flaring_stop ) ] if flaring_block . empty : print ( f \" No events found in the flaring interval [ { flaring_start } , { flaring_stop } ]. Skipping detailed analysis. \\n \" ) else : print ( f \" Flaring block extracted with { len ( flaring_block ) } events in the interval [ { flaring_start } , { flaring_stop } ].\" ) # Perform detailed flare analysis detailed_blocks_df = detailed_flare_analysis ( event_df = flaring_block , # Pass only the flaring block bayesian_blocks_df = bayesian_blocks_df , p0 = flare_p0 , flare_analysis_flag = do_detailed_flare_analysis , ) if detailed_blocks_df is not None : # Save the refined flare analysis results detailed_blocks_df . to_csv ( output_dir + \"10.1_detailed_flare_blocks.csv\" , index = False ) print ( \"Step 10.1 Complete: Detailed flare blocks saved. \\n \" ) else : print ( \"Skipping Step 10.1 as per configuration. \\n \" ) print ( \" \\n Step 10.5: Put change points back into real time\" ) ################ Step 11: Restore GTI Gaps to Blocks ###################### print ( \" \\n Step 11: Insert Data Gaps into Bayesian Blocks...\" ) try : # Adjust BBA blocks by reintroducing GTI gaps corrected_bba_blocks , gti_gaps_df = insert_gti_gaps ( bba_df = bayesian_blocks_df , gti_df = merged_gti , ) # Save the output files corrected_bba_csv = f \" { output_dir } 11_corrected_bba_blocks.csv\" corrected_bba_blocks . to_csv ( corrected_bba_csv , index = False ) print ( \" Corrected BBA Blocks saved.\" ) gti_gaps_csv = f \" { output_dir } 11_gti_gaps.csv\" gti_gaps_df . to_csv ( gti_gaps_csv , index = False ) print ( \" GTI Gaps saved.\" ) # Validate corrected BBA blocks against final event time final_bba_stop = corrected_bba_blocks [ \"stop\" ] . max () final_event_time = events_merged [ \"TIME\" ] . max () # From Step 7 print ( f \" Final BBA Stop Time: { final_bba_stop } \" ) print ( f \" Final Event Time: { final_event_time } \" ) if abs ( final_bba_stop - final_event_time ) > 1e-6 : print ( \"Validation Failed: Corrected BBA blocks do not align with final event time.\" ) else : print ( \"Validation Passed: Corrected BBA blocks align with final event time.\" ) except Exception as e : print ( f \"Error in Step 11: { e } \" ) print ( \"Step 11 Complete: Gaps reintroduced into Bayesian Blocks. \\n \" ) ####################### Step 12: Save BBA Results ######################### print ( \" \\n Step 12: Save Bayesian Block Analysis Results...\" ) try : # Calculate event counts, block lengths, and rates for each BBA block corrected_bba_blocks , flare_blocks = calculate_event_counts_and_rates ( corrected_bba_blocks , events_merged ) # Calculate confidence limits for each block corrected_bba_blocks = calculate_confidence_limits ( corrected_bba_blocks ) # Observation metadata analysis_params = { \"ncp_prior\" : ncp_prior , \"fp_rate\" : fp_rate , \"do_iter\" : do_iter , } # Define output file path output_file = f \" { output_dir } 12_gti_results.txt\" # Custom optional notes for each block # save_results_note = [ # \"Block 1 Note\", \"Block 2 Note\", \"Block 3 Note\", # \"Block 4 Note\", \"Block 5 Note\", \"Block 6 Note\", # \"Block 7 Note\", \"Block 8 Note\", \"Block 9 Note\", # \"Block 10 Note\", \"Block 11 Note\" # ] # Save BBA results as a text file save_bba_results_txt ( bba_df = corrected_bba_blocks , analysis_params = analysis_params , output_file = output_file , # save_results_note=save_results_note ) # save the flare version save_flare_results_txt ( bba_df = flare_blocks , analysis_params = analysis_params , output_file = f \" { output_dir } 12_bba_results.txt\" , # save_results_note=save_results_note ) print ( \"Step 12 Complete: BBA results saved. \\n \" ) except Exception as e : print ( f \"Error in Step 12: { e } \" ) ################# Step 13: Create Binned Light Curve #################### print ( \" \\n Step 13: Generate Regularly Binned Light Curve...\" ) # try: binsize = 100 # Example bin size in seconds energy_range = ( 3.0 , 79.0 ) # Example energy range in keV # Generate the light curve lightcurve_df = generate_lightcurve ( events_df = events_merged , # Use the events_merged DataFrame from Step 7 gti_df = merged_gti , # Use the GTI DataFrame from Step 4 binsize = binsize , energy_range = energy_range , gti_average = True , # Set to True if GTI-based binning ) # Save the light curve to a CSV file lightcurve_output_path = output_dir + f \"13_LC_ { binsize } .csv\" lightcurve_df . to_csv ( lightcurve_output_path , index = False ) print ( \"Step 13 Complete: Light curve saved. \\n \" ) # except Exception as e: # print(f\"Error in Step 13: {e}\") ####################### Step 14: Plot Light Curve with Bayesian Blocks ######################### print ( \" \\n Step 14: Plot Light Curve with Bayesian Blocks...\" ) # Define file paths lc_plot_path = output_dir + \"14_lightcurve_plot.png\" lc_csv_path = output_dir + \"14_lightcurve.csv\" bb_csv_path = output_dir + \"14_bayesian_blocks.csv\" # prep for plotting by getting bb count rates in gti intervals plot_frame = plot_prep ( corrected_bba_blocks , flare_blocks ) plotframe_csv_path = output_dir + \"14_plot_frame.csv\" plot_frame . to_csv ( plotframe_csv_path , index = False ) # Plot and save light curve plot_lightcurve ( lightcurve_df = lightcurve_df , # Regularly binned light curve # bb_df=corrected_bba_blocks, #old call (basically gti defined) bb_df = plot_frame , # Bayesian Blocks line_times = flare_blocks , output_path = lc_plot_path , lc_csv_path = lc_csv_path , bb_csv_path = bb_csv_path , convert_nustar_to_utc = convert_nustar_to_utc , plot_lines = True , ) print ( \"Step 14 Complete: Light Curve and Bayesian Blocks saved. \\n \" ) print ( \" \\n ------ Creating ReadMe with run information ------ \\n \" ) flags_dict = { \"Barycenter correction\" : \"True\" , \"BBA min energy\" : energy_min , \"BBA max energy\" : energy_max , \"plotting energy range\" : energy_range , \"input event file A\" : event_file_a , \"input event file B\" : event_file_b , \"input lccorr file A\" : lccorrfileA , \"input lccorr file B\" : lccorrfileB , } write_readme ( output_dir , flags_dict ) print ( \" \\n ------ End of Data Processing Pipeline ------ \\n \" )","title":"Main"},{"location":"main/#main.main","text":"Bayesian Block alorithm for use with NuSTAR data. 3 pairs of files are necessary as input lightcurve correction factor file (produced by nuproducts) event list extracted from region of interest (produced by xselect) barycorr file to do barycenter correction Has capability to handle only one module. To do that, simply comment out path names for the missing module Flow of the code define filenames and paths load events and filter by energy Load and clean GTIs Merge GTIs Filter events outside of common GTIs Get exposure correction factor Merge events from FPM A and B Get average count rates in GTIs Remove time gaps from events (in preperation for BB analysis) Perform Bayesian Block Routine Re-insert GTI gaps into data Print out txt file of BBA results Bin events into light curve Make light curve plot NOTE: parameters do not appear in the function signature, but instead are editable parameters of the code. Parameters: Name Type Description Default year str year of the observation being analyzed (used for leap second correction) required obsID str observation ID of observation being analyzed required output_dir str path to output directory required event_file_a str path to xselected event file for module A (optional) required event_file_b str path to xselected event file for module B (optional) required barycorr_event str path to barycenter corrected event file for module A or B (optional) required lccorrfileA str path to light curve correction file for module A (optional) required lccorrfileB str path to light curve correction file for module B (optional) required energy_min int energy range in keV for bayesian block analysis. Default 3.0 required energy_max int energy range in keV for bayesian block analysis. Default 30.0 required fp_rate int false positive rate used for bayesian block detection. Default 0.01 required binsize int bin size in seconds for light curve plotting. Default 100 required energy_range list energy range in keV for plotted light curve. Default: (3.0, 79.0) required Returns: 12_bba_results.txt: text file with times, count rates, upper and lower limits 14_lightcurve_plot.png: Light curve plot with 1 sigma upper and lower count rates per block ReadMe.txt: text file inputs and various script settings Grace Sanger-Johnson 9/23/2025 based on runBB.pro IDL package created by Nicolas Barriere Source code in main.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 def main (): \"\"\"Bayesian Block alorithm for use with NuSTAR data. 3 pairs of files are necessary as input: - lightcurve correction factor file (produced by nuproducts) - event list extracted from region of interest (produced by xselect) - barycorr file to do barycenter correction Has capability to handle only one module. To do that, simply comment out path names for the missing module Flow of the code: 1. define filenames and paths 2. load events and filter by energy 3. Load and clean GTIs 4. Merge GTIs 5. Filter events outside of common GTIs 6. Get exposure correction factor 7. Merge events from FPM A and B 8. Get average count rates in GTIs 9. Remove time gaps from events (in preperation for BB analysis) 10. Perform Bayesian Block Routine 11. Re-insert GTI gaps into data 12. Print out txt file of BBA results 13. Bin events into light curve 14. Make light curve plot NOTE: parameters do not appear in the function signature, but instead are editable parameters of the code. Parameters: year (str): year of the observation being analyzed (used for leap second correction) obsID (str): observation ID of observation being analyzed output_dir (str): path to output directory event_file_a (str): path to xselected event file for module A (optional) event_file_b (str): path to xselected event file for module B (optional) barycorr_event (str): path to barycenter corrected event file for module A or B (optional) lccorrfileA (str): path to light curve correction file for module A (optional) lccorrfileB (str): path to light curve correction file for module B (optional) energy_min (int): energy range in keV for bayesian block analysis. Default 3.0 energy_max (int): energy range in keV for bayesian block analysis. Default 30.0 fp_rate (int): false positive rate used for bayesian block detection. Default 0.01 binsize (int): bin size in seconds for light curve plotting. Default 100 energy_range (list): energy range in keV for plotted light curve. Default: (3.0, 79.0) Returns: 12_bba_results.txt: text file with times, count rates, upper and lower limits 14_lightcurve_plot.png: Light curve plot with 1 sigma upper and lower count rates per block ReadMe.txt: text file inputs and various script settings Grace Sanger-Johnson 9/23/2025 based on runBB.pro IDL package created by Nicolas Barriere \"\"\" year = \"2016\" obsID = \"40202001002\" path = ( \"/Users/gracesanger-johnson/xray_astro/SgrA/\" + year + \"/\" + obsID + \"/event_cl2/BBA_results\" ) # Define output directory output_dir = path + \"/3-30_BB_output/\" # make sure directory exists os . makedirs ( output_dir , exist_ok = True ) # output_dir = path+\"/testing/fp_testing/\" # Set default file paths (DO NOT EDIT): event_file_a = None event_file_b = None # Define input file paths barycorr_event = path + \"/nu\" + obsID + \"A01_cl_barycorr.evt\" event_file_a = path + \"/nu\" + obsID + \"A01_xselected.evt\" event_file_b = path + \"/nu\" + obsID + \"B01_xselected.evt\" lccorrfileA = path + \"/SgrA_correct_50ac_fpmA_lcsrccorrfile.fits\" lccorrfileB = path + \"/SgrA_correct_50ac_fpmB_lcsrccorrfile.fits\" # --- Define Parameters / General Used Ones --- energy_min = 3.0 # energy_max = 79.0 energy_max = 30.0 # --- For Step 3 / GTI Cleaning --- print ( \" \\n ------ Start Data Processing Pipeline ------ \\n \" ) ######################## Step 1: Load Data ####################### # copy other fits files if one module is missing one_mod = False if event_file_a is None : one_mod = True present_mod = \"B\" # gti_file_a, event_file_a, lccorrfileA= one_module(gti_file_b, event_file_b, lccorrfileB,output_dir) # duplicate_fits(gti_file_b, event_file_b, lccorrfileB, output_dir) duplicate_fits ( event_file_b , lccorrfileB , output_dir ) # gti_file_a = output_dir + \"gti_file_C\" event_file_a = output_dir + \"event_file_C\" lccorrfileA = output_dir + \"lccorrfile_C\" elif event_file_b is None : one_mod = True present_mod = \"A\" # gti_file_b, event_file_b, lccorrfileB = one_module(gti_file_a, event_file_a, lccorrfileA,output_dir) # duplicate_fits(gti_file_a, event_file_a, lccorrfileA, output_dir) duplicate_fits ( event_file_a , lccorrfileA , output_dir ) # gti_file_b = output_dir + \"gti_file_C\" event_file_b = output_dir + \"event_file_C\" lccorrfileB = output_dir + \"lccorrfile_C\" print ( \"Step 1: Loading GTI and event data...\" ) gtiA = load_gti_file ( event_file_a ) gtiB = load_gti_file ( event_file_b ) eventsA = load_event_file ( event_file_a ) eventsB = load_event_file ( event_file_b ) print ( f \" Module A: { len ( gtiA ) } GTI intervals, { len ( eventsA ) } events.\" ) print ( f \" Module B: { len ( gtiB ) } GTI intervals, { len ( eventsB ) } events.\" ) print ( \"Step 1.5: apply barycenter correction....\" ) barycorr ( barycorr_event , event_file_a , eventsA , eventsB , gtiA , gtiB ) print ( \"Step 1 Complete: Loaded GTI and event data for modules A and B. \\n \" ) # Debugging: Print energy ranges # print(f\"Energy range in Module A: min={eventsA['Energy'].min()}, max={eventsA['Energy'].max()}\") # print(f\"Energy range in Module B: min={eventsB['Energy'].min()}, max={eventsB['Energy'].max()}\") # print(\"\\nInspecting the first 10 rows of Module A event data:\") # print(eventsA.head(10)) # print(\"\\nInspecting the first 10 rows of Module B event data:\") # print(eventsB.head(10)) ################## Step 2: Filter Events by Energy ################## print ( \" \\n Step 2: Filtering events by energy...\" ) filtered_eventsA = filter_events_by_energy ( eventsA , energy_min , energy_max ) filtered_eventsB = filter_events_by_energy ( eventsB , energy_min , energy_max ) print ( f \" Module A: { len ( filtered_eventsA ) } events remaining.\" ) print ( f \" Module B: { len ( filtered_eventsB ) } events remaining.\" ) print ( f \" Filtered events for energy range [ { energy_min } , { energy_max } ] keV.\" ) # Save or pass the filtered data for the next steps filtered_eventsA . to_csv ( output_dir + \"2_filtered_events_moduleA.csv\" , index = False ) filtered_eventsB . to_csv ( output_dir + \"2_filtered_events_moduleB.csv\" , index = False ) print ( \"Step 2 Complete: Filtered event data saved for both modules. \\n \" ) ######################## Step 3: Clean GTIs ######################## # Parameters for GTI cleaning gti_threshold = 30 # Minimum duration (seconds) gti_starttrim = 15 # Trim seconds from start gti_stoptrim = 15 # Trim seconds from stop print ( \" \\n Step 3: Cleaning GTIs...\" ) print ( \" Module A: \" ) gtiA_cleaned = clean_gti ( gtiA , filtered_eventsA , threshold = gti_threshold , starttrim = gti_starttrim , stoptrim = gti_stoptrim , ) print ( \" Module B: \" ) gtiB_cleaned = clean_gti ( gtiB , filtered_eventsB , threshold = gti_threshold , starttrim = gti_starttrim , stoptrim = gti_stoptrim , ) # Save cleaned GTIs for debugging or further steps gtiA_cleaned . to_csv ( output_dir + \"3_gtiA_cleaned.csv\" , index = False ) gtiB_cleaned . to_csv ( output_dir + \"3_gtiB_cleaned.csv\" , index = False ) print ( \"Step 3 Complete: Cleaned GTIs saved. \\n \" ) ######################## Step 4: Merge GTIs ######################## print ( \" \\n Step 4: Merging GTIs...\" ) merged_gti = merge_gtis ( gtiA_cleaned , gtiB_cleaned ) # Save merged GTIs for debugging or further steps merged_gti . to_csv ( output_dir + \"4_merged_gti.csv\" , index = False ) print ( \"Step 4 Complete: Merged GTIs saved to 'merged_gti.csv'. \\n \" ) ########### Step 5: Filtering Events Using Common GTIs ############# print ( \" \\n Step 5: Filtering events using common GTIs...\" ) print ( \" Module A: \" ) filtered_eventsA_with_gti = filter_events_with_common_gti ( filtered_eventsA , merged_gti ) print ( \" Module B: \" ) filtered_eventsB_with_gti = filter_events_with_common_gti ( filtered_eventsB , merged_gti ) # Save the filtered events for debugging or further analysis filtered_eventsA_with_gti . to_csv ( output_dir + \"5_filtered_eventsA_with_gti.csv\" , index = False ) filtered_eventsB_with_gti . to_csv ( output_dir + \"5_filtered_eventsB_with_gti.csv\" , index = False ) print ( \"Step 5 Complete: Filtered events saved. \\n \" ) ########### Step 6: Get Correction Factors for Rvents ############# print ( \" \\n Step 6: Getting correction factors for photon events...\" ) # Apply the correction factor module for Module A print ( \" Module A: \" ) filtered_eventsA_with_gti [ \"CORRECTION_FACTOR\" ] = get_event_corr_factor ( lccorrfileA , filtered_eventsA_with_gti [ \"TIME\" ] ) # Apply the correction factor module for Module B print ( \" Module B: \" ) filtered_eventsB_with_gti [ \"CORRECTION_FACTOR\" ] = get_event_corr_factor ( lccorrfileB , filtered_eventsB_with_gti [ \"TIME\" ] ) # Save the updated DataFrames filtered_eventsA_with_gti . to_csv ( output_dir + \"6_filtered_eventsA_with_corr.csv\" , index = False ) filtered_eventsB_with_gti . to_csv ( output_dir + \"6_filtered_eventsB_with_corr.csv\" , index = False ) print ( \"Step 6 Complete: Correction factors saved. \\n \" ) #################### Step 7: Merge Events ###################### print ( \" \\n Step 7: Merging photon events...\" ) if one_mod is True : print ( \"making the missing module dataframe empty...\" ) if present_mod == \"A\" : filtered_eventsB_with_gti = empty_df ( filtered_eventsB_with_gti ) elif present_mod == \"B\" : filtered_eventsA_with_gti = empty_df ( filtered_eventsA_with_gti ) # Merge photon events with exposure factors and module labels events_merged = merge_events ( filtered_eventsA_with_gti , filtered_eventsB_with_gti , filtered_eventsA_with_gti [ \"CORRECTION_FACTOR\" ] . values , filtered_eventsB_with_gti [ \"CORRECTION_FACTOR\" ] . values , ) # delete NANs if present if one_mod is True : events_merged = events_merged . dropna () # Save the merged DataFrame events_merged . to_csv ( f \" { output_dir } 7_events_merged.csv\" , index = False ) print ( \"Step 7 Complete: Merged events saved. \\n \" ) ################# Step 8: Calculate Average Count Rate ################### # Configuration flag for count rate calculation calculate_average_count_rate = True # Set to False to skip this step # Optional flare GTI DataFrame (None if not provided) flare_gti = None # Replace with the flare GTI DataFrame if available print ( \" \\n Step 8: Calculating the average count rate during GTIs...\" ) # Calculate average count rates average_rates = calculate_average_rate ( events = events_merged , gti = merged_gti , flare_gti = flare_gti , calculate_average_count_rate = calculate_average_count_rate , ) if average_rates is not None : # Save the results to a CSV average_rates . to_csv ( f \" { output_dir } 8_average_count_rates.csv\" , index = False ) print ( \"Step 8 Complete: Average count rates saved. \\n \" ) ################# Step 9: Remove Gaps between GTIs ################### print ( \" \\n Step 9: Removing gaps between GTIs...\" ) # Original observation end time original_tt_stop = merged_gti [ \"STOP\" ] . max () print ( \" Original observation stop time:\" , original_tt_stop ) # Suppress gaps in event times events_no_gaps , updated_tt_stop , cumulative_gaps = suppress_gti_gaps ( event_df = events_merged , gti_df = merged_gti , original_tt_stop = original_tt_stop ) # Save the updated event DataFrame events_no_gaps . to_csv ( f \" { output_dir } 9_events_no_gaps.csv\" , index = False ) # Optionally save cumulative gaps for debugging cumulative_gaps_df = pd . DataFrame ({ \"Cumulative Gap Time\" : cumulative_gaps }) cumulative_gaps_df . to_csv ( f \" { output_dir } 9_cumulative_gaps.csv\" , index = False ) print ( \"Step 9 Complete: Events with suppressed gaps saved. \\n \" ) ################# Step 10: Bayesian Block Analysis ################### # Config Parameters plan_a = False # Plan A (Hardcode -- does not really exist -- DO NOT USE) plan_b = True # Plan B (Custom Astropy) fp_rate = 0.01 # False positive rate # ncp_prior = None # Prior number of change points ncp_prior = 4 - np . log10 ( fp_rate / ( 0.0136 * ( len ( events_no_gaps ) ** 0.478 )) ) # As Shuo did do_iter = False # Iterative refinement flag x_list = events_no_gaps [ \"Exposure\" ] . values # x_list=np.random.uniform(low=0.4, high=0.68, size=len(events_no_gaps['TIME'].values)) #for testing purposes print ( \" \\n Step 10: Bayesian Block Analysis...\" ) if plan_a : print ( \" Using Custom Bayesian Block Implementation...\" ) results = find_blocks ( events_no_gaps [ \"TIME\" ] . values , events_no_gaps [ \"Exposure\" ] . values , fp_rate , ncp_prior , do_iter , ) bayesian_blocks_df = format_bayesian_block_output ( results , fp_rate , ncp_prior , do_iter ) # Save results bayesian_blocks_df . to_csv ( output_dir + \"10_bayesian_blocks.csv\" , index = False ) else : print ( \" Using Astropy Bayesian Block Implementation...\" ) bayesian_blocks_df = bba_astropy ( events_no_gaps [ \"TIME\" ] . values , ncp_prior , fp_rate , x_list = x_list ) # Save results bayesian_blocks_df . to_csv ( output_dir + \"10_bayesian_blocks_astropy.csv\" , index = False ) print ( \"Step 10 Complete: Bayesian Block Analysis results saved. \\n \" ) ############ Step 10.1: Detailed Analysis of the Flaring Activity Block ############ # Configuration for detailed flare analysis do_detailed_flare_analysis = True # Do detailed analysis do_detailed_flare_analysis = False # Not do detailed analysis flare_p0 = 0.5 flaring_start = 262239084.32836443 # Configured flaring start time flaring_stop = 262241049.05592683 # Configured flaring stop time print ( \" \\n Step 10.1: Detailed Analysis of Flaring Activity Block...\" ) if do_detailed_flare_analysis : # Create the flaring block flaring_block = events_no_gaps [ ( events_no_gaps [ \"TIME\" ] >= flaring_start ) & ( events_no_gaps [ \"TIME\" ] < flaring_stop ) ] if flaring_block . empty : print ( f \" No events found in the flaring interval [ { flaring_start } , { flaring_stop } ]. Skipping detailed analysis. \\n \" ) else : print ( f \" Flaring block extracted with { len ( flaring_block ) } events in the interval [ { flaring_start } , { flaring_stop } ].\" ) # Perform detailed flare analysis detailed_blocks_df = detailed_flare_analysis ( event_df = flaring_block , # Pass only the flaring block bayesian_blocks_df = bayesian_blocks_df , p0 = flare_p0 , flare_analysis_flag = do_detailed_flare_analysis , ) if detailed_blocks_df is not None : # Save the refined flare analysis results detailed_blocks_df . to_csv ( output_dir + \"10.1_detailed_flare_blocks.csv\" , index = False ) print ( \"Step 10.1 Complete: Detailed flare blocks saved. \\n \" ) else : print ( \"Skipping Step 10.1 as per configuration. \\n \" ) print ( \" \\n Step 10.5: Put change points back into real time\" ) ################ Step 11: Restore GTI Gaps to Blocks ###################### print ( \" \\n Step 11: Insert Data Gaps into Bayesian Blocks...\" ) try : # Adjust BBA blocks by reintroducing GTI gaps corrected_bba_blocks , gti_gaps_df = insert_gti_gaps ( bba_df = bayesian_blocks_df , gti_df = merged_gti , ) # Save the output files corrected_bba_csv = f \" { output_dir } 11_corrected_bba_blocks.csv\" corrected_bba_blocks . to_csv ( corrected_bba_csv , index = False ) print ( \" Corrected BBA Blocks saved.\" ) gti_gaps_csv = f \" { output_dir } 11_gti_gaps.csv\" gti_gaps_df . to_csv ( gti_gaps_csv , index = False ) print ( \" GTI Gaps saved.\" ) # Validate corrected BBA blocks against final event time final_bba_stop = corrected_bba_blocks [ \"stop\" ] . max () final_event_time = events_merged [ \"TIME\" ] . max () # From Step 7 print ( f \" Final BBA Stop Time: { final_bba_stop } \" ) print ( f \" Final Event Time: { final_event_time } \" ) if abs ( final_bba_stop - final_event_time ) > 1e-6 : print ( \"Validation Failed: Corrected BBA blocks do not align with final event time.\" ) else : print ( \"Validation Passed: Corrected BBA blocks align with final event time.\" ) except Exception as e : print ( f \"Error in Step 11: { e } \" ) print ( \"Step 11 Complete: Gaps reintroduced into Bayesian Blocks. \\n \" ) ####################### Step 12: Save BBA Results ######################### print ( \" \\n Step 12: Save Bayesian Block Analysis Results...\" ) try : # Calculate event counts, block lengths, and rates for each BBA block corrected_bba_blocks , flare_blocks = calculate_event_counts_and_rates ( corrected_bba_blocks , events_merged ) # Calculate confidence limits for each block corrected_bba_blocks = calculate_confidence_limits ( corrected_bba_blocks ) # Observation metadata analysis_params = { \"ncp_prior\" : ncp_prior , \"fp_rate\" : fp_rate , \"do_iter\" : do_iter , } # Define output file path output_file = f \" { output_dir } 12_gti_results.txt\" # Custom optional notes for each block # save_results_note = [ # \"Block 1 Note\", \"Block 2 Note\", \"Block 3 Note\", # \"Block 4 Note\", \"Block 5 Note\", \"Block 6 Note\", # \"Block 7 Note\", \"Block 8 Note\", \"Block 9 Note\", # \"Block 10 Note\", \"Block 11 Note\" # ] # Save BBA results as a text file save_bba_results_txt ( bba_df = corrected_bba_blocks , analysis_params = analysis_params , output_file = output_file , # save_results_note=save_results_note ) # save the flare version save_flare_results_txt ( bba_df = flare_blocks , analysis_params = analysis_params , output_file = f \" { output_dir } 12_bba_results.txt\" , # save_results_note=save_results_note ) print ( \"Step 12 Complete: BBA results saved. \\n \" ) except Exception as e : print ( f \"Error in Step 12: { e } \" ) ################# Step 13: Create Binned Light Curve #################### print ( \" \\n Step 13: Generate Regularly Binned Light Curve...\" ) # try: binsize = 100 # Example bin size in seconds energy_range = ( 3.0 , 79.0 ) # Example energy range in keV # Generate the light curve lightcurve_df = generate_lightcurve ( events_df = events_merged , # Use the events_merged DataFrame from Step 7 gti_df = merged_gti , # Use the GTI DataFrame from Step 4 binsize = binsize , energy_range = energy_range , gti_average = True , # Set to True if GTI-based binning ) # Save the light curve to a CSV file lightcurve_output_path = output_dir + f \"13_LC_ { binsize } .csv\" lightcurve_df . to_csv ( lightcurve_output_path , index = False ) print ( \"Step 13 Complete: Light curve saved. \\n \" ) # except Exception as e: # print(f\"Error in Step 13: {e}\") ####################### Step 14: Plot Light Curve with Bayesian Blocks ######################### print ( \" \\n Step 14: Plot Light Curve with Bayesian Blocks...\" ) # Define file paths lc_plot_path = output_dir + \"14_lightcurve_plot.png\" lc_csv_path = output_dir + \"14_lightcurve.csv\" bb_csv_path = output_dir + \"14_bayesian_blocks.csv\" # prep for plotting by getting bb count rates in gti intervals plot_frame = plot_prep ( corrected_bba_blocks , flare_blocks ) plotframe_csv_path = output_dir + \"14_plot_frame.csv\" plot_frame . to_csv ( plotframe_csv_path , index = False ) # Plot and save light curve plot_lightcurve ( lightcurve_df = lightcurve_df , # Regularly binned light curve # bb_df=corrected_bba_blocks, #old call (basically gti defined) bb_df = plot_frame , # Bayesian Blocks line_times = flare_blocks , output_path = lc_plot_path , lc_csv_path = lc_csv_path , bb_csv_path = bb_csv_path , convert_nustar_to_utc = convert_nustar_to_utc , plot_lines = True , ) print ( \"Step 14 Complete: Light Curve and Bayesian Blocks saved. \\n \" ) print ( \" \\n ------ Creating ReadMe with run information ------ \\n \" ) flags_dict = { \"Barycenter correction\" : \"True\" , \"BBA min energy\" : energy_min , \"BBA max energy\" : energy_max , \"plotting energy range\" : energy_range , \"input event file A\" : event_file_a , \"input event file B\" : event_file_b , \"input lccorr file A\" : lccorrfileA , \"input lccorr file B\" : lccorrfileB , } write_readme ( output_dir , flags_dict ) print ( \" \\n ------ End of Data Processing Pipeline ------ \\n \" )","title":"main"},{"location":"make_readme/","text":"write_readme ( output_dir , flags_dict , filename = 'README.txt' , mode = 'w' ) Write a README text file with timestamp and user-defined flags Parameters: Name Type Description Default output_dir str Directory where README file will be saved. required flags_dict dict Dictionary of flags/inputs to record. required filename str Name of the README file (default \"README.txt\"). 'README.txt' mode str File mode: \"a\" to append, \"w\" to overwrite (default \"w\"). 'w' Source code in scripts/make_readme.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def write_readme ( output_dir , flags_dict , filename = \"README.txt\" , mode = \"w\" ): \"\"\" Write a README text file with timestamp and user-defined flags Parameters: output_dir (str): Directory where README file will be saved. flags_dict (dict): Dictionary of flags/inputs to record. filename(str): Name of the README file (default \"README.txt\"). mode (str): File mode: \"a\" to append, \"w\" to overwrite (default \"w\"). \"\"\" os . makedirs ( output_dir , exist_ok = True ) # Ensure directory exists filepath = os . path . join ( output_dir , filename ) with open ( filepath , mode ) as f : f . write ( \"=== Run Information === \\n \" ) f . write ( f \"Timestamp: { datetime . now () . strftime ( '%Y-%m- %d %H:%M:%S' ) } \\n \" ) f . write ( \"Flags and Inputs: \\n \" ) for key , value in flags_dict . items (): f . write ( f \" { key } : { value } \\n \" ) f . write ( \" \\n \" ) # Blank line for readability print ( f \"README written to { filepath } \" )","title":"Make readme"},{"location":"make_readme/#scripts.make_readme.write_readme","text":"Write a README text file with timestamp and user-defined flags Parameters: Name Type Description Default output_dir str Directory where README file will be saved. required flags_dict dict Dictionary of flags/inputs to record. required filename str Name of the README file (default \"README.txt\"). 'README.txt' mode str File mode: \"a\" to append, \"w\" to overwrite (default \"w\"). 'w' Source code in scripts/make_readme.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def write_readme ( output_dir , flags_dict , filename = \"README.txt\" , mode = \"w\" ): \"\"\" Write a README text file with timestamp and user-defined flags Parameters: output_dir (str): Directory where README file will be saved. flags_dict (dict): Dictionary of flags/inputs to record. filename(str): Name of the README file (default \"README.txt\"). mode (str): File mode: \"a\" to append, \"w\" to overwrite (default \"w\"). \"\"\" os . makedirs ( output_dir , exist_ok = True ) # Ensure directory exists filepath = os . path . join ( output_dir , filename ) with open ( filepath , mode ) as f : f . write ( \"=== Run Information === \\n \" ) f . write ( f \"Timestamp: { datetime . now () . strftime ( '%Y-%m- %d %H:%M:%S' ) } \\n \" ) f . write ( \"Flags and Inputs: \\n \" ) for key , value in flags_dict . items (): f . write ( f \" { key } : { value } \\n \" ) f . write ( \" \\n \" ) # Blank line for readability print ( f \"README written to { filepath } \" )","title":"write_readme"},{"location":"merge_events/","text":"merge_event ( events , exposure ) Merge photon event data from one module into a unified dataset. Parameters: Name Type Description Default events DataFrame Filtered event data for one module, including TIME and Energy.. required exposure ndarray Correction factors for one module events. required Returns: Type Description DataFrame Unified photon event DataFrame with TIME, Energy, Exposure, and Module columns. Source code in scripts/merge_events.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def merge_event ( events , exposure ): \"\"\" Merge photon event data from one module into a unified dataset. Parameters: events (pd.DataFrame): Filtered event data for one module, including TIME and Energy.. exposure (np.ndarray): Correction factors for one module events. Returns: (pd.DataFrame): Unified photon event DataFrame with TIME, Energy, Exposure, and Module columns. \"\"\" try : # Add correction factors and module labels to each DataFrame events [ \"Exposure\" ] = exposure # Debugging: # print(\"\\n1\") # # Check the lengths of events and exposures # print(f\"Length of eventsA: {len(eventsA)}, Length of exposureA: {len(exposureA)}\") # if len(eventsA) != len(exposureA): # print(\"Warning: Mismatch detected between eventsA and exposureA!\") # print(f\"Length of eventsB: {len(eventsB)}, Length of exposureB: {len(exposureB)}\") # if len(eventsB) != len(exposureB): # print(\"Warning: Mismatch detected between eventsB and exposureB!\") # Concatenate the DataFrames events_combined = events # Debugging: # print(\"\\n2\") # print(f\"Rows in Module A before concatenation: {len(eventsA)}\") # print(f\"Rows in Module B before concatenation: {len(eventsB)}\") # print(f\"Total rows after concatenation: {len(events_combined)}\") # Sort by TIME events_combined = events_combined . sort_values ( by = \"TIME\" ) . reset_index ( drop = True ) # Debugging: # print(\"\\n3\") # print(f\"Number of rows before sorting: {len(events_combined)}\") # print(f\"Number of NaN TIME values: {events_combined['TIME'].isna().sum()}\\n\") # Log statistics for the merged DataFrame print ( f \" Number of events Module: { len ( events ) } \" ) # print(f\" Number of events Module B: {len(eventsB)}\") print ( f \" Total events after merging: { len ( events_combined ) } \" ) print ( f \" Time range: { events_combined [ 'TIME' ] . min () } - { events_combined [ 'TIME' ] . max () } \" ) print ( f \" Energy range: { events_combined [ 'Energy' ] . min () } - { events_combined [ 'Energy' ] . max () } \" ) return events_combined except Exception as e : raise RuntimeError ( f \"Error in merging events: { e } \" ) merge_events ( eventsA , eventsB , exposureA , exposureB ) Merge photon event data from modules A and B into a unified dataset. Parameters: Name Type Description Default eventsA DataFrame Filtered event data for module A, including TIME and Energy. required eventsB DataFrame Filtered event data for module B, including TIME and Energy. required exposureA ndarray Correction factors for module A events. required exposureB ndarray Correction factors for module B events. required Returns: Type Description DataFrame Unified photon event DataFrame with TIME, Energy, Exposure, and Module columns. Source code in scripts/merge_events.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def merge_events ( eventsA , eventsB , exposureA , exposureB ): \"\"\" Merge photon event data from modules A and B into a unified dataset. Parameters: eventsA (pd.DataFrame): Filtered event data for module A, including TIME and Energy. eventsB (pd.DataFrame): Filtered event data for module B, including TIME and Energy. exposureA (np.ndarray): Correction factors for module A events. exposureB (np.ndarray): Correction factors for module B events. Returns: (pd.DataFrame): Unified photon event DataFrame with TIME, Energy, Exposure, and Module columns. \"\"\" try : # Add correction factors and module labels to each DataFrame eventsA [ \"Exposure\" ] = exposureA eventsA [ \"Module\" ] = \"A\" eventsB [ \"Exposure\" ] = exposureB eventsB [ \"Module\" ] = \"B\" # Concatenate the DataFrames events_combined = pd . concat ([ eventsA , eventsB ], ignore_index = True ) # Sort by TIME events_combined = events_combined . sort_values ( by = \"TIME\" ) . reset_index ( drop = True ) # Log statistics for the merged DataFrame print ( f \" Number of events Module A: { len ( eventsA ) } \" ) print ( f \" Number of events Module B: { len ( eventsB ) } \" ) print ( f \" Total events after merging: { len ( events_combined ) } \" ) print ( f \" Time range: { events_combined [ 'TIME' ] . min () } - { events_combined [ 'TIME' ] . max () } \" ) print ( f \" Energy range: { events_combined [ 'Energy' ] . min () } - { events_combined [ 'Energy' ] . max () } \" ) return events_combined except Exception as e : raise RuntimeError ( f \"Error in merging events: { e } \" )","title":"Merge events"},{"location":"merge_events/#scripts.merge_events.merge_event","text":"Merge photon event data from one module into a unified dataset. Parameters: Name Type Description Default events DataFrame Filtered event data for one module, including TIME and Energy.. required exposure ndarray Correction factors for one module events. required Returns: Type Description DataFrame Unified photon event DataFrame with TIME, Energy, Exposure, and Module columns. Source code in scripts/merge_events.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def merge_event ( events , exposure ): \"\"\" Merge photon event data from one module into a unified dataset. Parameters: events (pd.DataFrame): Filtered event data for one module, including TIME and Energy.. exposure (np.ndarray): Correction factors for one module events. Returns: (pd.DataFrame): Unified photon event DataFrame with TIME, Energy, Exposure, and Module columns. \"\"\" try : # Add correction factors and module labels to each DataFrame events [ \"Exposure\" ] = exposure # Debugging: # print(\"\\n1\") # # Check the lengths of events and exposures # print(f\"Length of eventsA: {len(eventsA)}, Length of exposureA: {len(exposureA)}\") # if len(eventsA) != len(exposureA): # print(\"Warning: Mismatch detected between eventsA and exposureA!\") # print(f\"Length of eventsB: {len(eventsB)}, Length of exposureB: {len(exposureB)}\") # if len(eventsB) != len(exposureB): # print(\"Warning: Mismatch detected between eventsB and exposureB!\") # Concatenate the DataFrames events_combined = events # Debugging: # print(\"\\n2\") # print(f\"Rows in Module A before concatenation: {len(eventsA)}\") # print(f\"Rows in Module B before concatenation: {len(eventsB)}\") # print(f\"Total rows after concatenation: {len(events_combined)}\") # Sort by TIME events_combined = events_combined . sort_values ( by = \"TIME\" ) . reset_index ( drop = True ) # Debugging: # print(\"\\n3\") # print(f\"Number of rows before sorting: {len(events_combined)}\") # print(f\"Number of NaN TIME values: {events_combined['TIME'].isna().sum()}\\n\") # Log statistics for the merged DataFrame print ( f \" Number of events Module: { len ( events ) } \" ) # print(f\" Number of events Module B: {len(eventsB)}\") print ( f \" Total events after merging: { len ( events_combined ) } \" ) print ( f \" Time range: { events_combined [ 'TIME' ] . min () } - { events_combined [ 'TIME' ] . max () } \" ) print ( f \" Energy range: { events_combined [ 'Energy' ] . min () } - { events_combined [ 'Energy' ] . max () } \" ) return events_combined except Exception as e : raise RuntimeError ( f \"Error in merging events: { e } \" )","title":"merge_event"},{"location":"merge_events/#scripts.merge_events.merge_events","text":"Merge photon event data from modules A and B into a unified dataset. Parameters: Name Type Description Default eventsA DataFrame Filtered event data for module A, including TIME and Energy. required eventsB DataFrame Filtered event data for module B, including TIME and Energy. required exposureA ndarray Correction factors for module A events. required exposureB ndarray Correction factors for module B events. required Returns: Type Description DataFrame Unified photon event DataFrame with TIME, Energy, Exposure, and Module columns. Source code in scripts/merge_events.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 def merge_events ( eventsA , eventsB , exposureA , exposureB ): \"\"\" Merge photon event data from modules A and B into a unified dataset. Parameters: eventsA (pd.DataFrame): Filtered event data for module A, including TIME and Energy. eventsB (pd.DataFrame): Filtered event data for module B, including TIME and Energy. exposureA (np.ndarray): Correction factors for module A events. exposureB (np.ndarray): Correction factors for module B events. Returns: (pd.DataFrame): Unified photon event DataFrame with TIME, Energy, Exposure, and Module columns. \"\"\" try : # Add correction factors and module labels to each DataFrame eventsA [ \"Exposure\" ] = exposureA eventsA [ \"Module\" ] = \"A\" eventsB [ \"Exposure\" ] = exposureB eventsB [ \"Module\" ] = \"B\" # Concatenate the DataFrames events_combined = pd . concat ([ eventsA , eventsB ], ignore_index = True ) # Sort by TIME events_combined = events_combined . sort_values ( by = \"TIME\" ) . reset_index ( drop = True ) # Log statistics for the merged DataFrame print ( f \" Number of events Module A: { len ( eventsA ) } \" ) print ( f \" Number of events Module B: { len ( eventsB ) } \" ) print ( f \" Total events after merging: { len ( events_combined ) } \" ) print ( f \" Time range: { events_combined [ 'TIME' ] . min () } - { events_combined [ 'TIME' ] . max () } \" ) print ( f \" Energy range: { events_combined [ 'Energy' ] . min () } - { events_combined [ 'Energy' ] . max () } \" ) return events_combined except Exception as e : raise RuntimeError ( f \"Error in merging events: { e } \" )","title":"merge_events"},{"location":"merge_gti/","text":"merge_gtis ( gtiA , gtiB ) Merge GTIs from modules A and B by finding overlapping intervals. Parameters: Name Type Description Default gtiA DataFrame Cleaned GTI DataFrame from module A with 'START' and 'STOP'. required gtiB DataFrame Cleaned GTI DataFrame from module B with 'START' and 'STOP'. required Returns: Type Description DataFrame Merged GTI DataFrame with 'START' and 'STOP' columns. Source code in scripts/merge_gtis.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def merge_gtis ( gtiA , gtiB ): \"\"\" Merge GTIs from modules A and B by finding overlapping intervals. Parameters: gtiA (pd.DataFrame): Cleaned GTI DataFrame from module A with 'START' and 'STOP'. gtiB (pd.DataFrame): Cleaned GTI DataFrame from module B with 'START' and 'STOP'. Returns: (pd.DataFrame): Merged GTI DataFrame with 'START' and 'STOP' columns. \"\"\" try : merged_gti = [] # Iterate through all GTIs in A and B for _ , rowA in gtiA . iterrows (): for _ , rowB in gtiB . iterrows (): # Find the overlap between the two intervals overlap_start = max ( rowA [ \"START\" ], rowB [ \"START\" ]) overlap_stop = min ( rowA [ \"STOP\" ], rowB [ \"STOP\" ]) # Add the interval if there is a valid overlap if overlap_start < overlap_stop : merged_gti . append ({ \"START\" : overlap_start , \"STOP\" : overlap_stop }) # Convert to DataFrame merged_gti = pd . DataFrame ( merged_gti ) print ( f \" GTIs in module A: { len ( gtiA ) } \" ) print ( f \" GTIs in module B: { len ( gtiB ) } \" ) print ( f \" GTIs after merging: { len ( merged_gti ) } \" ) return merged_gti except Exception as e : raise RuntimeError ( f \"Error during GTI merging: { e } \" )","title":"Merge gti"},{"location":"merge_gti/#scripts.merge_gtis.merge_gtis","text":"Merge GTIs from modules A and B by finding overlapping intervals. Parameters: Name Type Description Default gtiA DataFrame Cleaned GTI DataFrame from module A with 'START' and 'STOP'. required gtiB DataFrame Cleaned GTI DataFrame from module B with 'START' and 'STOP'. required Returns: Type Description DataFrame Merged GTI DataFrame with 'START' and 'STOP' columns. Source code in scripts/merge_gtis.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def merge_gtis ( gtiA , gtiB ): \"\"\" Merge GTIs from modules A and B by finding overlapping intervals. Parameters: gtiA (pd.DataFrame): Cleaned GTI DataFrame from module A with 'START' and 'STOP'. gtiB (pd.DataFrame): Cleaned GTI DataFrame from module B with 'START' and 'STOP'. Returns: (pd.DataFrame): Merged GTI DataFrame with 'START' and 'STOP' columns. \"\"\" try : merged_gti = [] # Iterate through all GTIs in A and B for _ , rowA in gtiA . iterrows (): for _ , rowB in gtiB . iterrows (): # Find the overlap between the two intervals overlap_start = max ( rowA [ \"START\" ], rowB [ \"START\" ]) overlap_stop = min ( rowA [ \"STOP\" ], rowB [ \"STOP\" ]) # Add the interval if there is a valid overlap if overlap_start < overlap_stop : merged_gti . append ({ \"START\" : overlap_start , \"STOP\" : overlap_stop }) # Convert to DataFrame merged_gti = pd . DataFrame ( merged_gti ) print ( f \" GTIs in module A: { len ( gtiA ) } \" ) print ( f \" GTIs in module B: { len ( gtiB ) } \" ) print ( f \" GTIs after merging: { len ( merged_gti ) } \" ) return merged_gti except Exception as e : raise RuntimeError ( f \"Error during GTI merging: { e } \" )","title":"merge_gtis"},{"location":"plot_lc/","text":"plot_lightcurve ( lightcurve_df , bb_df , output_path , lc_csv_path , bb_csv_path , line_times , convert_nustar_to_utc , plot_lines = False , title = 'Light Curve' , xlabel = 'Time (UTC)' , ylabel = 'Count Rate (cts/s)' ) Plot the regularly binned light curve with Bayesian Block overlay, convert times to UTC, and save final data. - Saves the light curve plot as an image file. - Saves the processed Light Curve DataFrame (`lightcurve_df`) as a `.csv` file. - Saves the processed Bayesian Block DataFrame (`bb_df`) as a `.csv` file. Parameters: Name Type Description Default lightcurve_df DataFrame required bb_df DataFrame required output_path str required lc_csv_path str output path for lightcurve csv required bb_csv_path str output path for bayesian block csv required line_times array UTC times for vertical lines to plot required convert_nustar_to_utc function Convert NuSTAR mission time to UTC. required plot_lines Boolean (default False) False title str (default \"Light Curve\") 'Light Curve' xlabel str (default \"Time (UTC)) 'Time (UTC)' ylabel str (default \"Count Rate (cts/s)\") 'Count Rate (cts/s)' Source code in scripts/plot_lc.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 def plot_lightcurve ( lightcurve_df , bb_df , output_path , lc_csv_path , bb_csv_path , line_times , convert_nustar_to_utc , plot_lines = False , title = \"Light Curve\" , xlabel = \"Time (UTC)\" , ylabel = \"Count Rate (cts/s)\" , ): \"\"\" Plot the regularly binned light curve with Bayesian Block overlay, convert times to UTC, and save final data. - Saves the light curve plot as an image file. - Saves the processed Light Curve DataFrame (`lightcurve_df`) as a `.csv` file. - Saves the processed Bayesian Block DataFrame (`bb_df`) as a `.csv` file. Parameters: lightcurve_df (pd.DataFrame): bb_df (pd.DataFrame): output_path (str): lc_csv_path (str): output path for lightcurve csv bb_csv_path (str):output path for bayesian block csv line_times (array): UTC times for vertical lines to plot convert_nustar_to_utc (function): Convert NuSTAR mission time to UTC. plot_lines (Boolean): (default False) title (str): (default \"Light Curve\") xlabel (str): (default \"Time (UTC)) ylabel (str): (default \"Count Rate (cts/s)\") \"\"\" if lightcurve_df . empty : raise ValueError ( \"Error: Light curve DataFrame is empty. Cannot plot.\" ) # ---- 1\ufe0f Compute Bin Centers and Error Bars in Mission Time (FLOATS) ---- bin_centers = ( lightcurve_df [ \"bin_start\" ] + lightcurve_df [ \"bin_end\" ]) / 2 bin_centers = mdates . date2num ( bin_centers . apply ( convert_nustar_to_utc )) bin_half_widths = ( lightcurve_df [ \"bin_end\" ] - lightcurve_df [ \"bin_start\" ]) / 2 bin_half_widths = bin_half_widths / 86400 vertical_errors = [ lightcurve_df [ \"count_rate\" ] - lightcurve_df [ \"lower_limit\" ], # Lower error lightcurve_df [ \"upper_limit\" ] - lightcurve_df [ \"count_rate\" ], # Upper error ] # ---- 2\ufe0f Convert Time to UTC (AFTER Calculations) ---- lightcurve_df [ \"UTC_bin_start\" ] = lightcurve_df [ \"bin_start\" ] . apply ( convert_nustar_to_utc ) lightcurve_df [ \"UTC_bin_end\" ] = lightcurve_df [ \"bin_end\" ] . apply ( convert_nustar_to_utc ) # force conversion to datetime # lightcurve_df['UTC_bin_start'] = mdates.date2num(lightcurve_df['UTC_bin_start']) # lightcurve_df['UTC_bin_end'] = mdates.date2num(lightcurve_df['UTC_bin_end']) # Convert bin centers **AFTER computing as float** # lightcurve_df['UTC_bin_center'] = pd.Series(bin_centers).apply(convert_nustar_to_utc) lightcurve_df [ \"UTC_bin_center\" ] = pd . Series ( bin_centers ) # Ensure only UTC columns are strings lightcurve_df [ \"UTC_bin_start\" ] = lightcurve_df [ \"UTC_bin_start\" ] . astype ( str ) lightcurve_df [ \"UTC_bin_end\" ] = lightcurve_df [ \"UTC_bin_end\" ] . astype ( str ) lightcurve_df [ \"UTC_bin_center\" ] = lightcurve_df [ \"UTC_bin_center\" ] . astype ( str ) # ---- 3\ufe0f Convert Bayesian Blocks Time to UTC ---- if not bb_df . empty : bb_df [ \"UTC_start\" ] = bb_df [ \"start\" ] . apply ( convert_nustar_to_utc ) # .astype(str) bb_df [ \"UTC_stop\" ] = bb_df [ \"stop\" ] . apply ( convert_nustar_to_utc ) # .astype(str) # force datetime conversion bb_df [ \"UTC_start\" ] = mdates . date2num ( bb_df [ \"UTC_start\" ]) bb_df [ \"UTC_stop\" ] = mdates . date2num ( bb_df [ \"UTC_stop\" ]) line_times [ \"UTC_start\" ] = line_times [ \"start\" ] . apply ( convert_nustar_to_utc ) # .astype(str) line_times [ \"UTC_stop\" ] = line_times [ \"stop\" ] . apply ( convert_nustar_to_utc ) # .astype(str) line_times [ \"UTC_start\" ] = mdates . date2num ( line_times [ \"UTC_start\" ]) line_times [ \"UTC_stop\" ] = mdates . date2num ( line_times [ \"UTC_stop\" ]) # Extract YYYY-MM-DD for title observation_date = lightcurve_df [ \"UTC_bin_start\" ] . iloc [ 0 ] . split ( \"T\" )[ 0 ] # observation_date = lightcurve_df[\"UTC_bin_start\"][0] #.astype(str) # ---- 4\ufe0f Plot the Light Curve ---- plt . figure ( figsize = ( 12 , 6 )) # Plot regularly binned light curve plt . errorbar ( bin_centers , # Use FLOAT values, NOT UTC lightcurve_df [ \"count_rate\" ], xerr = bin_half_widths , yerr = vertical_errors , fmt = \"o\" , color = \"black\" , ecolor = \"silver\" , capsize = 1 , elinewidth = 1 , markersize = 0.3 , label = \"Binned LC\" , ) # Plot Bayesian Block horizontal lines (red) if not bb_df . empty : for _ , row in bb_df . iterrows (): plt . plot ( [ row [ \"UTC_start\" ], row [ \"UTC_stop\" ]], [ row [ \"rate\" ], row [ \"rate\" ]], color = \"red\" , linewidth = 0.7 , label = \"Bayesian Blocks\" if _ == 0 else None , ) plt . plot ( [ row [ \"UTC_start\" ], row [ \"UTC_stop\" ]], [ row [ \"upperlim\" ], row [ \"upperlim\" ]], color = \"red\" , linestyle = ( 0 , ( 5 , 5 )), linewidth = 0.7 , ) # Dashed plt . plot ( [ row [ \"UTC_start\" ], row [ \"UTC_stop\" ]], [ row [ \"lowerlim\" ], row [ \"lowerlim\" ]], color = \"red\" , linestyle = ( 0 , ( 5 , 5 )), linewidth = 0.7 , ) # Dashed # if plot_lines is True: # plt.axvline( # x=row[\"UTC_start\"], color=\"green\", linestyle=\"--\", linewidth=0.7 # ) # plt.axvline( # x=row[\"UTC_stop\"], color=\"red\", linestyle=\"--\", linewidth=0.7 # ) # if not line_times.empty: # for _, row in line_times.iterrows(): # plt.axvline( # x=row[\"UTC_start\"], color=\"green\", linestyle=\"--\", linewidth=0.7 # ) # plt.axvline(x=row[\"UTC_stop\"], color=\"red\", linestyle=\"--\", linewidth=0.7) # Labels, title, and legend plt . title ( f \" { title } ( { observation_date } )\" ) # Include observation date in title plt . xlabel ( xlabel ) # plt.ylim(0, 2.6) # xticks = np.arange(bb_df[\"UTC_start\"].iloc[0], bb_df[\"UTC_stop\"].iloc[-1], 0.015) # xlabels = [f'\\\\${x:1.2f}' for x in xticks] # plt.xticks(xticks, labels=xlabels) # plt.xlim(17267.303013,17267.369417 ) plt . gca () . xaxis . set_major_formatter ( mdates . DateFormatter ( \"%H:%M\" )) plt . gcf () . autofmt_xdate () plt . ylabel ( ylabel ) plt . legend () # Save and close plot plt . savefig ( output_path , dpi = 300 , bbox_inches = \"tight\" ) plt . close () print ( \" Light Curve plot saved.\" ) # ---- 5\ufe0f Save Light Curve and Bayesian Blocks as CSV ---- lightcurve_df . to_csv ( lc_csv_path , index = False ) print ( \" Light Curve CSV saved.\" ) if not bb_df . empty : bb_df . to_csv ( bb_csv_path , index = False ) print ( \" Bayesian Blocks CSV saved.\" )","title":"Plot lc"},{"location":"plot_lc/#scripts.plot_lc.plot_lightcurve","text":"Plot the regularly binned light curve with Bayesian Block overlay, convert times to UTC, and save final data. - Saves the light curve plot as an image file. - Saves the processed Light Curve DataFrame (`lightcurve_df`) as a `.csv` file. - Saves the processed Bayesian Block DataFrame (`bb_df`) as a `.csv` file. Parameters: Name Type Description Default lightcurve_df DataFrame required bb_df DataFrame required output_path str required lc_csv_path str output path for lightcurve csv required bb_csv_path str output path for bayesian block csv required line_times array UTC times for vertical lines to plot required convert_nustar_to_utc function Convert NuSTAR mission time to UTC. required plot_lines Boolean (default False) False title str (default \"Light Curve\") 'Light Curve' xlabel str (default \"Time (UTC)) 'Time (UTC)' ylabel str (default \"Count Rate (cts/s)\") 'Count Rate (cts/s)' Source code in scripts/plot_lc.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 def plot_lightcurve ( lightcurve_df , bb_df , output_path , lc_csv_path , bb_csv_path , line_times , convert_nustar_to_utc , plot_lines = False , title = \"Light Curve\" , xlabel = \"Time (UTC)\" , ylabel = \"Count Rate (cts/s)\" , ): \"\"\" Plot the regularly binned light curve with Bayesian Block overlay, convert times to UTC, and save final data. - Saves the light curve plot as an image file. - Saves the processed Light Curve DataFrame (`lightcurve_df`) as a `.csv` file. - Saves the processed Bayesian Block DataFrame (`bb_df`) as a `.csv` file. Parameters: lightcurve_df (pd.DataFrame): bb_df (pd.DataFrame): output_path (str): lc_csv_path (str): output path for lightcurve csv bb_csv_path (str):output path for bayesian block csv line_times (array): UTC times for vertical lines to plot convert_nustar_to_utc (function): Convert NuSTAR mission time to UTC. plot_lines (Boolean): (default False) title (str): (default \"Light Curve\") xlabel (str): (default \"Time (UTC)) ylabel (str): (default \"Count Rate (cts/s)\") \"\"\" if lightcurve_df . empty : raise ValueError ( \"Error: Light curve DataFrame is empty. Cannot plot.\" ) # ---- 1\ufe0f Compute Bin Centers and Error Bars in Mission Time (FLOATS) ---- bin_centers = ( lightcurve_df [ \"bin_start\" ] + lightcurve_df [ \"bin_end\" ]) / 2 bin_centers = mdates . date2num ( bin_centers . apply ( convert_nustar_to_utc )) bin_half_widths = ( lightcurve_df [ \"bin_end\" ] - lightcurve_df [ \"bin_start\" ]) / 2 bin_half_widths = bin_half_widths / 86400 vertical_errors = [ lightcurve_df [ \"count_rate\" ] - lightcurve_df [ \"lower_limit\" ], # Lower error lightcurve_df [ \"upper_limit\" ] - lightcurve_df [ \"count_rate\" ], # Upper error ] # ---- 2\ufe0f Convert Time to UTC (AFTER Calculations) ---- lightcurve_df [ \"UTC_bin_start\" ] = lightcurve_df [ \"bin_start\" ] . apply ( convert_nustar_to_utc ) lightcurve_df [ \"UTC_bin_end\" ] = lightcurve_df [ \"bin_end\" ] . apply ( convert_nustar_to_utc ) # force conversion to datetime # lightcurve_df['UTC_bin_start'] = mdates.date2num(lightcurve_df['UTC_bin_start']) # lightcurve_df['UTC_bin_end'] = mdates.date2num(lightcurve_df['UTC_bin_end']) # Convert bin centers **AFTER computing as float** # lightcurve_df['UTC_bin_center'] = pd.Series(bin_centers).apply(convert_nustar_to_utc) lightcurve_df [ \"UTC_bin_center\" ] = pd . Series ( bin_centers ) # Ensure only UTC columns are strings lightcurve_df [ \"UTC_bin_start\" ] = lightcurve_df [ \"UTC_bin_start\" ] . astype ( str ) lightcurve_df [ \"UTC_bin_end\" ] = lightcurve_df [ \"UTC_bin_end\" ] . astype ( str ) lightcurve_df [ \"UTC_bin_center\" ] = lightcurve_df [ \"UTC_bin_center\" ] . astype ( str ) # ---- 3\ufe0f Convert Bayesian Blocks Time to UTC ---- if not bb_df . empty : bb_df [ \"UTC_start\" ] = bb_df [ \"start\" ] . apply ( convert_nustar_to_utc ) # .astype(str) bb_df [ \"UTC_stop\" ] = bb_df [ \"stop\" ] . apply ( convert_nustar_to_utc ) # .astype(str) # force datetime conversion bb_df [ \"UTC_start\" ] = mdates . date2num ( bb_df [ \"UTC_start\" ]) bb_df [ \"UTC_stop\" ] = mdates . date2num ( bb_df [ \"UTC_stop\" ]) line_times [ \"UTC_start\" ] = line_times [ \"start\" ] . apply ( convert_nustar_to_utc ) # .astype(str) line_times [ \"UTC_stop\" ] = line_times [ \"stop\" ] . apply ( convert_nustar_to_utc ) # .astype(str) line_times [ \"UTC_start\" ] = mdates . date2num ( line_times [ \"UTC_start\" ]) line_times [ \"UTC_stop\" ] = mdates . date2num ( line_times [ \"UTC_stop\" ]) # Extract YYYY-MM-DD for title observation_date = lightcurve_df [ \"UTC_bin_start\" ] . iloc [ 0 ] . split ( \"T\" )[ 0 ] # observation_date = lightcurve_df[\"UTC_bin_start\"][0] #.astype(str) # ---- 4\ufe0f Plot the Light Curve ---- plt . figure ( figsize = ( 12 , 6 )) # Plot regularly binned light curve plt . errorbar ( bin_centers , # Use FLOAT values, NOT UTC lightcurve_df [ \"count_rate\" ], xerr = bin_half_widths , yerr = vertical_errors , fmt = \"o\" , color = \"black\" , ecolor = \"silver\" , capsize = 1 , elinewidth = 1 , markersize = 0.3 , label = \"Binned LC\" , ) # Plot Bayesian Block horizontal lines (red) if not bb_df . empty : for _ , row in bb_df . iterrows (): plt . plot ( [ row [ \"UTC_start\" ], row [ \"UTC_stop\" ]], [ row [ \"rate\" ], row [ \"rate\" ]], color = \"red\" , linewidth = 0.7 , label = \"Bayesian Blocks\" if _ == 0 else None , ) plt . plot ( [ row [ \"UTC_start\" ], row [ \"UTC_stop\" ]], [ row [ \"upperlim\" ], row [ \"upperlim\" ]], color = \"red\" , linestyle = ( 0 , ( 5 , 5 )), linewidth = 0.7 , ) # Dashed plt . plot ( [ row [ \"UTC_start\" ], row [ \"UTC_stop\" ]], [ row [ \"lowerlim\" ], row [ \"lowerlim\" ]], color = \"red\" , linestyle = ( 0 , ( 5 , 5 )), linewidth = 0.7 , ) # Dashed # if plot_lines is True: # plt.axvline( # x=row[\"UTC_start\"], color=\"green\", linestyle=\"--\", linewidth=0.7 # ) # plt.axvline( # x=row[\"UTC_stop\"], color=\"red\", linestyle=\"--\", linewidth=0.7 # ) # if not line_times.empty: # for _, row in line_times.iterrows(): # plt.axvline( # x=row[\"UTC_start\"], color=\"green\", linestyle=\"--\", linewidth=0.7 # ) # plt.axvline(x=row[\"UTC_stop\"], color=\"red\", linestyle=\"--\", linewidth=0.7) # Labels, title, and legend plt . title ( f \" { title } ( { observation_date } )\" ) # Include observation date in title plt . xlabel ( xlabel ) # plt.ylim(0, 2.6) # xticks = np.arange(bb_df[\"UTC_start\"].iloc[0], bb_df[\"UTC_stop\"].iloc[-1], 0.015) # xlabels = [f'\\\\${x:1.2f}' for x in xticks] # plt.xticks(xticks, labels=xlabels) # plt.xlim(17267.303013,17267.369417 ) plt . gca () . xaxis . set_major_formatter ( mdates . DateFormatter ( \"%H:%M\" )) plt . gcf () . autofmt_xdate () plt . ylabel ( ylabel ) plt . legend () # Save and close plot plt . savefig ( output_path , dpi = 300 , bbox_inches = \"tight\" ) plt . close () print ( \" Light Curve plot saved.\" ) # ---- 5\ufe0f Save Light Curve and Bayesian Blocks as CSV ---- lightcurve_df . to_csv ( lc_csv_path , index = False ) print ( \" Light Curve CSV saved.\" ) if not bb_df . empty : bb_df . to_csv ( bb_csv_path , index = False ) print ( \" Bayesian Blocks CSV saved.\" )","title":"plot_lightcurve"},{"location":"save_bba_results/","text":"calculate_confidence_limits ( bba_df ) Calculate the 1-sigma confidence limits (upper and lower) for each block's event rate. Parameters: Name Type Description Default bba_df DataFrame DataFrame containing BBA results with 'Counts' and 'Length (s)' columns. required Returns: Type Description DataFrame Updated BBA DataFrame with '1sig upper lim' and '1sig lower lim' columns. Source code in scripts/save_bba_results.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 def calculate_confidence_limits ( bba_df ): \"\"\" Calculate the 1-sigma confidence limits (upper and lower) for each block's event rate. Parameters: bba_df (pd.DataFrame): DataFrame containing BBA results with 'Counts' and 'Length (s)' columns. Returns: (pd.DataFrame): Updated BBA DataFrame with '1sig upper lim' and '1sig lower lim' columns. \"\"\" upper_limits = [] lower_limits = [] for _ , row in bba_df . iterrows (): counts = row [ \"Counts\" ] length = row [ \"Length (s)\" ] # Gehrels approximation if counts > 0 : upper_limit = ( counts + np . sqrt ( counts + 0.75 )) / length lower_limit = ( ( counts - np . sqrt ( counts - 0.25 )) / length if counts > 1 else 0.0 ) else : upper_limit = 0.0 lower_limit = 0.0 upper_limits . append ( upper_limit ) lower_limits . append ( lower_limit ) # Add the calculated columns to the DataFrame bba_df [ \"1sig upper lim\" ] = upper_limits bba_df [ \"1sig lower lim\" ] = lower_limits return bba_df calculate_event_counts_and_rates ( bba_df , event_df , exposure_col = 'Exposure' ) Calculate the number of events ( Counts ), the length of each block ( Length (s) ), and the event rate ( Rate (cts/s) ) for each Bayesian Block interval. Parameters: Name Type Description Default bba_df DataFrame DataFrame containing BBA block intervals with 'start' and 'stop' columns. required event_df DataFrame DataFrame containing events with 'TIME' and optional Exposure columns. required exposure_col str Name of the column in event_df containing the exposure correction factor. 'Exposure' Returns: Type Description DataFrame Updated BBA DataFrame with 'Counts', 'Length (s)', and 'Rate (cts/s)' columns. Source code in scripts/save_bba_results.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def calculate_event_counts_and_rates ( bba_df , event_df , exposure_col = \"Exposure\" ): \"\"\" Calculate the number of events (`Counts`), the length of each block (`Length (s)`), and the event rate (`Rate (cts/s)`) for each Bayesian Block interval. Parameters: bba_df (pd.DataFrame): DataFrame containing BBA block intervals with 'start' and 'stop' columns. event_df (pd.DataFrame): DataFrame containing events with 'TIME' and optional `Exposure` columns. exposure_col (str): Name of the column in `event_df` containing the exposure correction factor. Returns: (pd.DataFrame): Updated BBA DataFrame with 'Counts', 'Length (s)', and 'Rate (cts/s)' columns. \"\"\" counts = [] lengths = [] rates = [] # block_frame=bba_df.copy() # initialize tuple to store info block_list = [ { \"start\" : None , \"stop\" : None , \"exposure\" : 0 , \"NuSTAR duration\" : 0 , \"counts\" : 0 , \"total exposure\" : 0 , \"rate\" : 0 , \"upperlim\" : 0 , \"lowerlim\" : 0 , } for i in range ( int ( bba_df [ \"block_label\" ] . iloc [ - 1 ]) + 1 ) ] for _ , block in bba_df . iterrows (): block_start = block [ \"start\" ] # old version block_stop = block [ \"stop\" ] # old version block_label = int ( block [ \"block_label\" ]) # start times if block_list [ block_label ][ \"start\" ] is None : block_list [ block_label ][ \"start\" ] = block_start # stop times block_list [ block_label ][ \"stop\" ] = block_stop # Calculate block length block_length = block_stop - block_start lengths . append ( block_length ) # old version block_list [ block_label ][ \"exposure\" ] += block_length # Filter events in the current block block_events = event_df [ ( event_df [ \"TIME\" ] >= block_start ) & ( event_df [ \"TIME\" ] < block_stop ) ] # Calculate number of events count = len ( block_events ) counts . append ( count ) block_list [ block_label ][ \"counts\" ] += count # get correction fraction (PSF, Vignetting, ect) correction_factor = np . mean ( block_events [ \"CORRECTION_FACTOR\" ]) # Calculate event rate if doing exposure correction if not block_events . empty : total_exposure = block_events [ exposure_col ] . sum () block_list [ block_label ][ \"total exposure\" ] += total_exposure rate = count / block_length rate /= correction_factor else : rate = 0.0 rates . append ( rate ) # get the count rates of the chunks block_list [ block_label ][ \"rate\" ] = ( block_list [ block_label ][ \"counts\" ] / block_list [ block_label ][ \"exposure\" ] ) / correction_factor # get livetime duration block_list [ block_label ][ \"NuSTAR duration\" ] = ( block_list [ block_label ][ \"stop\" ] - block_list [ block_label ][ \"start\" ] ) # also compute upper and lower count rate limits (for the block version I am working with) # counts = block_list[block_label]['counts'] # length = block_list[block_label]['duration'] # Gehrels approximation if block_list [ block_label ][ \"counts\" ] > 0 : upper_limit = ( block_list [ block_label ][ \"counts\" ] + np . sqrt ( block_list [ block_label ][ \"counts\" ] + 0.75 ) ) / block_list [ block_label ][ \"exposure\" ] lower_limit = ( ( block_list [ block_label ][ \"counts\" ] - np . sqrt ( block_list [ block_label ][ \"counts\" ] - 0.25 ) ) / block_list [ block_label ][ \"exposure\" ] if block_list [ block_label ][ \"counts\" ] > 1 else 0.0 ) else : upper_limit = 0.0 lower_limit = 0.0 block_list [ block_label ][ \"upperlim\" ] = upper_limit / correction_factor block_list [ block_label ][ \"lowerlim\" ] = lower_limit / correction_factor block . start_list = [ block_list [ i ][ \"start\" ] for i in range ( int ( bba_df [ \"block_label\" ] . iloc [ - 1 ])) ] # upper_limits.append(upper_limit) # lower_limits.append(lower_limit) # Add new columns to the BBA DataFrame bba_df [ \"Counts\" ] = counts bba_df [ \"Length (s)\" ] = lengths bba_df [ \"Rate (cts/s)\" ] = rates # construct new flare df flare_df = pd . DataFrame ( block_list ) return bba_df , flare_df convert_nustar_to_utc ( nustar_time ) Convert NuSTAR mission time to UTC. Parameters: Name Type Description Default nustar_time float NuSTAR mission time in seconds since the reference epoch. required Returns: Type Description str UTC timestamp in ISO format (e.g., '2018-04-24T03:06:32'). Source code in scripts/save_bba_results.py 7 8 9 10 11 12 13 14 15 16 17 18 19 def convert_nustar_to_utc ( nustar_time ): \"\"\" Convert NuSTAR mission time to UTC. Parameters: nustar_time (float): NuSTAR mission time in seconds since the reference epoch. Returns: (str): UTC timestamp in ISO format (e.g., '2018-04-24T03:06:32'). \"\"\" nustar_epoch = datetime ( 2010 , 1 , 1 ) # NuSTAR reference epoch utc_time = nustar_epoch + timedelta ( seconds = nustar_time ) return utc_time # .isoformat() save_bba_results_txt ( bba_df , analysis_params , output_file , save_results_note = None ) Save the Bayesian Block Analysis results to a text file with clean formatting. Parameters: Name Type Description Default bba_df DataFrame DataFrame containing BBA results with columns like 'start', 'stop', 'Counts', 'length', 'rate', etc. required analysis_params dict Dictionary containing analysis parameters (e.g., ncp_prior, fp_rate). required output_file str Path to save the results text file. required save_results_note list Optional list of strings to populate the \"Notes\" column. None Source code in scripts/save_bba_results.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def save_bba_results_txt ( bba_df , analysis_params , output_file , save_results_note = None ): \"\"\" Save the Bayesian Block Analysis results to a text file with clean formatting. Parameters: bba_df (pd.DataFrame): DataFrame containing BBA results with columns like 'start', 'stop', 'Counts', 'length', 'rate', etc. analysis_params (dict): Dictionary containing analysis parameters (e.g., ncp_prior, fp_rate). output_file (str): Path to save the results text file. save_results_note (list): Optional list of strings to populate the \"Notes\" column. \"\"\" # Add UT start and stop columns to BBA DataFrame bba_df [ \"UT_start\" ] = bba_df [ \"start\" ] . apply ( convert_nustar_to_utc ) . astype ( str ) bba_df [ \"UT_stop\" ] = bba_df [ \"stop\" ] . apply ( convert_nustar_to_utc ) . astype ( str ) # Add Notes column or populate it with save_results_note if save_results_note is not None : if len ( save_results_note ) != len ( bba_df ): raise ValueError ( \"Length of save_results_note must match the number of rows in bba_df.\" ) bba_df [ \"Notes\" ] = save_results_note elif \"Notes\" not in bba_df . columns : bba_df [ \"Notes\" ] = \"\" # Default to empty string if Notes not present # Create rows for the table table_data = [] for _ , row in bba_df . iterrows (): table_data . append ( [ row [ \"UT_start\" ] . split ( \".\" )[ 0 ], # Format UT start (precision: seconds) row [ \"UT_stop\" ] . split ( \".\" )[ 0 ], # Format UT stop (precision: seconds) f \" { row [ 'start' ] : .1f } \" , f \" { row [ 'stop' ] : .1f } \" , # f\"{int(row['start'])}\", # f\"{int(row['stop'])}\", f \" { int ( row [ 'Counts' ]) } \" , f \" { row [ 'Length (s)' ] : .1f } \" , f \" { row [ 'Rate (cts/s)' ] : .3E } \" , f \" { row [ '1sig upper lim' ] : .3E } \" , f \" { row [ '1sig lower lim' ] : .3E } \" , row [ \"Notes\" ], ] ) # Table headers headers = [ \"UT start\" , \"UT stop\" , \"NuSTAR start\" , \"NuSTAR stop\" , \"Counts\" , \"Length (s)\" , \"Rate (cts/s)\" , \"1sig upper lim\" , \"1sig lower lim\" , \"Notes\" , ] # Center align columns align = \"center\" # Generate formatted table formatted_table = tabulate ( table_data , headers = headers , tablefmt = \"plain\" , numalign = align , stralign = align , floatfmt = ( \".1f\" ), ) with open ( output_file , \"w\" ) as f : # Write analysis parameters f . write ( f \"Number of blocks: { len ( bba_df ) } \\n \" ) f . write ( f \"ncp_prior: { analysis_params [ 'ncp_prior' ] } \\n \" ) f . write ( f \"fp_rate: { analysis_params [ 'fp_rate' ] : .1E } \\n \" ) f . write ( f \"do_iter: { analysis_params [ 'do_iter' ] } \\n\\n \" ) # Write formatted table f . write ( formatted_table ) print ( \" Can customize notes in the bba_results.txt file.\" ) save_flare_results_txt ( bba_df , analysis_params , output_file , save_results_note = None ) Save the flare Bayesian Block Analysis results to a text file with clean formatting. Parameters: Name Type Description Default bba_df DataFrame DataFrame containing BBA results with columns like 'start', 'stop', 'Counts', 'length', 'rate', etc. required analysis_params dict Dictionary containing analysis parameters (e.g., ncp_prior, fp_rate). required output_file str Path to save the results text file. required save_results_note list Optional list of strings to populate the \"Notes\" column. None Source code in scripts/save_bba_results.py 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 def save_flare_results_txt ( bba_df , analysis_params , output_file , save_results_note = None ): \"\"\" Save the flare Bayesian Block Analysis results to a text file with clean formatting. Parameters: bba_df (pd.DataFrame): DataFrame containing BBA results with columns like 'start', 'stop', 'Counts', 'length', 'rate', etc. analysis_params (dict): Dictionary containing analysis parameters (e.g., ncp_prior, fp_rate). output_file (str): Path to save the results text file. save_results_note (list): Optional list of strings to populate the \"Notes\" column. \"\"\" # drop the total exposure column bba_df . drop ([ \"total exposure\" ], axis = 1 ) # Add UT start and stop columns to BBA DataFrame bba_df [ \"UT_start\" ] = bba_df [ \"start\" ] . apply ( convert_nustar_to_utc ) . astype ( str ) bba_df [ \"UT_stop\" ] = bba_df [ \"stop\" ] . apply ( convert_nustar_to_utc ) . astype ( str ) # Add Notes column or populate it with save_results_note if save_results_note is not None : if len ( save_results_note ) != len ( bba_df ): raise ValueError ( \"Length of save_results_note must match the number of rows in bba_df.\" ) bba_df [ \"Notes\" ] = save_results_note elif \"Notes\" not in bba_df . columns : bba_df [ \"Notes\" ] = \"\" # Default to empty string if Notes not present # Create rows for the table table_data = [] for _ , row in bba_df . iterrows (): table_data . append ( [ row [ \"UT_start\" ] . split ( \".\" )[ 0 ], # Format UT start (precision: seconds) row [ \"UT_stop\" ] . split ( \".\" )[ 0 ], # Format UT stop (precision: seconds) f \" { row [ 'start' ] : .1f } \" , f \" { row [ 'stop' ] : .1f } \" , # f\"{int(row['start'])}\", # f\"{int(row['stop'])}\", f \" { row [ 'exposure' ] : .1f } \" , f \" { row [ 'NuSTAR duration' ] : .1f } \" , f \" { int ( row [ 'counts' ]) } \" , f \" { row [ 'rate' ] : .3E } \" , f \" { row [ 'upperlim' ] : .3E } \" , f \" { row [ 'lowerlim' ] : .3E } \" , row [ \"Notes\" ], ] ) # Table headers headers = [ \"UT start\" , \"UT stop\" , \"NuSTAR start\" , \"NuSTAR stop\" , \"Length (s)\" , \"Livetime (s)\" , \"Counts\" , \"Rate (cts/s)\" , \"1sig upper lim\" , \"1sig lower lim\" , \"Notes\" , ] # Center align columns align = \"center\" # Generate formatted table formatted_table = tabulate ( table_data , headers = headers , tablefmt = \"plain\" , numalign = align , stralign = align , floatfmt = ( \".1f\" , \".1f\" , \".1f\" , \".1f\" , \".1f\" , \".1f\" , \".1f\" , \".4f\" , \".4f\" , \".4f\" , \".1f\" , ), ) with open ( output_file , \"w\" ) as f : # Write analysis parameters f . write ( f \"Number of blocks: { len ( bba_df ) } \\n \" ) f . write ( f \"ncp_prior: { analysis_params [ 'ncp_prior' ] } \\n \" ) f . write ( f \"fp_rate: { analysis_params [ 'fp_rate' ] : .1E } \\n \" ) f . write ( f \"do_iter: { analysis_params [ 'do_iter' ] } \\n\\n \" ) # Write formatted table f . write ( formatted_table )","title":"Save bba results"},{"location":"save_bba_results/#scripts.save_bba_results.calculate_confidence_limits","text":"Calculate the 1-sigma confidence limits (upper and lower) for each block's event rate. Parameters: Name Type Description Default bba_df DataFrame DataFrame containing BBA results with 'Counts' and 'Length (s)' columns. required Returns: Type Description DataFrame Updated BBA DataFrame with '1sig upper lim' and '1sig lower lim' columns. Source code in scripts/save_bba_results.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 def calculate_confidence_limits ( bba_df ): \"\"\" Calculate the 1-sigma confidence limits (upper and lower) for each block's event rate. Parameters: bba_df (pd.DataFrame): DataFrame containing BBA results with 'Counts' and 'Length (s)' columns. Returns: (pd.DataFrame): Updated BBA DataFrame with '1sig upper lim' and '1sig lower lim' columns. \"\"\" upper_limits = [] lower_limits = [] for _ , row in bba_df . iterrows (): counts = row [ \"Counts\" ] length = row [ \"Length (s)\" ] # Gehrels approximation if counts > 0 : upper_limit = ( counts + np . sqrt ( counts + 0.75 )) / length lower_limit = ( ( counts - np . sqrt ( counts - 0.25 )) / length if counts > 1 else 0.0 ) else : upper_limit = 0.0 lower_limit = 0.0 upper_limits . append ( upper_limit ) lower_limits . append ( lower_limit ) # Add the calculated columns to the DataFrame bba_df [ \"1sig upper lim\" ] = upper_limits bba_df [ \"1sig lower lim\" ] = lower_limits return bba_df","title":"calculate_confidence_limits"},{"location":"save_bba_results/#scripts.save_bba_results.calculate_event_counts_and_rates","text":"Calculate the number of events ( Counts ), the length of each block ( Length (s) ), and the event rate ( Rate (cts/s) ) for each Bayesian Block interval. Parameters: Name Type Description Default bba_df DataFrame DataFrame containing BBA block intervals with 'start' and 'stop' columns. required event_df DataFrame DataFrame containing events with 'TIME' and optional Exposure columns. required exposure_col str Name of the column in event_df containing the exposure correction factor. 'Exposure' Returns: Type Description DataFrame Updated BBA DataFrame with 'Counts', 'Length (s)', and 'Rate (cts/s)' columns. Source code in scripts/save_bba_results.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def calculate_event_counts_and_rates ( bba_df , event_df , exposure_col = \"Exposure\" ): \"\"\" Calculate the number of events (`Counts`), the length of each block (`Length (s)`), and the event rate (`Rate (cts/s)`) for each Bayesian Block interval. Parameters: bba_df (pd.DataFrame): DataFrame containing BBA block intervals with 'start' and 'stop' columns. event_df (pd.DataFrame): DataFrame containing events with 'TIME' and optional `Exposure` columns. exposure_col (str): Name of the column in `event_df` containing the exposure correction factor. Returns: (pd.DataFrame): Updated BBA DataFrame with 'Counts', 'Length (s)', and 'Rate (cts/s)' columns. \"\"\" counts = [] lengths = [] rates = [] # block_frame=bba_df.copy() # initialize tuple to store info block_list = [ { \"start\" : None , \"stop\" : None , \"exposure\" : 0 , \"NuSTAR duration\" : 0 , \"counts\" : 0 , \"total exposure\" : 0 , \"rate\" : 0 , \"upperlim\" : 0 , \"lowerlim\" : 0 , } for i in range ( int ( bba_df [ \"block_label\" ] . iloc [ - 1 ]) + 1 ) ] for _ , block in bba_df . iterrows (): block_start = block [ \"start\" ] # old version block_stop = block [ \"stop\" ] # old version block_label = int ( block [ \"block_label\" ]) # start times if block_list [ block_label ][ \"start\" ] is None : block_list [ block_label ][ \"start\" ] = block_start # stop times block_list [ block_label ][ \"stop\" ] = block_stop # Calculate block length block_length = block_stop - block_start lengths . append ( block_length ) # old version block_list [ block_label ][ \"exposure\" ] += block_length # Filter events in the current block block_events = event_df [ ( event_df [ \"TIME\" ] >= block_start ) & ( event_df [ \"TIME\" ] < block_stop ) ] # Calculate number of events count = len ( block_events ) counts . append ( count ) block_list [ block_label ][ \"counts\" ] += count # get correction fraction (PSF, Vignetting, ect) correction_factor = np . mean ( block_events [ \"CORRECTION_FACTOR\" ]) # Calculate event rate if doing exposure correction if not block_events . empty : total_exposure = block_events [ exposure_col ] . sum () block_list [ block_label ][ \"total exposure\" ] += total_exposure rate = count / block_length rate /= correction_factor else : rate = 0.0 rates . append ( rate ) # get the count rates of the chunks block_list [ block_label ][ \"rate\" ] = ( block_list [ block_label ][ \"counts\" ] / block_list [ block_label ][ \"exposure\" ] ) / correction_factor # get livetime duration block_list [ block_label ][ \"NuSTAR duration\" ] = ( block_list [ block_label ][ \"stop\" ] - block_list [ block_label ][ \"start\" ] ) # also compute upper and lower count rate limits (for the block version I am working with) # counts = block_list[block_label]['counts'] # length = block_list[block_label]['duration'] # Gehrels approximation if block_list [ block_label ][ \"counts\" ] > 0 : upper_limit = ( block_list [ block_label ][ \"counts\" ] + np . sqrt ( block_list [ block_label ][ \"counts\" ] + 0.75 ) ) / block_list [ block_label ][ \"exposure\" ] lower_limit = ( ( block_list [ block_label ][ \"counts\" ] - np . sqrt ( block_list [ block_label ][ \"counts\" ] - 0.25 ) ) / block_list [ block_label ][ \"exposure\" ] if block_list [ block_label ][ \"counts\" ] > 1 else 0.0 ) else : upper_limit = 0.0 lower_limit = 0.0 block_list [ block_label ][ \"upperlim\" ] = upper_limit / correction_factor block_list [ block_label ][ \"lowerlim\" ] = lower_limit / correction_factor block . start_list = [ block_list [ i ][ \"start\" ] for i in range ( int ( bba_df [ \"block_label\" ] . iloc [ - 1 ])) ] # upper_limits.append(upper_limit) # lower_limits.append(lower_limit) # Add new columns to the BBA DataFrame bba_df [ \"Counts\" ] = counts bba_df [ \"Length (s)\" ] = lengths bba_df [ \"Rate (cts/s)\" ] = rates # construct new flare df flare_df = pd . DataFrame ( block_list ) return bba_df , flare_df","title":"calculate_event_counts_and_rates"},{"location":"save_bba_results/#scripts.save_bba_results.convert_nustar_to_utc","text":"Convert NuSTAR mission time to UTC. Parameters: Name Type Description Default nustar_time float NuSTAR mission time in seconds since the reference epoch. required Returns: Type Description str UTC timestamp in ISO format (e.g., '2018-04-24T03:06:32'). Source code in scripts/save_bba_results.py 7 8 9 10 11 12 13 14 15 16 17 18 19 def convert_nustar_to_utc ( nustar_time ): \"\"\" Convert NuSTAR mission time to UTC. Parameters: nustar_time (float): NuSTAR mission time in seconds since the reference epoch. Returns: (str): UTC timestamp in ISO format (e.g., '2018-04-24T03:06:32'). \"\"\" nustar_epoch = datetime ( 2010 , 1 , 1 ) # NuSTAR reference epoch utc_time = nustar_epoch + timedelta ( seconds = nustar_time ) return utc_time # .isoformat()","title":"convert_nustar_to_utc"},{"location":"save_bba_results/#scripts.save_bba_results.save_bba_results_txt","text":"Save the Bayesian Block Analysis results to a text file with clean formatting. Parameters: Name Type Description Default bba_df DataFrame DataFrame containing BBA results with columns like 'start', 'stop', 'Counts', 'length', 'rate', etc. required analysis_params dict Dictionary containing analysis parameters (e.g., ncp_prior, fp_rate). required output_file str Path to save the results text file. required save_results_note list Optional list of strings to populate the \"Notes\" column. None Source code in scripts/save_bba_results.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def save_bba_results_txt ( bba_df , analysis_params , output_file , save_results_note = None ): \"\"\" Save the Bayesian Block Analysis results to a text file with clean formatting. Parameters: bba_df (pd.DataFrame): DataFrame containing BBA results with columns like 'start', 'stop', 'Counts', 'length', 'rate', etc. analysis_params (dict): Dictionary containing analysis parameters (e.g., ncp_prior, fp_rate). output_file (str): Path to save the results text file. save_results_note (list): Optional list of strings to populate the \"Notes\" column. \"\"\" # Add UT start and stop columns to BBA DataFrame bba_df [ \"UT_start\" ] = bba_df [ \"start\" ] . apply ( convert_nustar_to_utc ) . astype ( str ) bba_df [ \"UT_stop\" ] = bba_df [ \"stop\" ] . apply ( convert_nustar_to_utc ) . astype ( str ) # Add Notes column or populate it with save_results_note if save_results_note is not None : if len ( save_results_note ) != len ( bba_df ): raise ValueError ( \"Length of save_results_note must match the number of rows in bba_df.\" ) bba_df [ \"Notes\" ] = save_results_note elif \"Notes\" not in bba_df . columns : bba_df [ \"Notes\" ] = \"\" # Default to empty string if Notes not present # Create rows for the table table_data = [] for _ , row in bba_df . iterrows (): table_data . append ( [ row [ \"UT_start\" ] . split ( \".\" )[ 0 ], # Format UT start (precision: seconds) row [ \"UT_stop\" ] . split ( \".\" )[ 0 ], # Format UT stop (precision: seconds) f \" { row [ 'start' ] : .1f } \" , f \" { row [ 'stop' ] : .1f } \" , # f\"{int(row['start'])}\", # f\"{int(row['stop'])}\", f \" { int ( row [ 'Counts' ]) } \" , f \" { row [ 'Length (s)' ] : .1f } \" , f \" { row [ 'Rate (cts/s)' ] : .3E } \" , f \" { row [ '1sig upper lim' ] : .3E } \" , f \" { row [ '1sig lower lim' ] : .3E } \" , row [ \"Notes\" ], ] ) # Table headers headers = [ \"UT start\" , \"UT stop\" , \"NuSTAR start\" , \"NuSTAR stop\" , \"Counts\" , \"Length (s)\" , \"Rate (cts/s)\" , \"1sig upper lim\" , \"1sig lower lim\" , \"Notes\" , ] # Center align columns align = \"center\" # Generate formatted table formatted_table = tabulate ( table_data , headers = headers , tablefmt = \"plain\" , numalign = align , stralign = align , floatfmt = ( \".1f\" ), ) with open ( output_file , \"w\" ) as f : # Write analysis parameters f . write ( f \"Number of blocks: { len ( bba_df ) } \\n \" ) f . write ( f \"ncp_prior: { analysis_params [ 'ncp_prior' ] } \\n \" ) f . write ( f \"fp_rate: { analysis_params [ 'fp_rate' ] : .1E } \\n \" ) f . write ( f \"do_iter: { analysis_params [ 'do_iter' ] } \\n\\n \" ) # Write formatted table f . write ( formatted_table ) print ( \" Can customize notes in the bba_results.txt file.\" )","title":"save_bba_results_txt"},{"location":"save_bba_results/#scripts.save_bba_results.save_flare_results_txt","text":"Save the flare Bayesian Block Analysis results to a text file with clean formatting. Parameters: Name Type Description Default bba_df DataFrame DataFrame containing BBA results with columns like 'start', 'stop', 'Counts', 'length', 'rate', etc. required analysis_params dict Dictionary containing analysis parameters (e.g., ncp_prior, fp_rate). required output_file str Path to save the results text file. required save_results_note list Optional list of strings to populate the \"Notes\" column. None Source code in scripts/save_bba_results.py 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 def save_flare_results_txt ( bba_df , analysis_params , output_file , save_results_note = None ): \"\"\" Save the flare Bayesian Block Analysis results to a text file with clean formatting. Parameters: bba_df (pd.DataFrame): DataFrame containing BBA results with columns like 'start', 'stop', 'Counts', 'length', 'rate', etc. analysis_params (dict): Dictionary containing analysis parameters (e.g., ncp_prior, fp_rate). output_file (str): Path to save the results text file. save_results_note (list): Optional list of strings to populate the \"Notes\" column. \"\"\" # drop the total exposure column bba_df . drop ([ \"total exposure\" ], axis = 1 ) # Add UT start and stop columns to BBA DataFrame bba_df [ \"UT_start\" ] = bba_df [ \"start\" ] . apply ( convert_nustar_to_utc ) . astype ( str ) bba_df [ \"UT_stop\" ] = bba_df [ \"stop\" ] . apply ( convert_nustar_to_utc ) . astype ( str ) # Add Notes column or populate it with save_results_note if save_results_note is not None : if len ( save_results_note ) != len ( bba_df ): raise ValueError ( \"Length of save_results_note must match the number of rows in bba_df.\" ) bba_df [ \"Notes\" ] = save_results_note elif \"Notes\" not in bba_df . columns : bba_df [ \"Notes\" ] = \"\" # Default to empty string if Notes not present # Create rows for the table table_data = [] for _ , row in bba_df . iterrows (): table_data . append ( [ row [ \"UT_start\" ] . split ( \".\" )[ 0 ], # Format UT start (precision: seconds) row [ \"UT_stop\" ] . split ( \".\" )[ 0 ], # Format UT stop (precision: seconds) f \" { row [ 'start' ] : .1f } \" , f \" { row [ 'stop' ] : .1f } \" , # f\"{int(row['start'])}\", # f\"{int(row['stop'])}\", f \" { row [ 'exposure' ] : .1f } \" , f \" { row [ 'NuSTAR duration' ] : .1f } \" , f \" { int ( row [ 'counts' ]) } \" , f \" { row [ 'rate' ] : .3E } \" , f \" { row [ 'upperlim' ] : .3E } \" , f \" { row [ 'lowerlim' ] : .3E } \" , row [ \"Notes\" ], ] ) # Table headers headers = [ \"UT start\" , \"UT stop\" , \"NuSTAR start\" , \"NuSTAR stop\" , \"Length (s)\" , \"Livetime (s)\" , \"Counts\" , \"Rate (cts/s)\" , \"1sig upper lim\" , \"1sig lower lim\" , \"Notes\" , ] # Center align columns align = \"center\" # Generate formatted table formatted_table = tabulate ( table_data , headers = headers , tablefmt = \"plain\" , numalign = align , stralign = align , floatfmt = ( \".1f\" , \".1f\" , \".1f\" , \".1f\" , \".1f\" , \".1f\" , \".1f\" , \".4f\" , \".4f\" , \".4f\" , \".1f\" , ), ) with open ( output_file , \"w\" ) as f : # Write analysis parameters f . write ( f \"Number of blocks: { len ( bba_df ) } \\n \" ) f . write ( f \"ncp_prior: { analysis_params [ 'ncp_prior' ] } \\n \" ) f . write ( f \"fp_rate: { analysis_params [ 'fp_rate' ] : .1E } \\n \" ) f . write ( f \"do_iter: { analysis_params [ 'do_iter' ] } \\n\\n \" ) # Write formatted table f . write ( formatted_table )","title":"save_flare_results_txt"},{"location":"suppress_gti_gaps/","text":"suppress_gti_gaps ( event_df , gti_df , original_tt_stop ) Remove gaps between GTIs, adjusting photon event times to ensure a continuous timeline. Parameters: Name Type Description Default event_df DataFrame DataFrame containing photon events with a 'TIME' column. required gti_df DataFrame DataFrame with GTI intervals (START, STOP). required original_tt_stop float Original end time of the observation. required Returns: Type Description tuple Updated event_df, updated tt_stop, cumulative_gap_times Source code in scripts/suppress_gti_gaps.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def suppress_gti_gaps ( event_df , gti_df , original_tt_stop ): \"\"\" Remove gaps between GTIs, adjusting photon event times to ensure a continuous timeline. Parameters: event_df (pd.DataFrame): DataFrame containing photon events with a 'TIME' column. gti_df (pd.DataFrame): DataFrame with GTI intervals (START, STOP). original_tt_stop (float): Original end time of the observation. Returns: (tuple): Updated event_df, updated tt_stop, cumulative_gap_times \"\"\" try : # Validate inputs if event_df . empty or gti_df . empty : raise ValueError ( \"Event or GTI DataFrame is empty.\" ) if not all ( col in gti_df . columns for col in [ \"START\" , \"STOP\" ]): raise ValueError ( \"GTI DataFrame must contain 'START' and 'STOP' columns.\" ) if \"TIME\" not in event_df . columns : raise ValueError ( \"Event DataFrame must contain a 'TIME' column.\" ) # Calculate gaps between GTIs gap_durations = gti_df [ \"START\" ][ 1 :] . values - gti_df [ \"STOP\" ][: - 1 ] . values cumulative_gap_times = np . cumsum ( gap_durations ) cumulative_gap_times = np . insert ( cumulative_gap_times , 0 , 0 ) # Include a zero for the first GTI # Adjust event times adjusted_times = event_df [ \"TIME\" ] . copy () for i in range ( 1 , len ( gti_df )): # Identify events in each GTI and adjust their times mask = ( event_df [ \"TIME\" ] >= gti_df [ \"START\" ] . iloc [ i ]) & ( event_df [ \"TIME\" ] < gti_df [ \"STOP\" ] . iloc [ i ] ) adjusted_times [ mask ] -= cumulative_gap_times [ i ] # Update event DataFrame updated_event_df = event_df . copy () updated_event_df [ \"TIME\" ] = adjusted_times # Adjust tt_stop last_event_time = updated_event_df [ \"TIME\" ] . iloc [ - 1 ] updated_tt_stop = last_event_time # Log cumulative gaps for debugging print ( f \" Total gap time removed: { cumulative_gap_times [ - 1 ] } \" ) print ( f \" New observation stop time: { updated_tt_stop } \" ) return updated_event_df , updated_tt_stop , cumulative_gap_times except Exception as e : raise RuntimeError ( f \"Error in suppressing GTI gaps: { e } \" )","title":"Suppress gti gaps"},{"location":"suppress_gti_gaps/#scripts.suppress_gti_gaps.suppress_gti_gaps","text":"Remove gaps between GTIs, adjusting photon event times to ensure a continuous timeline. Parameters: Name Type Description Default event_df DataFrame DataFrame containing photon events with a 'TIME' column. required gti_df DataFrame DataFrame with GTI intervals (START, STOP). required original_tt_stop float Original end time of the observation. required Returns: Type Description tuple Updated event_df, updated tt_stop, cumulative_gap_times Source code in scripts/suppress_gti_gaps.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def suppress_gti_gaps ( event_df , gti_df , original_tt_stop ): \"\"\" Remove gaps between GTIs, adjusting photon event times to ensure a continuous timeline. Parameters: event_df (pd.DataFrame): DataFrame containing photon events with a 'TIME' column. gti_df (pd.DataFrame): DataFrame with GTI intervals (START, STOP). original_tt_stop (float): Original end time of the observation. Returns: (tuple): Updated event_df, updated tt_stop, cumulative_gap_times \"\"\" try : # Validate inputs if event_df . empty or gti_df . empty : raise ValueError ( \"Event or GTI DataFrame is empty.\" ) if not all ( col in gti_df . columns for col in [ \"START\" , \"STOP\" ]): raise ValueError ( \"GTI DataFrame must contain 'START' and 'STOP' columns.\" ) if \"TIME\" not in event_df . columns : raise ValueError ( \"Event DataFrame must contain a 'TIME' column.\" ) # Calculate gaps between GTIs gap_durations = gti_df [ \"START\" ][ 1 :] . values - gti_df [ \"STOP\" ][: - 1 ] . values cumulative_gap_times = np . cumsum ( gap_durations ) cumulative_gap_times = np . insert ( cumulative_gap_times , 0 , 0 ) # Include a zero for the first GTI # Adjust event times adjusted_times = event_df [ \"TIME\" ] . copy () for i in range ( 1 , len ( gti_df )): # Identify events in each GTI and adjust their times mask = ( event_df [ \"TIME\" ] >= gti_df [ \"START\" ] . iloc [ i ]) & ( event_df [ \"TIME\" ] < gti_df [ \"STOP\" ] . iloc [ i ] ) adjusted_times [ mask ] -= cumulative_gap_times [ i ] # Update event DataFrame updated_event_df = event_df . copy () updated_event_df [ \"TIME\" ] = adjusted_times # Adjust tt_stop last_event_time = updated_event_df [ \"TIME\" ] . iloc [ - 1 ] updated_tt_stop = last_event_time # Log cumulative gaps for debugging print ( f \" Total gap time removed: { cumulative_gap_times [ - 1 ] } \" ) print ( f \" New observation stop time: { updated_tt_stop } \" ) return updated_event_df , updated_tt_stop , cumulative_gap_times except Exception as e : raise RuntimeError ( f \"Error in suppressing GTI gaps: { e } \" )","title":"suppress_gti_gaps"},{"location":"tutorial/","text":"Using the Bayesian Block Routine Note This package is designed to work exclusively with NuSTAR observations. Here we are going to run though an example, showing how to run observation 30801024002 through the Bayesian Block routine, and how to get the files you need. Getting the data you need To run this package, you will need four types of data files: A barycenter corrected event file Event files of the observation from FPMA, FPMB, or both (not barycenter corrected) Light curve correction files for the event files used above. This tutorial assumes that you have already obtained cleaned data products through NuPipeline and barycentered events and light curve correction files through NuProducts. These files are a standard part of the NuSTAR data pipeline procedure, and more information can be found in the NuSTAR Manual . Event Files Once you have cleaned data, we can extract event files. For accurate analysis, the event files must exclusively contain data from the target of analysis. To achieve this, we can use another HEASOFT tool called XSELECT. The way we use XSELECT here is to essentially take a spatial cut out of data in the region we are interested in. For example, we can take a 30\" source region centered on Sgr A * by opening the XSELECT dialogue in the command line: xselect XSELECT will then launch and you will be prompted to enter a session name. Lets call our session tutorial: ** XSELECT V2.5b ** >Enter session name >[xsel30916] tutorial We will then enter the event data file that we want to extract information from: read event nu30801024002A01_cl.evt you will then be prompted to enter the event file directory. After you input the directory, it will ask to reset the mission to NUSTAR. Select yes. It will then output something like: Notes: XSELECT set up for NUSTAR Time keyword is TIME in units of s Default timing binsize = 5.0000 Setting... Image keywords = X Y with binning = 1 WMAP keywords = X Y with binning = 1 Energy keyword = PI with binning = 1 Getting Min and Max for Energy Column... Got min and max for PI: 0 4095 could not get minimum time resolution of the data read MJDREF = 5.5197000766019E+04 with TIMESYS = TDB Number of files read in: 1 ******************** Observation Catalogue ******************** Data Directory is: /data/directory/ HK Directory is: /data/directory/ OBJECT OBS_ID DATE-OBS 1 SgrAstar 30801024002 2023-04-13T04:49:49 We can then input our region size by giving the path and filename of the .reg region file that we want to select: tutorial:NUSTAR-FPMA > filter region SgrA_30ac.reg Then we extract the event: tutorial:NUSTAR-FPMA > extract event Then we can save the event: save event And give the output file name when prompted. When asked to use fultered events as input data file, select no. You must then quit the program before repeating the process for other data. You can quit the program by: quit and saving the session if you wish (not required). Running the data After you have obtained the required files, you can run it through the bayesian block routine. Do do this, you must edit some lines in the main.py script. For example, lets run through an example using observation ID 30801024002. This observation was taken in 2023, so I would set: year = \"2023\" obsID = \"30801024002\" path = (\"path/to/prepared/files\") # Define output directory output_dir = path + \"/desired_output/\" [.....] # Define input file paths barycorr_event = path + \"/nu\" + obsID + \"A01_cl_barycorr.evt\" event_file_a = path + \"/nu\" + obsID + \"A01_xselected.evt\" event_file_b = path + \"/nu\" + obsID + \"B01_xselected.evt\" lccorrfileA = path + \"/SgrA_correct_50ac_fpmA_lcsrccorrfile.fits\" lccorrfileB = path + \"/SgrA_correct_50ac_fpmB_lcsrccorrfile.fits\" # --- Define Parameters / General Used Ones --- energy_min = 3.0 energy_max = 30.0 With the correct names for the event files, light curve correction files, and desired energy range for analysis. Using data from only one module If you are only using data from one observation module, you can comment out the filenames of the ones that you are not using. For example, if you were only using data from module A, your script might look like: year = \"2023\" obsID = \"30801024002\" path = (\"path/to/prepared/files\") # Define output directory output_dir = path + \"/desired_output/\" [.....] # Define input file paths barycorr_event = path + \"/nu\" + obsID + \"A01_cl_barycorr.evt\" event_file_a = path + \"/nu\" + obsID + \"A01_xselected.evt\" #event_file_b = path + \"/nu\" + obsID + \"B01_xselected.evt\" lccorrfileA = path + \"/SgrA_correct_50ac_fpmA_lcsrccorrfile.fits\" #lccorrfileB = path + \"/SgrA_correct_50ac_fpmB_lcsrccorrfile.fits\" # --- Define Parameters / General Used Ones --- energy_min = 3.0 energy_max = 30.0 Output csv files, plots, and txt files will then be put in the output directory specified above.","title":"Tutorials"},{"location":"tutorial/#using-the-bayesian-block-routine","text":"Note This package is designed to work exclusively with NuSTAR observations. Here we are going to run though an example, showing how to run observation 30801024002 through the Bayesian Block routine, and how to get the files you need.","title":"Using the Bayesian Block Routine"},{"location":"tutorial/#getting-the-data-you-need","text":"To run this package, you will need four types of data files: A barycenter corrected event file Event files of the observation from FPMA, FPMB, or both (not barycenter corrected) Light curve correction files for the event files used above. This tutorial assumes that you have already obtained cleaned data products through NuPipeline and barycentered events and light curve correction files through NuProducts. These files are a standard part of the NuSTAR data pipeline procedure, and more information can be found in the NuSTAR Manual .","title":"Getting the data you need"},{"location":"tutorial/#event-files","text":"Once you have cleaned data, we can extract event files. For accurate analysis, the event files must exclusively contain data from the target of analysis. To achieve this, we can use another HEASOFT tool called XSELECT. The way we use XSELECT here is to essentially take a spatial cut out of data in the region we are interested in. For example, we can take a 30\" source region centered on Sgr A * by opening the XSELECT dialogue in the command line: xselect XSELECT will then launch and you will be prompted to enter a session name. Lets call our session tutorial: ** XSELECT V2.5b ** >Enter session name >[xsel30916] tutorial We will then enter the event data file that we want to extract information from: read event nu30801024002A01_cl.evt you will then be prompted to enter the event file directory. After you input the directory, it will ask to reset the mission to NUSTAR. Select yes. It will then output something like: Notes: XSELECT set up for NUSTAR Time keyword is TIME in units of s Default timing binsize = 5.0000 Setting... Image keywords = X Y with binning = 1 WMAP keywords = X Y with binning = 1 Energy keyword = PI with binning = 1 Getting Min and Max for Energy Column... Got min and max for PI: 0 4095 could not get minimum time resolution of the data read MJDREF = 5.5197000766019E+04 with TIMESYS = TDB Number of files read in: 1 ******************** Observation Catalogue ******************** Data Directory is: /data/directory/ HK Directory is: /data/directory/ OBJECT OBS_ID DATE-OBS 1 SgrAstar 30801024002 2023-04-13T04:49:49 We can then input our region size by giving the path and filename of the .reg region file that we want to select: tutorial:NUSTAR-FPMA > filter region SgrA_30ac.reg Then we extract the event: tutorial:NUSTAR-FPMA > extract event Then we can save the event: save event And give the output file name when prompted. When asked to use fultered events as input data file, select no. You must then quit the program before repeating the process for other data. You can quit the program by: quit and saving the session if you wish (not required).","title":"Event Files"},{"location":"tutorial/#running-the-data","text":"After you have obtained the required files, you can run it through the bayesian block routine. Do do this, you must edit some lines in the main.py script. For example, lets run through an example using observation ID 30801024002. This observation was taken in 2023, so I would set: year = \"2023\" obsID = \"30801024002\" path = (\"path/to/prepared/files\") # Define output directory output_dir = path + \"/desired_output/\" [.....] # Define input file paths barycorr_event = path + \"/nu\" + obsID + \"A01_cl_barycorr.evt\" event_file_a = path + \"/nu\" + obsID + \"A01_xselected.evt\" event_file_b = path + \"/nu\" + obsID + \"B01_xselected.evt\" lccorrfileA = path + \"/SgrA_correct_50ac_fpmA_lcsrccorrfile.fits\" lccorrfileB = path + \"/SgrA_correct_50ac_fpmB_lcsrccorrfile.fits\" # --- Define Parameters / General Used Ones --- energy_min = 3.0 energy_max = 30.0 With the correct names for the event files, light curve correction files, and desired energy range for analysis. Using data from only one module If you are only using data from one observation module, you can comment out the filenames of the ones that you are not using. For example, if you were only using data from module A, your script might look like: year = \"2023\" obsID = \"30801024002\" path = (\"path/to/prepared/files\") # Define output directory output_dir = path + \"/desired_output/\" [.....] # Define input file paths barycorr_event = path + \"/nu\" + obsID + \"A01_cl_barycorr.evt\" event_file_a = path + \"/nu\" + obsID + \"A01_xselected.evt\" #event_file_b = path + \"/nu\" + obsID + \"B01_xselected.evt\" lccorrfileA = path + \"/SgrA_correct_50ac_fpmA_lcsrccorrfile.fits\" #lccorrfileB = path + \"/SgrA_correct_50ac_fpmB_lcsrccorrfile.fits\" # --- Define Parameters / General Used Ones --- energy_min = 3.0 energy_max = 30.0 Output csv files, plots, and txt files will then be put in the output directory specified above.","title":"Running the data"}]}